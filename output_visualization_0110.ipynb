{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Main plot\n",
        "- `Knees bending-Frontal elevation of arms`\n",
        "- `Waist bends forward-Jump front & back`\n",
        "- `Jogging-Running`"
      ],
      "metadata": {
        "id": "adV8_YVdU7_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ UPDATED (comment-based): only 2 plots per scenario\n",
        "#   1) Window-level rep_rate mean±std (+ boundary + A/B shading)\n",
        "#   2) Latent trajectory PCA2 (scatter + mean trajectory line + boundary marker + start/end + std circles)\n",
        "#   - NO titles\n",
        "#   - Bigger fonts\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns  # kept\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d  # kept\n",
        "from matplotlib.patches import Circle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from matplotlib.collections import LineCollection\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ Save helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _ensure_dir(path: str):\n",
        "    if path is None or path == \"\":\n",
        "        return\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def _safe_fname(s: str):\n",
        "    s = str(s)\n",
        "    for ch in ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|', '\\n', '\\t']:\n",
        "        s = s.replace(ch, '_')\n",
        "    s = s.strip().replace(' ', '_')\n",
        "    while '__' in s:\n",
        "        s = s.replace('__', '_')\n",
        "    return s\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ Mixed A-B builder\n",
        "# ---------------------------------------------------------------------\n",
        "def get_single_trial_from_full_data(subj, act_id, gt_count, full_data, target_map, feature_map, normalize=True):\n",
        "    act_name = target_map.get(act_id)\n",
        "    feats = feature_map.get(act_id)\n",
        "\n",
        "    if subj not in full_data or act_name not in full_data[subj]:\n",
        "        return None\n",
        "\n",
        "    raw_df = full_data[subj][act_name][feats]\n",
        "    x = raw_df.values.astype(np.float32)\n",
        "\n",
        "    if normalize:\n",
        "        mean = x.mean(axis=0)\n",
        "        std = x.std(axis=0) + 1e-6\n",
        "        x = (x - mean) / std\n",
        "\n",
        "    return {\n",
        "        \"data\": x,\n",
        "        \"count\": float(gt_count),\n",
        "        \"meta\": f\"{subj}_{act_name}\"\n",
        "    }\n",
        "\n",
        "\n",
        "def build_mixed_ab_trial(subj, actA_id, actB_id, config, full_data):\n",
        "    gt_map = config.get(\"GT_BY_ACT\", {})\n",
        "    if actA_id not in gt_map or actB_id not in gt_map:\n",
        "        return None\n",
        "    if subj not in gt_map[actA_id] or subj not in gt_map[actB_id]:\n",
        "        return None\n",
        "\n",
        "    gtA = float(gt_map[actA_id][subj])\n",
        "    gtB = float(gt_map[actB_id][subj])\n",
        "    gt_total = gtA + gtB\n",
        "\n",
        "    A = get_single_trial_from_full_data(\n",
        "        subj, actA_id, gtA, full_data,\n",
        "        config[\"TARGET_ACTIVITIES_MAP\"], config[\"ACT_FEATURE_MAP\"],\n",
        "        normalize=False\n",
        "    )\n",
        "    B = get_single_trial_from_full_data(\n",
        "        subj, actB_id, gtB, full_data,\n",
        "        config[\"TARGET_ACTIVITIES_MAP\"], config[\"ACT_FEATURE_MAP\"],\n",
        "        normalize=False\n",
        "    )\n",
        "    if A is None or B is None:\n",
        "        return None\n",
        "\n",
        "    xA_raw = A[\"data\"]\n",
        "    xB_raw = B[\"data\"]\n",
        "    boundary = int(xA_raw.shape[0])\n",
        "\n",
        "    x_mix_raw = np.concatenate([xA_raw, xB_raw], axis=0).astype(np.float32)\n",
        "\n",
        "    mean = x_mix_raw.mean(axis=0)\n",
        "    std = x_mix_raw.std(axis=0) + 1e-6\n",
        "    x_mix = (x_mix_raw - mean) / std\n",
        "\n",
        "    xA = (xA_raw - mean) / std\n",
        "    xB = (xB_raw - mean) / std\n",
        "\n",
        "    actA_name = config[\"TARGET_ACTIVITIES_MAP\"].get(actA_id, str(actA_id))\n",
        "    actB_name = config[\"TARGET_ACTIVITIES_MAP\"].get(actB_id, str(actB_id))\n",
        "\n",
        "    return {\n",
        "        \"data\": x_mix,\n",
        "        \"count\": float(gt_total),\n",
        "        \"meta\": f\"{subj}__{actA_name}__TO__{actB_name}\",\n",
        "        \"boundary\": boundary,\n",
        "        \"meta_detail\": {\n",
        "            \"subj\": subj,\n",
        "            \"actA_id\": actA_id, \"actB_id\": actB_id,\n",
        "            \"actA_name\": actA_name, \"actB_name\": actB_name,\n",
        "            \"gtA\": gtA, \"gtB\": gtB,\n",
        "            \"gt_total\": gt_total,\n",
        "        },\n",
        "        \"data_A\": xA,\n",
        "        \"data_B\": xB,\n",
        "        \"T_A\": int(xA.shape[0]),\n",
        "        \"T_B\": int(xB.shape[0]),\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Helpers for scenario-avg plots\n",
        "# ---------------------------------------------------------------------\n",
        "def get_window_rate_curve(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "\n",
        "    if T <= win_len:\n",
        "        pred_count, rates = predict_count_by_windowing(\n",
        "            model, x_np, fs, win_sec, stride_sec, device, tau=tau, batch_size=batch_size\n",
        "        )\n",
        "        t_cent = np.array([0.5 * (T / float(fs))], dtype=np.float32)\n",
        "        return t_cent, rates.astype(np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "    rates = np.concatenate(rates, axis=0).astype(np.float32)\n",
        "\n",
        "    centers = np.array([(st + 0.5 * win_len) / float(fs) for st in starts], dtype=np.float32)\n",
        "    return centers, rates\n",
        "\n",
        "\n",
        "def resample_1d(y, new_len):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    if y.size == 0:\n",
        "        return np.zeros((new_len,), dtype=np.float32)\n",
        "    if y.size == 1:\n",
        "        return np.full((new_len,), float(y[0]), dtype=np.float32)\n",
        "    x_old = np.linspace(0.0, 1.0, num=y.size, dtype=np.float32)\n",
        "    x_new = np.linspace(0.0, 1.0, num=new_len, dtype=np.float32)\n",
        "    return np.interp(x_new, x_old, y).astype(np.float32)\n",
        "\n",
        "\n",
        "def resample_2d(Y, new_len):\n",
        "    Y = np.asarray(Y, dtype=np.float32)\n",
        "    if Y.shape[0] == 0:\n",
        "        return np.zeros((new_len, Y.shape[1]), dtype=np.float32)\n",
        "    if Y.shape[0] == 1:\n",
        "        return np.repeat(Y, repeats=new_len, axis=0).astype(np.float32)\n",
        "    x_old = np.linspace(0.0, 1.0, num=Y.shape[0], dtype=np.float32)\n",
        "    x_new = np.linspace(0.0, 1.0, num=new_len, dtype=np.float32)\n",
        "    out = []\n",
        "    for d in range(Y.shape[1]):\n",
        "        out.append(np.interp(x_new, x_old, Y[:, d]))\n",
        "    return np.stack(out, axis=1).astype(np.float32)\n",
        "\n",
        "\n",
        "def global_pca2(Z_all):\n",
        "    Z_all = np.asarray(Z_all, dtype=np.float32)\n",
        "    mu = Z_all.mean(axis=0, keepdims=True)\n",
        "    Zc = Z_all - mu\n",
        "    _, _, Vt = np.linalg.svd(Zc, full_matrices=False)\n",
        "    V2 = Vt[:2].astype(np.float32)\n",
        "    return mu.squeeze(0).astype(np.float32), V2\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ UPDATED PLOTTER: comment-based 2 plots only, no titles, bigger fonts\n",
        "#   (1) Window mean±std + boundary + shading\n",
        "#   (2) PCA2 latent: scatter + mean trajectory line + boundary marker + start/end + std circles\n",
        "# ---------------------------------------------------------------------\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable  # ✅ 추가 import (plot 함수에서만 사용)\n",
        "\n",
        "def plot_scenario_mean_std_two_plots(\n",
        "    save_dir,\n",
        "    scen_name,\n",
        "    win_t, win_mean, win_std,\n",
        "    lat_xy_mean, lat_xy_std,\n",
        "    n_subjects,\n",
        "    dpi=600,\n",
        "):\n",
        "    _ensure_dir(save_dir if save_dir is not None else \"scenario_viz\")\n",
        "    out_dir = save_dir if save_dir is not None else \"scenario_viz\"\n",
        "    scen_tag = _safe_fname(scen_name)\n",
        "\n",
        "    # -------------------------\n",
        "    # (1) Window-level Rep rate mean±std\n",
        "    # -------------------------\n",
        "    fig1 = plt.figure(figsize=(26.0, 9.2))\n",
        "    ax = fig1.gca()\n",
        "\n",
        "    # subtle A/B shading (keep but slightly lighter)\n",
        "    ax.axvspan(0.0, 1.0, alpha=0.035, color=\"k\", lw=0)\n",
        "    ax.axvspan(1.0, 2.0, alpha=0.022, color=\"k\", lw=0)\n",
        "\n",
        "    ax.plot(\n",
        "        win_t, win_mean,\n",
        "        linewidth=7.8,\n",
        "        solid_capstyle=\"round\",\n",
        "        label=\"Window rep rate (mean)\"\n",
        "    )\n",
        "    ax.fill_between(\n",
        "        win_t,\n",
        "        win_mean - win_std, win_mean + win_std,\n",
        "        alpha=0.16,  # ✅ band alpha down\n",
        "        linewidth=0,\n",
        "        label=\"±1 std\"\n",
        "    )\n",
        "\n",
        "    ax.axvline(\n",
        "        1.0,\n",
        "        linestyle=\"--\",\n",
        "        linewidth=4.2,  # ✅ boundary thinner but still bold\n",
        "        label=\"Boundary\"\n",
        "    )\n",
        "\n",
        "    ax.set_xlabel(\"Normalized time (A:0–1, B:1–2)\", fontsize=54, labelpad=10)\n",
        "    ax.set_ylabel(\"Rep rate (reps/s)\", fontsize=54, labelpad=10)  # ✅ underscore 제거\n",
        "    ax.tick_params(axis=\"both\", labelsize=46, width=1.6, length=7)\n",
        "\n",
        "    # paper-like spine thickness\n",
        "    for sp in ax.spines.values():\n",
        "        sp.set_linewidth(1.6)\n",
        "\n",
        "    # legend: 조금만 줄이기\n",
        "    ax.legend(\n",
        "        fontsize=42,\n",
        "        frameon=True,\n",
        "        loc=\"upper left\",\n",
        "        borderpad=0.55,\n",
        "        labelspacing=0.35,\n",
        "        handlelength=2.4\n",
        "    )\n",
        "\n",
        "    fig1.tight_layout()\n",
        "    fig1.savefig(os.path.join(out_dir, f\"{scen_tag}__1_win_rep_rate.png\"), dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig1)\n",
        "\n",
        "    # -------------------------\n",
        "    # (2) Latent trajectory PCA2 (gradient line)\n",
        "    # -------------------------\n",
        "    fig2 = plt.figure(figsize=(10.0, 10.0))\n",
        "    ax = fig2.gca()\n",
        "\n",
        "    lat_xy_mean = np.asarray(lat_xy_mean, dtype=np.float32)\n",
        "    T = lat_xy_mean.shape[0]\n",
        "    t_lat = np.linspace(0.0, 2.0, num=T, dtype=np.float32)\n",
        "\n",
        "    pts = lat_xy_mean.reshape(-1, 1, 2)\n",
        "    segs = np.concatenate([pts[:-1], pts[1:]], axis=1)\n",
        "\n",
        "    lc = LineCollection(\n",
        "        segs,\n",
        "        array=t_lat[:-1],\n",
        "        linewidth=2.6,          # ✅ trajectory 더 또렷하게\n",
        "        alpha=0.95\n",
        "    )\n",
        "    ax.add_collection(lc)\n",
        "\n",
        "    ax.set_xlabel(\"PC1\", fontsize=25, labelpad=10)\n",
        "    ax.set_ylabel(\"PC2\", fontsize=25, labelpad=10)\n",
        "    ax.tick_params(axis=\"both\", labelsize=20, width=1.4, length=6)\n",
        "\n",
        "    for sp in ax.spines.values():\n",
        "        sp.set_linewidth(1.4)\n",
        "\n",
        "    # ✅ equal 유지 + 중앙 고정\n",
        "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
        "    ax.set_anchor(\"C\")\n",
        "\n",
        "    # square bounds 유지\n",
        "    x = lat_xy_mean[:, 0]; y = lat_xy_mean[:, 1]\n",
        "    xmin, xmax = float(np.min(x)), float(np.max(x))\n",
        "    ymin, ymax = float(np.min(y)), float(np.max(y))\n",
        "    cx = 0.5 * (xmin + xmax); cy = 0.5 * (ymin + ymax)\n",
        "    rx = 0.5 * (xmax - xmin); ry = 0.5 * (ymax - ymin)\n",
        "    r = max(rx, ry) * 1.12 + 1e-6\n",
        "    ax.set_xlim(cx - r, cx + r)\n",
        "    ax.set_ylim(cy - r, cy + r)\n",
        "\n",
        "    # ✅ colorbar가 axis를 “쪼그라뜨리는” 느낌을 줄이기 위해 divider로 별도 축 생성\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"4.5%\", pad=0.10)\n",
        "    cbar = fig2.colorbar(lc, cax=cax)\n",
        "    cbar.set_label(\"Normalized time (0→2)\", fontsize=22, labelpad=10)\n",
        "    cbar.ax.tick_params(labelsize=18, width=1.2, length=5)\n",
        "\n",
        "    fig2.tight_layout()\n",
        "    fig2.savefig(os.path.join(out_dir, f\"{scen_tag}__2_latent_pca2.png\"), dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig2)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Dataset / Collate\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),\n",
        "        \"mask\": torch.stack(masks),\n",
        "        \"count\": torch.stack(counts),\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)\n",
        "        z = z.transpose(1, 2)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)\n",
        "        x_hat = self.net(zt)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)\n",
        "        amp = F.softplus(out[..., 0])\n",
        "        phase_logits = out[..., 1:]\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        micro_rate_t = amp_t\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"phase_p\": phase_p,\n",
        "            \"phase_logits\": phase_logits,\n",
        "            \"micro_rate_t\": micro_rate_t,\n",
        "            \"rep_rate_t\": rep_rate_t,\n",
        "            \"k_hat\": k_hat,\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Main (Scenario-wise LOSO)\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6:  'Waist bends forward',\n",
        "            7:  'Frontal elevation of arms',\n",
        "            8:  'Knees bending',\n",
        "            10:  'Jogging',\n",
        "            11: 'Running',\n",
        "            12: 'Jump front & back',\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6:  ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            7:  ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            8:  ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            10: ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            11: ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            12: ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Windowing Params\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        \"GT_BY_ACT\": {\n",
        "            6:  {\"subject1\": 21, \"subject2\": 19, \"subject3\": 21, \"subject4\": 20, \"subject5\": 20,\n",
        "                 \"subject6\": 20, \"subject7\": 20, \"subject8\": 21, \"subject9\": 21, \"subject10\": 20},\n",
        "            7:  {\"subject1\": 20, \"subject2\": 20, \"subject3\": 20, \"subject4\": 20, \"subject5\": 20,\n",
        "                 \"subject6\": 20, \"subject7\": 20, \"subject8\": 19, \"subject9\": 19, \"subject10\": 20},\n",
        "            8:  {\"subject1\": 20, \"subject2\": 21, \"subject3\": 21, \"subject4\": 19, \"subject5\": 20,\n",
        "                 \"subject6\": 20, \"subject7\": 21, \"subject8\": 21, \"subject9\": 21, \"subject10\": 21},\n",
        "            10: {\"subject1\": 157, \"subject2\": 161, \"subject3\": 154, \"subject4\": 154, \"subject5\": 160,\n",
        "                 \"subject6\": 156, \"subject7\": 153, \"subject8\": 160, \"subject9\": 166, \"subject10\": 156},\n",
        "            11: {\"subject1\": 165, \"subject2\": 158, \"subject3\": 174, \"subject4\": 163, \"subject5\": 157,\n",
        "                 \"subject6\": 172, \"subject7\": 149, \"subject8\": 166, \"subject9\": 174, \"subject10\": 172},\n",
        "            12: {\"subject1\": 20, \"subject2\": 22, \"subject3\": 21, \"subject4\": 21, \"subject5\": 20,\n",
        "                 \"subject6\": 21, \"subject7\": 19, \"subject8\": 20, \"subject9\": 20, \"subject10\": 20},\n",
        "        },\n",
        "\n",
        "        \"MIX_SCENARIOS\": [\n",
        "            (\"Knees bending-Frontal elevation of arms\", 8, 7),\n",
        "            (\"Waist bends forward-Jump front & back\", 6, 12),\n",
        "            (\"Jogging-Running\", 10, 11),\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    VIZ_DIR = \"scenario_viz\"\n",
        "    os.makedirs(VIZ_DIR, exist_ok=True)\n",
        "\n",
        "    # resample lengths (fixed for averaging)\n",
        "    NW = 60     # window-rate points per segment (A,B)\n",
        "    NL = 200    # latent points per segment (A,B)\n",
        "\n",
        "    for scen_idx, (scen_name, actA_id, actB_id) in enumerate(CONFIG.get(\"MIX_SCENARIOS\", []), start=1):\n",
        "        TRAIN_ACT_ID = actA_id\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 100)\n",
        "        print(f\"1. 시나리오{scen_idx}: {scen_name}\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        loso_results = []\n",
        "        scenario_records = []\n",
        "        scenario_diffs = []\n",
        "        scenario_diffs_A = []\n",
        "        scenario_diffs_B = []\n",
        "\n",
        "        # ✅ buffers for TWO plots only\n",
        "        buf_win_concat = []     # (2*NW,)\n",
        "        buf_z_concat = []       # list of (Tmix,D)\n",
        "        per_subject_latent = [] # list of {\"zA\",\"zB\"}\n",
        "\n",
        "        for fold_idx, test_subj in enumerate(subjects):\n",
        "            set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "            gt_train_map = CONFIG[\"GT_BY_ACT\"][TRAIN_ACT_ID]\n",
        "            train_labels = [(s, TRAIN_ACT_ID, gt_train_map[s]) for s in subjects if s != test_subj]\n",
        "            test_labels  = [(test_subj, TRAIN_ACT_ID, gt_train_map[test_subj])]\n",
        "\n",
        "            train_trials = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "            test_trials  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "            if not test_trials or not train_trials:\n",
        "                continue\n",
        "\n",
        "            train_data = trial_list_to_windows(\n",
        "                train_trials, fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                drop_last=CONFIG[\"drop_last\"]\n",
        "            )\n",
        "\n",
        "            g = torch.Generator()\n",
        "            g.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "            train_loader = DataLoader(\n",
        "                TrialDataset(train_data),\n",
        "                batch_size=CONFIG[\"batch_size\"],\n",
        "                shuffle=True,\n",
        "                collate_fn=collate_variable_length,\n",
        "                generator=g,\n",
        "                num_workers=0\n",
        "            )\n",
        "\n",
        "            input_ch = train_data[0]['data'].shape[1]\n",
        "            model = KAutoCountModel(\n",
        "                input_ch=input_ch,\n",
        "                hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "                latent_dim=CONFIG[\"latent_dim\"],\n",
        "                K_max=CONFIG[\"K_max\"]\n",
        "            ).to(device)\n",
        "\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "            for epoch in range(CONFIG[\"epochs\"]):\n",
        "                _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "                scheduler.step()\n",
        "\n",
        "            model.eval\n",
        "            model.eval()\n",
        "\n",
        "            # (A) LOSO on TRAIN_ACT_ID\n",
        "            item = test_trials[0]\n",
        "            x_np = item[\"data\"]\n",
        "            count_gt = float(item[\"count\"])\n",
        "            count_pred_win, _ = predict_count_by_windowing(\n",
        "                model, x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            fold_mae = float(abs(count_pred_win - count_gt))\n",
        "            loso_results.append(fold_mae)\n",
        "\n",
        "            # (B) Scenario eval\n",
        "            mixed_item = build_mixed_ab_trial(test_subj, actA_id, actB_id, CONFIG, full_data)\n",
        "            if mixed_item is None:\n",
        "                continue\n",
        "\n",
        "            x_mix = mixed_item[\"data\"]\n",
        "            boundary = mixed_item[\"boundary\"]\n",
        "            gt_total = float(mixed_item[\"count\"])\n",
        "            md = mixed_item[\"meta_detail\"]\n",
        "\n",
        "            pred_total, _ = predict_count_by_windowing(\n",
        "                model, x_np=x_mix,\n",
        "                fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            diff = float(pred_total - gt_total)\n",
        "            mae = float(abs(diff))\n",
        "            scenario_diffs.append(diff)\n",
        "\n",
        "            # A/B split MAE\n",
        "            xA = mixed_item[\"data_A\"]\n",
        "            xB = mixed_item[\"data_B\"]\n",
        "            gtA = float(md[\"gtA\"])\n",
        "            gtB = float(md[\"gtB\"])\n",
        "\n",
        "            pred_A, _ = predict_count_by_windowing(\n",
        "                model, x_np=xA,\n",
        "                fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            pred_B, _ = predict_count_by_windowing(\n",
        "                model, x_np=xB,\n",
        "                fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            diff_A = float(pred_A - gtA)\n",
        "            diff_B = float(pred_B - gtB)\n",
        "            mae_A = float(abs(diff_A))\n",
        "            mae_B = float(abs(diff_B))\n",
        "            scenario_diffs_A.append(diff_A)\n",
        "            scenario_diffs_B.append(diff_B)\n",
        "\n",
        "            # full forward for z (latent) (+ keep entropy/k_hat log if you want)\n",
        "            x_tensor = torch.tensor(x_mix, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                _, z_m, _, aux_m = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "\n",
        "            phase_p_m = aux_m[\"phase_p\"].squeeze(0).detach().cpu().numpy()  # (Tmix,K)\n",
        "            k_hat_m = float(aux_m[\"k_hat\"].item())\n",
        "            ent_m = compute_phase_entropy_mean(phase_p_m)\n",
        "\n",
        "            z_td = z_m.squeeze(0).detach().cpu().numpy()  # (Tmix,D)\n",
        "\n",
        "            scenario_records.append({\n",
        "                \"subj\": test_subj,\n",
        "                \"line\": (\n",
        "                    f\"[Scenario] {scen_name} | {test_subj} | \"\n",
        "                    f\"{md['actA_name']}({md['gtA']:.0f}) -> {md['actB_name']}({md['gtB']:.0f}) | \"\n",
        "                    f\"GT_total={gt_total:.0f} | Pred_total(win)={pred_total:.2f} | Diff_total={diff:+.2f} | \"\n",
        "                    f\"MAE_total={mae:.2f} | \"\n",
        "                    f\"Pred_A={pred_A:.2f} (Diff_A={diff_A:+.2f}, MAE_A={mae_A:.2f}) | \"\n",
        "                    f\"Pred_B={pred_B:.2f} (Diff_B={diff_B:+.2f}, MAE_B={mae_B:.2f}) | \"\n",
        "                    f\"k_hat(full)={k_hat_m:.2f} | ent(full)={ent_m:.3f} | boundary={boundary}\"\n",
        "                )\n",
        "            })\n",
        "\n",
        "            # (1) window-level rep_rate curves: A/B separately -> resample -> concat\n",
        "            _, rA_w = get_window_rate_curve(\n",
        "                model, xA, fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            _, rB_w = get_window_rate_curve(\n",
        "                model, xB, fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            rA_rs = resample_1d(rA_w, NW)\n",
        "            rB_rs = resample_1d(rB_w, NW)\n",
        "            buf_win_concat.append(np.concatenate([rA_rs, rB_rs], axis=0))\n",
        "\n",
        "            # (2) latent: store raw z split for global PCA later\n",
        "            zA = z_td[:boundary, :]\n",
        "            zB = z_td[boundary:, :]\n",
        "            buf_z_concat.append(z_td)\n",
        "            per_subject_latent.append({\"zA\": zA, \"zB\": zB})\n",
        "\n",
        "        # -------------------------\n",
        "        # Print summaries (same as your original)\n",
        "        # -------------------------\n",
        "        print(\"-\" * 100)\n",
        "        if len(loso_results) > 0:\n",
        "            print(\"TRain 결과 ->\")\n",
        "            print(\"-\" * 100)\n",
        "            print(f\" >>> Final LOSO Result (Average MAE): {np.mean(loso_results):.3f}\")\n",
        "            print(f\" >>> Standard Deviation: {np.std(loso_results):.3f}\")\n",
        "            print(\"-\" * 100)\n",
        "        else:\n",
        "            print(\"TRain 결과 -> (no folds computed)\")\n",
        "            print(\"-\" * 100)\n",
        "\n",
        "        subj2line = {r[\"subj\"]: r[\"line\"] for r in scenario_records}\n",
        "        for s in subjects:\n",
        "            if s in subj2line:\n",
        "                print(subj2line[s])\n",
        "\n",
        "        print(\"\\n\" + \"-\" * 100)\n",
        "        print(f\"시나리오{scen_idx} 결과\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        if len(scenario_diffs) == 0:\n",
        "            print(f\"[{scen_name}] No samples (all skipped). Fill GT_BY_ACT for required activities.\")\n",
        "        else:\n",
        "            diffs = np.array(scenario_diffs, dtype=np.float32)\n",
        "            mae = float(np.mean(np.abs(diffs)))\n",
        "            rmse = float(np.sqrt(np.mean(diffs ** 2)))\n",
        "            print(f\"[TOTAL] [{scen_name}] N={len(diffs):2d} | MAE={mae:.3f} | RMSE={rmse:.3f} | mean(diff)={diffs.mean():+.3f} | std(diff)={diffs.std():.3f}\")\n",
        "\n",
        "            diffsA = np.array(scenario_diffs_A, dtype=np.float32)\n",
        "            maeA = float(np.mean(np.abs(diffsA)))\n",
        "            rmseA = float(np.sqrt(np.mean(diffsA ** 2)))\n",
        "            print(f\"[A]     [{scen_name}] N={len(diffsA):2d} | MAE={maeA:.3f} | RMSE={rmseA:.3f} | mean(diff)={diffsA.mean():+.3f} | std(diff)={diffsA.std():.3f}\")\n",
        "\n",
        "            diffsB = np.array(scenario_diffs_B, dtype=np.float32)\n",
        "            maeB = float(np.mean(np.abs(diffsB)))\n",
        "            rmseB = float(np.sqrt(np.mean(diffsB ** 2)))\n",
        "            print(f\"[B]     [{scen_name}] N={len(diffsB):2d} | MAE={maeB:.3f} | RMSE={rmseB:.3f} | mean(diff)={diffsB.mean():+.3f} | std(diff)={diffsB.std():.3f}\")\n",
        "\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        # -------------------------\n",
        "        # ✅ TWO-PLOT scenario-avg visualization\n",
        "        # -------------------------\n",
        "        n_ok = len(buf_win_concat)\n",
        "        if n_ok == 0:\n",
        "            print(\"[Viz Skip] No scenario samples collected for averaging.\")\n",
        "            continue\n",
        "\n",
        "        # (1) window-level mean±std on normalized axis [0,2]\n",
        "        W = np.stack(buf_win_concat, axis=0)  # (N, 2*NW)\n",
        "        win_mean = W.mean(axis=0)\n",
        "        win_std  = W.std(axis=0)\n",
        "        win_t = np.linspace(0.0, 2.0, num=2*NW, dtype=np.float32)\n",
        "\n",
        "        # (2) latent: global PCA basis from all z\n",
        "        Z_all = np.concatenate(buf_z_concat, axis=0)  # (sumT, D)\n",
        "        mu, V2 = global_pca2(Z_all)\n",
        "\n",
        "        # project each subject, resample A/B to NL then concat -> (2*NL,2)\n",
        "        lat_list = []\n",
        "        for d in per_subject_latent:\n",
        "            zA = d[\"zA\"]; zB = d[\"zB\"]\n",
        "            pcA = (zA - mu) @ V2.T\n",
        "            pcB = (zB - mu) @ V2.T\n",
        "            pcA_rs = resample_2d(pcA, NL)\n",
        "            pcB_rs = resample_2d(pcB, NL)\n",
        "            lat_list.append(np.concatenate([pcA_rs, pcB_rs], axis=0))\n",
        "\n",
        "        LAT = np.stack(lat_list, axis=0)  # (N,2*NL,2)\n",
        "        lat_xy_mean = LAT.mean(axis=0)\n",
        "        lat_xy_std  = LAT.std(axis=0)\n",
        "\n",
        "        plot_scenario_mean_std_two_plots(\n",
        "            save_dir=VIZ_DIR,\n",
        "            scen_name=scen_name,\n",
        "            win_t=win_t, win_mean=win_mean, win_std=win_std,\n",
        "            lat_xy_mean=lat_xy_mean, lat_xy_std=lat_xy_std,\n",
        "            n_subjects=n_ok,\n",
        "            dpi=600\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdfZ51vvo33a",
        "outputId": "f5966859-412a-474f-ebd7-117b822878c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오1: Knees bending-Frontal elevation of arms\n",
            "====================================================================================================\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 4.274\n",
            " >>> Standard Deviation: 3.012\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject1 | Knees bending(20) -> Frontal elevation of arms(20) | GT_total=40 | Pred_total(win)=66.34 | Diff_total=+26.34 | MAE_total=26.34 | Pred_A=47.42 (Diff_A=+27.42, MAE_A=27.42) | Pred_B=18.71 (Diff_B=-1.29, MAE_B=1.29) | k_hat(full)=1.02 | ent(full)=0.059 | boundary=3379\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject2 | Knees bending(21) -> Frontal elevation of arms(20) | GT_total=41 | Pred_total(win)=42.73 | Diff_total=+1.73 | MAE_total=1.73 | Pred_A=24.43 (Diff_A=+3.43, MAE_A=3.43) | Pred_B=18.26 (Diff_B=-1.74, MAE_B=1.74) | k_hat(full)=1.01 | ent(full)=0.041 | boundary=3430\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject3 | Knees bending(21) -> Frontal elevation of arms(20) | GT_total=41 | Pred_total(win)=30.07 | Diff_total=-10.93 | MAE_total=10.93 | Pred_A=14.94 (Diff_A=-6.06, MAE_A=6.06) | Pred_B=15.02 (Diff_B=-4.98, MAE_B=4.98) | k_hat(full)=1.01 | ent(full)=0.036 | boundary=3175\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject4 | Knees bending(19) -> Frontal elevation of arms(20) | GT_total=39 | Pred_total(win)=48.13 | Diff_total=+9.13 | MAE_total=9.13 | Pred_A=26.86 (Diff_A=+7.86, MAE_A=7.86) | Pred_B=21.45 (Diff_B=+1.45, MAE_B=1.45) | k_hat(full)=1.60 | ent(full)=0.163 | boundary=3123\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject5 | Knees bending(20) -> Frontal elevation of arms(20) | GT_total=40 | Pred_total(win)=31.99 | Diff_total=-8.01 | MAE_total=8.01 | Pred_A=14.43 (Diff_A=-5.57, MAE_A=5.57) | Pred_B=17.82 (Diff_B=-2.18, MAE_B=2.18) | k_hat(full)=1.02 | ent(full)=0.050 | boundary=2714\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject6 | Knees bending(20) -> Frontal elevation of arms(20) | GT_total=40 | Pred_total(win)=25.42 | Diff_total=-14.58 | MAE_total=14.58 | Pred_A=13.74 (Diff_A=-6.26, MAE_A=6.26) | Pred_B=11.71 (Diff_B=-8.29, MAE_B=8.29) | k_hat(full)=1.01 | ent(full)=0.033 | boundary=2304\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject7 | Knees bending(21) -> Frontal elevation of arms(20) | GT_total=41 | Pred_total(win)=36.12 | Diff_total=-4.88 | MAE_total=4.88 | Pred_A=12.28 (Diff_A=-8.72, MAE_A=8.72) | Pred_B=24.25 (Diff_B=+4.25, MAE_B=4.25) | k_hat(full)=1.00 | ent(full)=0.012 | boundary=2816\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject8 | Knees bending(21) -> Frontal elevation of arms(19) | GT_total=40 | Pred_total(win)=35.73 | Diff_total=-4.27 | MAE_total=4.27 | Pred_A=12.78 (Diff_A=-8.22, MAE_A=8.22) | Pred_B=23.25 (Diff_B=+4.25, MAE_B=4.25) | k_hat(full)=1.01 | ent(full)=0.036 | boundary=2560\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject9 | Knees bending(21) -> Frontal elevation of arms(19) | GT_total=40 | Pred_total(win)=36.83 | Diff_total=-3.17 | MAE_total=3.17 | Pred_A=12.01 (Diff_A=-8.99, MAE_A=8.99) | Pred_B=24.88 (Diff_B=+5.88, MAE_B=5.88) | k_hat(full)=1.01 | ent(full)=0.017 | boundary=2969\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject10 | Knees bending(21) -> Frontal elevation of arms(20) | GT_total=41 | Pred_total(win)=32.91 | Diff_total=-8.09 | MAE_total=8.09 | Pred_A=11.21 (Diff_A=-9.79, MAE_A=9.79) | Pred_B=21.97 (Diff_B=+1.97, MAE_B=1.97) | k_hat(full)=1.01 | ent(full)=0.018 | boundary=2867\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오1 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[TOTAL] [Knees bending-Frontal elevation of arms] N=10 | MAE=9.113 | RMSE=11.375 | mean(diff)=-1.672 | std(diff)=11.251\n",
            "[A]     [Knees bending-Frontal elevation of arms] N=10 | MAE=9.234 | RMSE=11.193 | mean(diff)=-1.490 | std(diff)=11.094\n",
            "[B]     [Knees bending-Frontal elevation of arms] N=10 | MAE=3.629 | RMSE=4.240 | mean(diff)=-0.068 | std(diff)=4.240\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오2: Waist bends forward-Jump front & back\n",
            "====================================================================================================\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 5.261\n",
            " >>> Standard Deviation: 3.006\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario] Waist bends forward-Jump front & back | subject1 | Waist bends forward(21) -> Jump front & back(20) | GT_total=41 | Pred_total(win)=26.26 | Diff_total=-14.74 | MAE_total=14.74 | Pred_A=13.11 (Diff_A=-7.89, MAE_A=7.89) | Pred_B=15.15 (Diff_B=-4.85, MAE_B=4.85) | k_hat(full)=1.10 | ent(full)=0.214 | boundary=3072\n",
            "[Scenario] Waist bends forward-Jump front & back | subject2 | Waist bends forward(19) -> Jump front & back(22) | GT_total=41 | Pred_total(win)=31.27 | Diff_total=-9.73 | MAE_total=9.73 | Pred_A=16.44 (Diff_A=-2.56, MAE_A=2.56) | Pred_B=18.16 (Diff_B=-3.84, MAE_B=3.84) | k_hat(full)=1.07 | ent(full)=0.155 | boundary=3174\n",
            "[Scenario] Waist bends forward-Jump front & back | subject3 | Waist bends forward(21) -> Jump front & back(21) | GT_total=42 | Pred_total(win)=39.70 | Diff_total=-2.30 | MAE_total=2.30 | Pred_A=15.13 (Diff_A=-5.87, MAE_A=5.87) | Pred_B=26.94 (Diff_B=+5.94, MAE_B=5.94) | k_hat(full)=1.04 | ent(full)=0.101 | boundary=3226\n",
            "[Scenario] Waist bends forward-Jump front & back | subject4 | Waist bends forward(20) -> Jump front & back(21) | GT_total=41 | Pred_total(win)=29.09 | Diff_total=-11.91 | MAE_total=11.91 | Pred_A=14.76 (Diff_A=-5.24, MAE_A=5.24) | Pred_B=16.69 (Diff_B=-4.31, MAE_B=4.31) | k_hat(full)=1.05 | ent(full)=0.121 | boundary=3328\n",
            "[Scenario] Waist bends forward-Jump front & back | subject5 | Waist bends forward(20) -> Jump front & back(20) | GT_total=40 | Pred_total(win)=37.98 | Diff_total=-2.02 | MAE_total=2.02 | Pred_A=20.05 (Diff_A=+0.05, MAE_A=0.05) | Pred_B=20.57 (Diff_B=+0.57, MAE_B=0.57) | k_hat(full)=1.06 | ent(full)=0.139 | boundary=2765\n",
            "[Scenario] Waist bends forward-Jump front & back | subject6 | Waist bends forward(20) -> Jump front & back(21) | GT_total=41 | Pred_total(win)=23.48 | Diff_total=-17.52 | MAE_total=17.52 | Pred_A=9.06 (Diff_A=-10.94, MAE_A=10.94) | Pred_B=14.99 (Diff_B=-6.01, MAE_B=6.01) | k_hat(full)=1.01 | ent(full)=0.043 | boundary=2202\n",
            "[Scenario] Waist bends forward-Jump front & back | subject7 | Waist bends forward(20) -> Jump front & back(19) | GT_total=39 | Pred_total(win)=39.25 | Diff_total=+0.25 | MAE_total=0.25 | Pred_A=17.74 (Diff_A=-2.26, MAE_A=2.26) | Pred_B=23.68 (Diff_B=+4.68, MAE_B=4.68) | k_hat(full)=1.09 | ent(full)=0.190 | boundary=3072\n",
            "[Scenario] Waist bends forward-Jump front & back | subject8 | Waist bends forward(21) -> Jump front & back(20) | GT_total=41 | Pred_total(win)=27.09 | Diff_total=-13.91 | MAE_total=13.91 | Pred_A=9.27 (Diff_A=-11.73, MAE_A=11.73) | Pred_B=20.10 (Diff_B=+0.10, MAE_B=0.10) | k_hat(full)=1.01 | ent(full)=0.037 | boundary=2151\n",
            "[Scenario] Waist bends forward-Jump front & back | subject9 | Waist bends forward(21) -> Jump front & back(20) | GT_total=41 | Pred_total(win)=35.21 | Diff_total=-5.79 | MAE_total=5.79 | Pred_A=15.25 (Diff_A=-5.75, MAE_A=5.75) | Pred_B=22.36 (Diff_B=+2.36, MAE_B=2.36) | k_hat(full)=1.01 | ent(full)=0.022 | boundary=2867\n",
            "[Scenario] Waist bends forward-Jump front & back | subject10 | Waist bends forward(20) -> Jump front & back(20) | GT_total=40 | Pred_total(win)=27.36 | Diff_total=-12.64 | MAE_total=12.64 | Pred_A=13.27 (Diff_A=-6.73, MAE_A=6.73) | Pred_B=15.24 (Diff_B=-4.76, MAE_B=4.76) | k_hat(full)=1.03 | ent(full)=0.073 | boundary=2458\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오2 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[TOTAL] [Waist bends forward-Jump front & back] N=10 | MAE=9.080 | RMSE=10.756 | mean(diff)=-9.030 | std(diff)=5.843\n",
            "[A]     [Waist bends forward-Jump front & back] N=10 | MAE=5.902 | RMSE=6.867 | mean(diff)=-5.893 | std(diff)=3.526\n",
            "[B]     [Waist bends forward-Jump front & back] N=10 | MAE=3.742 | RMSE=4.227 | mean(diff)=-1.012 | std(diff)=4.105\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오3: Jogging-Running\n",
            "====================================================================================================\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 10.421\n",
            " >>> Standard Deviation: 10.920\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario] Jogging-Running | subject1 | Jogging(157) -> Running(165) | GT_total=322 | Pred_total(win)=321.54 | Diff_total=-0.46 | MAE_total=0.46 | Pred_A=121.58 (Diff_A=-35.42, MAE_A=35.42) | Pred_B=201.85 (Diff_B=+36.85, MAE_B=36.85) | k_hat(full)=1.23 | ent(full)=0.311 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject2 | Jogging(161) -> Running(158) | GT_total=319 | Pred_total(win)=338.97 | Diff_total=+19.97 | MAE_total=19.97 | Pred_A=148.89 (Diff_A=-12.11, MAE_A=12.11) | Pred_B=189.01 (Diff_B=+31.01, MAE_B=31.01) | k_hat(full)=1.73 | ent(full)=0.485 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject3 | Jogging(154) -> Running(174) | GT_total=328 | Pred_total(win)=335.16 | Diff_total=+7.16 | MAE_total=7.16 | Pred_A=154.89 (Diff_A=+0.89, MAE_A=0.89) | Pred_B=182.06 (Diff_B=+8.06, MAE_B=8.06) | k_hat(full)=1.34 | ent(full)=0.315 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject4 | Jogging(154) -> Running(163) | GT_total=317 | Pred_total(win)=371.57 | Diff_total=+54.57 | MAE_total=54.57 | Pred_A=184.04 (Diff_A=+30.04, MAE_A=30.04) | Pred_B=187.40 (Diff_B=+24.40, MAE_B=24.40) | k_hat(full)=1.28 | ent(full)=0.278 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject5 | Jogging(160) -> Running(157) | GT_total=317 | Pred_total(win)=357.05 | Diff_total=+40.05 | MAE_total=40.05 | Pred_A=175.40 (Diff_A=+15.40, MAE_A=15.40) | Pred_B=181.58 (Diff_B=+24.58, MAE_B=24.58) | k_hat(full)=1.41 | ent(full)=0.330 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject6 | Jogging(156) -> Running(172) | GT_total=328 | Pred_total(win)=373.09 | Diff_total=+45.09 | MAE_total=45.09 | Pred_A=150.75 (Diff_A=-5.25, MAE_A=5.25) | Pred_B=225.75 (Diff_B=+53.75, MAE_B=53.75) | k_hat(full)=1.46 | ent(full)=0.359 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject7 | Jogging(153) -> Running(149) | GT_total=302 | Pred_total(win)=344.47 | Diff_total=+42.47 | MAE_total=42.47 | Pred_A=169.21 (Diff_A=+16.21, MAE_A=16.21) | Pred_B=174.35 (Diff_B=+25.35, MAE_B=25.35) | k_hat(full)=1.45 | ent(full)=0.356 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject8 | Jogging(160) -> Running(166) | GT_total=326 | Pred_total(win)=319.89 | Diff_total=-6.11 | MAE_total=6.11 | Pred_A=121.44 (Diff_A=-38.56, MAE_A=38.56) | Pred_B=201.68 (Diff_B=+35.68, MAE_B=35.68) | k_hat(full)=1.47 | ent(full)=0.433 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject9 | Jogging(166) -> Running(174) | GT_total=340 | Pred_total(win)=381.31 | Diff_total=+41.31 | MAE_total=41.31 | Pred_A=178.65 (Diff_A=+12.65, MAE_A=12.65) | Pred_B=203.41 (Diff_B=+29.41, MAE_B=29.41) | k_hat(full)=1.61 | ent(full)=0.324 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject10 | Jogging(156) -> Running(172) | GT_total=328 | Pred_total(win)=392.60 | Diff_total=+64.60 | MAE_total=64.60 | Pred_A=189.32 (Diff_A=+33.32, MAE_A=33.32) | Pred_B=203.67 (Diff_B=+31.67, MAE_B=31.67) | k_hat(full)=1.39 | ent(full)=0.330 | boundary=3072\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오3 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[TOTAL] [Jogging-Running] N=10 | MAE=32.179 | RMSE=38.452 | mean(diff)=+30.864 | std(diff)=22.934\n",
            "[A]     [Jogging-Running] N=10 | MAE=19.986 | RMSE=23.642 | mean(diff)=+1.715 | std(diff)=23.580\n",
            "[B]     [Jogging-Running] N=10 | MAE=30.075 | RMSE=32.026 | mean(diff)=+30.075 | std(diff)=11.007\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Integrator VS. Ours PCA plot\n",
        "- `Knees bending`, `Jump front and back`, `Jogging`"
      ],
      "metadata": {
        "id": "unCMHX10VAmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# PCA2D ONLY (Ours vs Integrator) — Activity-wise folders, fold-wise plots\n",
        "#\n",
        "# - For each activity:\n",
        "#   - LOSO fold loop\n",
        "#   - Train OURS model on train subjects (window-proxy supervision)\n",
        "#   - Extract OURS latent z(t) from the held-out test trial (full-trial forward)\n",
        "#   - Build Integrator window-feature trajectory from raw (order-invariant features)\n",
        "#   - Plot PCA2D comparison in ONE figure with 2 subplots (OURS vs INTEGRATOR)\n",
        "#   - NO titles, NO start/stop markers\n",
        "#   - Ticks padding tightened (closer to axes)\n",
        "#   - Saved as PNG (dpi=600), no plt.show()\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# IO helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _safe_filename(s: str) -> str:\n",
        "    s = str(s)\n",
        "    s = re.sub(r\"[^\\w\\-_\\. ]\", \"_\", s)\n",
        "    s = s.strip().replace(\" \", \"_\")\n",
        "    return s[:200] if len(s) > 200 else s\n",
        "\n",
        "\n",
        "def _ensure_dir(p: str):\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Strict seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Data loading (mHealth)\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    \"\"\"\n",
        "    Returns list of dict:\n",
        "      - data: z-scored (T,C)\n",
        "      - raw : raw (T,C)\n",
        "      - count: trial count\n",
        "      - meta : string\n",
        "    \"\"\"\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_id = int(act_id)\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            mean = raw_np.mean(axis=0, keepdims=True)\n",
        "            std = raw_np.std(axis=0, keepdims=True) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                \"data\": norm_np,\n",
        "                \"raw\": raw_np,\n",
        "                \"count\": float(gt_count),\n",
        "                \"meta\": f\"{subj}_{act_name}\",\n",
        "                \"subj\": subj,\n",
        "                \"act_id\": act_id,\n",
        "                \"act_name\": act_name,\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Windowing (TRAIN) + variable-length collate\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]  # (T,C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C,T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),    # (B,C,Tmax)\n",
        "        \"mask\": torch.stack(masks),          # (B,Tmax)\n",
        "        \"count\": torch.stack(counts),        # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),\n",
        "        \"meta\": metas,\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# OURS model (same as before but we only need z(t))\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B,D,T)\n",
        "        z = z.transpose(1, 2)      # (B,T,D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B,D,T)\n",
        "        return self.net(zt)        # (B,C,T)\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=128, K_max=6):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                 # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])     # (B,T)\n",
        "        logits = out[..., 1:]             # (B,T,K)\n",
        "        phase = F.softmax(logits / tau, dim=-1)\n",
        "        return amp, phase\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)        # (B,T,D)\n",
        "        x_hat = self.decoder(z)    # (B,C,T)\n",
        "        amp, phase = self.rate_head(z, tau=tau)  # (B,T), (B,T,K)\n",
        "\n",
        "        # rep_rate_t (for training losses)\n",
        "        p_bar = phase.mean(dim=1) if mask is None else (phase * mask.unsqueeze(-1)).sum(dim=1) / (mask.sum(dim=1, keepdim=True) + 1e-6)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)\n",
        "        rep_rate_t = amp / (k_hat.unsqueeze(1) + 1e-6)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        # avg rep rate (supervision target)\n",
        "        if mask is None:\n",
        "            avg_rep = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\"rep_rate_t\": rep_rate_t, \"phase\": phase}\n",
        "        return avg_rep, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Training (minimal)\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask.unsqueeze(1)\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase, mask=None, eps=1e-8):\n",
        "    ent = -(phase * (phase + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.unsqueeze(-1).to(dtype=phase.dtype, device=phase.device)\n",
        "        p_bar = (phase * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean()\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.0075)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        rate_hat, _, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_ent = phase_entropy_loss(aux[\"phase\"], mask)\n",
        "        loss_effk = effK_usage_loss(aux[\"phase\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_ent * loss_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# PCA2D helpers (no sklearn)\n",
        "# ---------------------------------------------------------------------\n",
        "def _fit_pca_basis(X, n_comp=2, eps=1e-6):\n",
        "    X = np.asarray(X, dtype=np.float32)\n",
        "    mu = X.mean(axis=0, keepdims=True)\n",
        "    sig = X.std(axis=0, keepdims=True) + eps\n",
        "    Xs = (X - mu) / sig\n",
        "    _, _, Vt = np.linalg.svd(Xs, full_matrices=False)\n",
        "    W = Vt[:n_comp].T\n",
        "    return mu.squeeze(0), sig.squeeze(0), W\n",
        "\n",
        "\n",
        "def _pca_project(X, mu, sig, W):\n",
        "    X = np.asarray(X, dtype=np.float32)\n",
        "    Xs = (X - mu[None, :]) / (sig[None, :] + 1e-12)\n",
        "    return Xs @ W\n",
        "\n",
        "\n",
        "def _downsample_by_index(X, t, max_N):\n",
        "    N = X.shape[0]\n",
        "    if N <= max_N:\n",
        "        return X, t\n",
        "    idx = np.linspace(0, N - 1, max_N).astype(int)\n",
        "    return X[idx], t[idx]\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Integrator: build window-feature trajectory (order-invariant)\n",
        "# ---------------------------------------------------------------------\n",
        "def integrator_window_feature_vector(x_raw: np.ndarray, fs: int, win_sec: float, stride_sec: float):\n",
        "    \"\"\"\n",
        "    Returns t_cent (N,), F (N, 2C)\n",
        "      per channel: log(1+energy), log(1+var)\n",
        "    \"\"\"\n",
        "    x = np.asarray(x_raw, dtype=np.float32)\n",
        "    T, C = x.shape\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    win_len = max(10, min(win_len, T))\n",
        "    stride = max(1, stride)\n",
        "\n",
        "    centers, feats = [], []\n",
        "    for st in range(0, max(1, T - win_len + 1), stride):\n",
        "        ed = st + win_len\n",
        "        w = x[st:ed] - x[st:ed].mean(axis=0, keepdims=True)\n",
        "        var = np.var(w, axis=0) + 1e-12\n",
        "        E = np.sum(w * w, axis=0) / max(float(fs), 1e-12)\n",
        "        f = np.concatenate([np.log1p(E), np.log1p(var)], axis=0)  # (2C,)\n",
        "        centers.append(0.5 * (st + ed) / float(fs))\n",
        "        feats.append(f.astype(np.float32))\n",
        "\n",
        "    return np.asarray(centers, dtype=np.float32), np.stack(feats, axis=0).astype(np.float32)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Plot: PCA2D comparison (2 subplots, no titles)\n",
        "# ---------------------------------------------------------------------\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "def plot_pca2d_comparison(\n",
        "    ours_z_tD, ours_t_sec,\n",
        "    int_F_nD, int_t_sec,\n",
        "    save_path,\n",
        "    dpi=600,\n",
        "    max_points=2000,\n",
        "    show_colorbar=True,\n",
        "    tick_pad=0,\n",
        "    point_size=10,\n",
        "    alpha=0.9\n",
        "):\n",
        "    ours_z_tD = np.asarray(ours_z_tD, dtype=np.float32)\n",
        "    ours_t_sec = np.asarray(ours_t_sec, dtype=np.float32)\n",
        "    int_F_nD = np.asarray(int_F_nD, dtype=np.float32)\n",
        "    int_t_sec = np.asarray(int_t_sec, dtype=np.float32)\n",
        "\n",
        "    ours_z_tD, ours_t_sec = _downsample_by_index(ours_z_tD, ours_t_sec, max_points)\n",
        "    int_F_nD, int_t_sec   = _downsample_by_index(int_F_nD, int_t_sec, max_points)\n",
        "\n",
        "    ours_c = (ours_t_sec - ours_t_sec.min()) / max(1e-6, (ours_t_sec.max() - ours_t_sec.min()))\n",
        "    int_c  = (int_t_sec  - int_t_sec.min())  / max(1e-6, (int_t_sec.max()  - int_t_sec.min()))\n",
        "\n",
        "    mu_o, sig_o, W_o = _fit_pca_basis(ours_z_tD, n_comp=2)\n",
        "    P_o = _pca_project(ours_z_tD, mu_o, sig_o, W_o)\n",
        "\n",
        "    mu_i, sig_i, W_i = _fit_pca_basis(int_F_nD, n_comp=2)\n",
        "    P_i = _pca_project(int_F_nD, mu_i, sig_i, W_i)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12.0, 5.0), sharey=True, gridspec_kw={\"wspace\": 0.10, \"width_ratios\": [1, 1]})\n",
        "    ax0, ax1 = axes\n",
        "\n",
        "    sc0 = ax0.scatter(P_o[:, 0], P_o[:, 1], c=ours_c, s=point_size, alpha=alpha, cmap=\"viridis\")\n",
        "    sc1 = ax1.scatter(P_i[:, 0], P_i[:, 1], c=int_c,  s=point_size, alpha=alpha, cmap=\"viridis\")\n",
        "\n",
        "    ax0.set_xlabel(\"PC1\", fontsize=16); ax0.set_ylabel(\"PC2\", fontsize=16)\n",
        "    ax1.set_xlabel(\"PC1\", fontsize=16); ax1.set_ylabel(\"\", fontsize=16)\n",
        "\n",
        "    for ax in (ax0, ax1):\n",
        "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=14, pad=tick_pad)\n",
        "        ax.grid(False)\n",
        "\n",
        "    # ✅ 여기 추가: 두 subplot에 동일한 xlim/ylim 적용\n",
        "    all_x = np.concatenate([P_o[:, 0], P_i[:, 0]], axis=0)\n",
        "    all_y = np.concatenate([P_o[:, 1], P_i[:, 1]], axis=0)\n",
        "\n",
        "    x_min, x_max = float(all_x.min()), float(all_x.max())\n",
        "    y_min, y_max = float(all_y.min()), float(all_y.max())\n",
        "\n",
        "    # padding (range의 5%)\n",
        "    x_pad = 0.05 * max(1e-6, (x_max - x_min))\n",
        "    y_pad = 0.05 * max(1e-6, (y_max - y_min))\n",
        "\n",
        "    xlim = (x_min - x_pad, x_max + x_pad)\n",
        "    ylim = (y_min - y_pad, y_max + y_pad)\n",
        "\n",
        "    ax0.set_xlim(*xlim); ax1.set_xlim(*xlim)\n",
        "    ax0.set_ylim(*ylim); ax1.set_ylim(*ylim)\n",
        "\n",
        "    if show_colorbar:\n",
        "        divider = make_axes_locatable(ax1)\n",
        "        cax = divider.append_axes(\"right\", size=\"4.5%\", pad=0.10)\n",
        "        cbar = fig.colorbar(sc1, cax=cax)\n",
        "        cbar.set_label(\"Normalized time (0→1)\", fontsize=14, labelpad=10)\n",
        "        cbar.ax.tick_params(labelsize=12, pad=tick_pad)\n",
        "        cbar.outline.set_linewidth(1.2)\n",
        "\n",
        "    _ensure_dir(os.path.dirname(save_path))\n",
        "    fig.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Main (LOSO per activity, PCA2D-only)\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "        \"out_dir\": \"./outputs_pca2d_only_compare\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            8:  'Knees bending',\n",
        "            10: 'Jogging',\n",
        "            12: 'Jump front & back',\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            8:  ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            10: ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            12: ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "        },\n",
        "\n",
        "        # Train params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Windowing for training\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # Integrator feature trajectory\n",
        "        \"INT_WIN_SEC\": 0.6,\n",
        "        \"INT_STRIDE_SEC\": 0.06,   # 더 촘촘하게 -> 점 개수 늘어서 blob 느낌\n",
        "    }\n",
        "\n",
        "    # Plot controls\n",
        "    PLOT = {\n",
        "        \"dpi\": 600,\n",
        "        \"max_points\": 2500,\n",
        "        \"show_colorbar\": True,   # 필요 없으면 False\n",
        "        \"tick_pad\": 3,          # 축에 붙게\n",
        "        \"point_size\": 10,\n",
        "        \"alpha\": 0.9,\n",
        "    }\n",
        "\n",
        "    # Count-only labels (그대로 사용)\n",
        "    CONFIG[\"ALL_LABELS\"] = [\n",
        "        # act 8\n",
        "        (\"subject1\", 8, 20), (\"subject2\", 8, 21), (\"subject3\", 8, 21), (\"subject4\", 8, 19), (\"subject5\", 8, 20),\n",
        "        (\"subject6\", 8, 20), (\"subject7\", 8, 21), (\"subject8\", 8, 21), (\"subject9\", 8, 21), (\"subject10\", 8, 21),\n",
        "\n",
        "        # act 10\n",
        "        (\"subject1\", 10, 157), (\"subject2\", 10, 161), (\"subject3\", 10, 154), (\"subject4\", 10, 154), (\"subject5\", 10, 160),\n",
        "        (\"subject6\", 10, 156), (\"subject7\", 10, 153), (\"subject8\", 10, 160), (\"subject9\", 10, 166), (\"subject10\", 10, 156),\n",
        "\n",
        "        # act 12\n",
        "        (\"subject1\", 12, 20), (\"subject2\", 12, 22), (\"subject3\", 12, 21), (\"subject4\", 12, 21), (\"subject5\", 12, 20),\n",
        "        (\"subject6\", 12, 21), (\"subject7\", 12, 19), (\"subject8\", 12, 20), (\"subject9\", 12, 20), (\"subject10\", 12, 20),\n",
        "    ]\n",
        "\n",
        "    _ensure_dir(CONFIG[\"out_dir\"])\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 90)\n",
        "    print(\" >>> PCA2D ONLY (OURS vs INTEGRATOR) — per activity, per fold\")\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    for act_id, act_name in CONFIG[\"TARGET_ACTIVITIES_MAP\"].items():\n",
        "        labels_act = [x for x in CONFIG[\"ALL_LABELS\"] if int(x[1]) == int(act_id)]\n",
        "        if len(labels_act) == 0:\n",
        "            print(f\"[Skip] act {act_id}: no labels.\")\n",
        "            continue\n",
        "\n",
        "        subjects_act = sorted(list(set([x[0] for x in labels_act])))\n",
        "\n",
        "        act_out_dir = _ensure_dir(os.path.join(CONFIG[\"out_dir\"], f\"act{act_id}_{_safe_filename(act_name)}\"))\n",
        "        print(\"\\n\" + \"=\" * 90)\n",
        "        print(f\"[Activity] {act_id}: {act_name} | #subjects={len(subjects_act)}\")\n",
        "        print(\"=\" * 90)\n",
        "\n",
        "        for fold_idx, test_subj in enumerate(subjects_act):\n",
        "            set_strict_seed(CONFIG[\"seed\"])\n",
        "            fold_out_dir = _ensure_dir(os.path.join(act_out_dir, f\"fold{fold_idx+1:02d}_{_safe_filename(test_subj)}\"))\n",
        "\n",
        "            train_labels = [x for x in labels_act if x[0] != test_subj]\n",
        "            test_labels  = [x for x in labels_act if x[0] == test_subj]\n",
        "\n",
        "            train_trials = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "            test_trials  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "            if not test_trials:\n",
        "                print(f\"[Skip] fold {fold_idx+1}: {test_subj} no data.\")\n",
        "                continue\n",
        "\n",
        "            # Train OURS\n",
        "            train_windows = trial_list_to_windows(\n",
        "                train_trials, fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                drop_last=CONFIG[\"drop_last\"]\n",
        "            )\n",
        "\n",
        "            g = torch.Generator()\n",
        "            g.manual_seed(CONFIG[\"seed\"])\n",
        "            train_loader = DataLoader(\n",
        "                TrialDataset(train_windows),\n",
        "                batch_size=CONFIG[\"batch_size\"],\n",
        "                shuffle=True,\n",
        "                collate_fn=collate_variable_length,\n",
        "                generator=g,\n",
        "                num_workers=0\n",
        "            )\n",
        "\n",
        "            input_ch = train_windows[0][\"data\"].shape[1]\n",
        "            model = KAutoCountModel(\n",
        "                input_ch=input_ch,\n",
        "                hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "                latent_dim=CONFIG[\"latent_dim\"],\n",
        "                K_max=CONFIG[\"K_max\"]\n",
        "            ).to(device)\n",
        "\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "            for _ in range(CONFIG[\"epochs\"]):\n",
        "                train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "                scheduler.step()\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            # Take ONE test trial (보통 1개)\n",
        "            item = test_trials[0]\n",
        "            x_np = item[\"data\"]     # (T,C) z-scored\n",
        "            x_raw = item[\"raw\"]     # (T,C) raw\n",
        "\n",
        "            # OURS z(t)\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "            with torch.no_grad():\n",
        "                _, z_full, _, _ = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "            ours_z = z_full.squeeze(0).detach().cpu().numpy()  # (T,D)\n",
        "            ours_t = np.arange(ours_z.shape[0], dtype=np.float32) / float(CONFIG[\"fs\"])\n",
        "\n",
        "            # Integrator features trajectory\n",
        "            int_t, int_F = integrator_window_feature_vector(\n",
        "                x_raw, fs=CONFIG[\"fs\"],\n",
        "                win_sec=float(CONFIG[\"INT_WIN_SEC\"]),\n",
        "                stride_sec=float(CONFIG[\"INT_STRIDE_SEC\"])\n",
        "            )\n",
        "\n",
        "            # Plot PCA2D comparison\n",
        "            save_path = os.path.join(fold_out_dir, \"pca2d_compare.png\")\n",
        "            plot_pca2d_comparison(\n",
        "                ours_z_tD=ours_z, ours_t_sec=ours_t,\n",
        "                int_F_nD=int_F, int_t_sec=int_t,\n",
        "                save_path=save_path,\n",
        "                dpi=int(PLOT[\"dpi\"]),\n",
        "                max_points=int(PLOT[\"max_points\"]),\n",
        "                show_colorbar=bool(PLOT[\"show_colorbar\"]),\n",
        "                tick_pad=float(PLOT[\"tick_pad\"]),\n",
        "                point_size=float(PLOT[\"point_size\"]),\n",
        "                alpha=float(PLOT[\"alpha\"]),\n",
        "            )\n",
        "\n",
        "            print(f\"  Fold {fold_idx+1:02d} | Test {test_subj} | saved: {save_path}\")\n",
        "\n",
        "    print(\"\\nDone.\")\n",
        "    print(f\"[Output root] {os.path.abspath(CONFIG['out_dir'])}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vxZ6apk9AK1",
        "outputId": "0d02010f-ce3b-437f-f7cb-b9ae457c31e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            " >>> PCA2D ONLY (OURS vs INTEGRATOR) — per activity, per fold\n",
            "------------------------------------------------------------------------------------------\n",
            "\n",
            "==========================================================================================\n",
            "[Activity] 8: Knees bending | #subjects=10\n",
            "==========================================================================================\n",
            "  Fold 01 | Test subject1 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold01_subject1/pca2d_compare.png\n",
            "  Fold 02 | Test subject10 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold02_subject10/pca2d_compare.png\n",
            "  Fold 03 | Test subject2 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold03_subject2/pca2d_compare.png\n",
            "  Fold 04 | Test subject3 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold04_subject3/pca2d_compare.png\n",
            "  Fold 05 | Test subject4 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold05_subject4/pca2d_compare.png\n",
            "  Fold 06 | Test subject5 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold06_subject5/pca2d_compare.png\n",
            "  Fold 07 | Test subject6 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold07_subject6/pca2d_compare.png\n",
            "  Fold 08 | Test subject7 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold08_subject7/pca2d_compare.png\n",
            "  Fold 09 | Test subject8 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold09_subject8/pca2d_compare.png\n",
            "  Fold 10 | Test subject9 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold10_subject9/pca2d_compare.png\n",
            "\n",
            "==========================================================================================\n",
            "[Activity] 10: Jogging | #subjects=10\n",
            "==========================================================================================\n",
            "  Fold 01 | Test subject1 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold01_subject1/pca2d_compare.png\n",
            "  Fold 02 | Test subject10 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold02_subject10/pca2d_compare.png\n",
            "  Fold 03 | Test subject2 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold03_subject2/pca2d_compare.png\n",
            "  Fold 04 | Test subject3 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold04_subject3/pca2d_compare.png\n",
            "  Fold 05 | Test subject4 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold05_subject4/pca2d_compare.png\n",
            "  Fold 06 | Test subject5 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold06_subject5/pca2d_compare.png\n",
            "  Fold 07 | Test subject6 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold07_subject6/pca2d_compare.png\n",
            "  Fold 08 | Test subject7 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold08_subject7/pca2d_compare.png\n",
            "  Fold 09 | Test subject8 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold09_subject8/pca2d_compare.png\n",
            "  Fold 10 | Test subject9 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold10_subject9/pca2d_compare.png\n",
            "\n",
            "==========================================================================================\n",
            "[Activity] 12: Jump front & back | #subjects=10\n",
            "==========================================================================================\n",
            "  Fold 01 | Test subject1 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold01_subject1/pca2d_compare.png\n",
            "  Fold 02 | Test subject10 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold02_subject10/pca2d_compare.png\n",
            "  Fold 03 | Test subject2 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold03_subject2/pca2d_compare.png\n",
            "  Fold 04 | Test subject3 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold04_subject3/pca2d_compare.png\n",
            "  Fold 05 | Test subject4 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold05_subject4/pca2d_compare.png\n",
            "  Fold 06 | Test subject5 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold06_subject5/pca2d_compare.png\n",
            "  Fold 07 | Test subject6 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold07_subject6/pca2d_compare.png\n",
            "  Fold 08 | Test subject7 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold08_subject7/pca2d_compare.png\n",
            "  Fold 09 | Test subject8 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold09_subject8/pca2d_compare.png\n",
            "  Fold 10 | Test subject9 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold10_subject9/pca2d_compare.png\n",
            "\n",
            "Done.\n",
            "[Output root] /content/outputs_pca2d_only_compare\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Activity shift: Success VS. Failure case PCA & line plot\n",
        "- `Frontal elevation of arms→Knees bending `\n",
        "- `Knees bending→Jogging `"
      ],
      "metadata": {
        "id": "vHk87xiIZsbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# A1 Pairwise (SUCCESS + FAILURE only) Full Code  ✅ (requested)\n",
        "#\n",
        "# ✅ Only 2 experiments:\n",
        "#   (SUCCESS) Frontal elevation of arms (7) -> Knees bending (8)\n",
        "#   (FAILURE) Knees bending (8) -> Jogging (10)\n",
        "#\n",
        "# ✅ A1 visualization (per experiment):\n",
        "#   (1) Train-PCA basis에 Test를 투영해서 \"초점/방향(manifold alignment) 차이\"를 2-panel로 비교\n",
        "#   (2) pred_rate(t) vs gt_rate overlay (성공/실패 둘 다 저장; 실패에서 특히 해석용)\n",
        "#\n",
        "# -------------------------\n",
        "# \"다른건 건드리지 말고\" 원칙에 따라,\n",
        "# ❗ 꼭 필요해서 바꾼 부분:\n",
        "#   ✅ xlim, ylim만 A1_PCA_train7_test8_subject5 기준으로 \"고정\"되도록 추가/적용\n",
        "# -------------------------\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 0) Small helpers (dir / downsample / PCA)\n",
        "# ---------------------------------------------------------------------\n",
        "def _ensure_dir(path: str):\n",
        "    if path is None or path == \"\":\n",
        "        return\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "def _downsample_by_index(X, t, max_points=2000):\n",
        "    X = np.asarray(X)\n",
        "    t = np.asarray(t)\n",
        "    n = len(t)\n",
        "    if n <= max_points:\n",
        "        return X, t\n",
        "    idx = np.linspace(0, n - 1, max_points).astype(np.int64)\n",
        "    return X[idx], t[idx]\n",
        "\n",
        "\n",
        "def _fit_pca_basis(z_tD, n_comp=2, eps=1e-6):\n",
        "    \"\"\"\n",
        "    z_tD: (T,D)\n",
        "    returns: mu(D,), sig(D,), W(D,n_comp)\n",
        "    \"\"\"\n",
        "    Z = np.asarray(z_tD, dtype=np.float32)\n",
        "    mu = Z.mean(axis=0)\n",
        "    sig = Z.std(axis=0) + eps\n",
        "    Zs = (Z - mu) / sig\n",
        "    # Cov (D,D)\n",
        "    C = (Zs.T @ Zs) / max(1, (Zs.shape[0] - 1))\n",
        "    # eig via SVD (stable)\n",
        "    U, S, Vt = np.linalg.svd(C, full_matrices=False)\n",
        "    W = U[:, :n_comp]\n",
        "    return mu.astype(np.float32), sig.astype(np.float32), W.astype(np.float32)\n",
        "\n",
        "\n",
        "def _pca_project(z_tD, mu, sig, W):\n",
        "    Z = np.asarray(z_tD, dtype=np.float32)\n",
        "    Zs = (Z - mu) / sig\n",
        "    return (Zs @ W).astype(np.float32)\n",
        "\n",
        "\n",
        "# ✅ xlim/ylim 기준(Reference) 계산 helper: A1_PCA_train7_test8_subject5 기준으로 저장\n",
        "def _compute_ref_xlim_ylim_from_train_test_latents(train_z_tD, test_z_tD, pad_ratio=0.05,\n",
        "                                                   q_low=1.0, q_high=99.0):\n",
        "    mu, sig, W = _fit_pca_basis(train_z_tD, n_comp=2)\n",
        "    P_tr = _pca_project(train_z_tD, mu, sig, W)\n",
        "    P_te = _pca_project(test_z_tD,  mu, sig, W)\n",
        "\n",
        "    all_x = np.concatenate([P_tr[:,0], P_te[:,0]])\n",
        "    all_y = np.concatenate([P_tr[:,1], P_te[:,1]])\n",
        "\n",
        "    x_min, x_max = np.percentile(all_x, [q_low, q_high])\n",
        "    y_min, y_max = np.percentile(all_y, [q_low, q_high])\n",
        "\n",
        "    x_pad = pad_ratio * max(1e-6, (x_max - x_min))\n",
        "    y_pad = pad_ratio * max(1e-6, (y_max - y_min))\n",
        "\n",
        "    return (float(x_min - x_pad), float(x_max + x_pad)), (float(y_min - y_pad), float(y_max + y_pad))\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score (trial-wise)\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,              # (T,C)\n",
        "                'count': float(gt_count),      # trial total count\n",
        "                'meta': f\"{subj}_{act_name}\",\n",
        "                'subj': subj,\n",
        "                'act_id': act_id,\n",
        "            })\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    TRAIN 전용: trial -> sliding windows\n",
        "    window 라벨은 trial 평균 rate로부터 생성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]  # (T,C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    TEST 전용: trial -> windows inference -> window rate 평균 -> total count\n",
        "    x_np: (T,C) numpy (정규화된 상태)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return float(pred_count)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N,C,win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count)\n",
        "\n",
        "\n",
        "# ✅ [MOD-1] 추가: pred_rate(t) 시계열까지 반환\n",
        "def predict_count_and_rates_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    returns:\n",
        "      pred_count: float\n",
        "      t_centers_sec: (N,) window center time in sec\n",
        "      rates: (N,) predicted rate per window (reps/s)\n",
        "      gt_rate: float (trial-level gt_count / duration)\n",
        "      total_dur: float\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        rate = float(rate_hat.item())\n",
        "        pred_count = rate * total_dur\n",
        "        t_centers = np.array([0.5 * total_dur], dtype=np.float32)\n",
        "        rates = np.array([rate], dtype=np.float32)\n",
        "        return float(pred_count), t_centers, rates, float(total_dur)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    centers = np.array([st + 0.5 * win_len for st in starts], dtype=np.float32) / float(fs)\n",
        "\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N,C,win_len)\n",
        "\n",
        "    rates_list = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates_list.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates_list, axis=0).astype(np.float32)  # (N,)\n",
        "    pred_count = float(rates.mean() * total_dur)\n",
        "    return float(pred_count), centers, rates, float(total_dur)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.8) Dataset / Collate\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B,D,T)\n",
        "        z = z.transpose(1, 2)      # (B,T,D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B,D,T)\n",
        "        x_hat = self.net(zt)       # (B,C,T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        micro_rate_t = amp_t             # (B,T)\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,)\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6) # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"phase_p\": phase_p,          # (B,T,K)\n",
        "            \"rep_rate_t\": rep_rate_t,    # (B,T)\n",
        "            \"k_hat\": k_hat,              # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean()\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)  # (T,K)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4.5) A1 Plots\n",
        "# ---------------------------------------------------------------------\n",
        "def plot_pca2d_train_test_overlay(\n",
        "    train_z_tD, train_t_sec,\n",
        "    test_z_tD, test_t_sec,\n",
        "    save_path,\n",
        "    dpi=600,\n",
        "    max_points=2000,\n",
        "    show_colorbar=True,\n",
        "    tick_pad=0,\n",
        "    point_size=10,\n",
        "    alpha_test=0.9,\n",
        "    alpha_train=0.35,\n",
        "    train_lw=2.0,\n",
        "    # ✅ NEW: 고정 축 (A1_PCA_train7_test8_subject5 기준)\n",
        "    fixed_xlim=None,   # (xmin, xmax)\n",
        "    fixed_ylim=None    # (ymin, ymax)\n",
        "):\n",
        "    \"\"\"\n",
        "    ✅ PCA basis is FIT on TRAIN, then BOTH train/test are projected into the SAME basis.\n",
        "    ✅ Single-axis OVERLAY:\n",
        "       - TRAIN: gray scatter (reference manifold)\n",
        "       - TEST : time-colored scatter + single colorbar (time)\n",
        "    \"\"\"\n",
        "    train_z_tD = np.asarray(train_z_tD, dtype=np.float32)\n",
        "    train_t_sec = np.asarray(train_t_sec, dtype=np.float32)\n",
        "    test_z_tD  = np.asarray(test_z_tD,  dtype=np.float32)\n",
        "    test_t_sec = np.asarray(test_t_sec, dtype=np.float32)\n",
        "\n",
        "    train_z_tD, train_t_sec = _downsample_by_index(train_z_tD, train_t_sec, max_points)\n",
        "    test_z_tD,  test_t_sec  = _downsample_by_index(test_z_tD,  test_t_sec,  max_points)\n",
        "\n",
        "    test_c = (test_t_sec - test_t_sec.min()) / max(1e-6, (test_t_sec.max() - test_t_sec.min()))\n",
        "\n",
        "    mu, sig, W = _fit_pca_basis(train_z_tD, n_comp=2)\n",
        "    P_tr = _pca_project(train_z_tD, mu, sig, W)   # (T,2)\n",
        "    P_te = _pca_project(test_z_tD,  mu, sig, W)   # (T,2)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(7.0, 6.0))\n",
        "\n",
        "    ax.scatter(P_tr[:, 0], P_tr[:, 1], s=point_size, alpha=0.25, color=\"0.6\", label=\"Train (ref)\")\n",
        "    sc = ax.scatter(P_te[:, 0], P_te[:, 1], c=test_c, s=point_size, alpha=alpha_test, cmap=\"viridis\", label=\"Test (proj)\")\n",
        "\n",
        "    ax.set_xlabel(\"PC1\", fontsize=16)\n",
        "    ax.set_ylabel(\"PC2\", fontsize=16)\n",
        "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=14, pad=tick_pad)\n",
        "    ax.grid(False)\n",
        "\n",
        "    # ✅ xlim/ylim: 고정 축이 주어지면 그대로 사용\n",
        "    if fixed_xlim is not None and fixed_ylim is not None:\n",
        "        ax.set_xlim(float(fixed_xlim[0]), float(fixed_xlim[1]))\n",
        "        ax.set_ylim(float(fixed_ylim[0]), float(fixed_ylim[1]))\n",
        "    else:\n",
        "        all_x = np.concatenate([P_tr[:, 0], P_te[:, 0]], axis=0)\n",
        "        all_y = np.concatenate([P_tr[:, 1], P_te[:, 1]], axis=0)\n",
        "\n",
        "        x_min, x_max = float(all_x.min()), float(all_x.max())\n",
        "        y_min, y_max = float(all_y.min()), float(all_y.max())\n",
        "\n",
        "        x_pad = 0.05 * max(1e-6, (x_max - x_min))\n",
        "        y_pad = 0.05 * max(1e-6, (y_max - y_min))\n",
        "\n",
        "        ax.set_xlim(x_min - x_pad, x_max + x_pad)\n",
        "        ax.set_ylim(y_min - y_pad, y_max + y_pad)\n",
        "\n",
        "    if show_colorbar:\n",
        "        divider = make_axes_locatable(ax)\n",
        "        cax = divider.append_axes(\"right\", size=\"4.5%\", pad=0.10)\n",
        "        cbar = fig.colorbar(sc, cax=cax)\n",
        "        cbar.set_label(\"Normalized time (0→1)\", fontsize=14, labelpad=10)\n",
        "        cbar.ax.tick_params(labelsize=12, pad=tick_pad)\n",
        "        cbar.outline.set_linewidth(1.2)\n",
        "\n",
        "    _ensure_dir(os.path.dirname(save_path))\n",
        "    fig.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def plot_predrate_vs_gtrate(\n",
        "    t_sec,\n",
        "    pred_rate,\n",
        "    gt_rate,\n",
        "    save_path,\n",
        "    dpi=600,\n",
        "    tick_pad=0,\n",
        "    smooth_sigma=1.0,\n",
        "    lw_pred=4.0,\n",
        "    lw_gt=4.0,\n",
        "    legend_loc=\"best\"\n",
        "):\n",
        "    \"\"\"\n",
        "    pred_rate: (N,) window-level predicted repetition rate (reps/s)\n",
        "    gt_rate  : float, trial-level constant rate = count / duration\n",
        "    \"\"\"\n",
        "    t_sec = np.asarray(t_sec, dtype=np.float32)\n",
        "    pred_rate = np.asarray(pred_rate, dtype=np.float32)\n",
        "\n",
        "    if smooth_sigma is not None and smooth_sigma > 0:\n",
        "        pred_plot = gaussian_filter1d(pred_rate, sigma=float(smooth_sigma))\n",
        "    else:\n",
        "        pred_plot = pred_rate\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(8.5, 5.5))\n",
        "\n",
        "    ax.plot(\n",
        "        t_sec, pred_plot,\n",
        "        linewidth=lw_pred,\n",
        "        label=\"Pred rate (window-level)\"\n",
        "    )\n",
        "\n",
        "    ax.axhline(\n",
        "        y=float(gt_rate),\n",
        "        linestyle=\"--\",\n",
        "        linewidth=lw_gt,\n",
        "        label=\"GT rate (trial-level)\"\n",
        "    )\n",
        "\n",
        "    ax.set_xlabel(\"Time (s)\", fontsize=34)\n",
        "    ax.set_ylabel(\"Repetition rate (reps/s)\", fontsize=34)\n",
        "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=24, pad=tick_pad)\n",
        "    ax.grid(False)\n",
        "    ax.legend(loc=legend_loc, frameon=False, fontsize=18)\n",
        "\n",
        "    _ensure_dir(os.path.dirname(save_path))\n",
        "    fig.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        rate_hat, _, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Main (SUCCESS + FAILURE only)\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "        \"fs\": 50,\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "\n",
        "        # windowing\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # loss\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # outputs\n",
        "        \"plot_dir\": \"/content/A1_pairwise_A1viz\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            7: 'Frontal elevation of arms',\n",
        "            8: 'Knees bending',\n",
        "            10: 'Jogging',\n",
        "            11: 'Running',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            10: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                 'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                 'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                 'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                 'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            11: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                 'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                 'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                 'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                 'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                 'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                 'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                 'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                 'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "        },\n",
        "\n",
        "        \"COUNT_TABLE\": {\n",
        "            6: {\n",
        "                \"subject1\": 21, \"subject2\": 19, \"subject3\": 21, \"subject4\": 20, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 20, \"subject8\": 21, \"subject9\": 21, \"subject10\": 20,\n",
        "            },\n",
        "            7: {\n",
        "                \"subject1\": 20, \"subject2\": 20, \"subject3\": 20, \"subject4\": 20, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 20, \"subject8\": 19, \"subject9\": 19, \"subject10\": 20,\n",
        "            },\n",
        "            8: {\n",
        "                \"subject1\": 20, \"subject2\": 21, \"subject3\": 21, \"subject4\": 19, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 21, \"subject8\": 21, \"subject9\": 21, \"subject10\": 21,\n",
        "            },\n",
        "            10: {\n",
        "                \"subject1\": 157, \"subject2\": 161, \"subject3\": 154, \"subject4\": 154, \"subject5\": 160,\n",
        "                \"subject6\": 156, \"subject7\": 153, \"subject8\": 160, \"subject9\": 166, \"subject10\": 156,\n",
        "            },\n",
        "            11: {\n",
        "                \"subject1\": 165, \"subject2\": 158, \"subject3\": 174, \"subject4\": 163, \"subject5\": 157,\n",
        "                \"subject6\": 172, \"subject7\": 149, \"subject8\": 166, \"subject9\": 174, \"subject10\": 172,\n",
        "            },\n",
        "            12: {\n",
        "                \"subject1\": 20, \"subject2\": 22, \"subject3\": 21, \"subject4\": 21, \"subject5\": 20,\n",
        "                \"subject6\": 21, \"subject7\": 19, \"subject8\": 20, \"subject9\": 20, \"subject10\": 20,\n",
        "            },\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # -----------------------------\n",
        "    # sanity: feature 동일성 체크\n",
        "    # -----------------------------\n",
        "    feats_ref = None\n",
        "    for act_id, feats in CONFIG[\"ACT_FEATURE_MAP\"].items():\n",
        "        if feats_ref is None:\n",
        "            feats_ref = tuple(feats)\n",
        "        elif tuple(feats) != feats_ref:\n",
        "            raise ValueError(f\"[ERROR] ACT_FEATURE_MAP must be identical across activities. mismatch at act_id={act_id}\")\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n",
        "    if not full_data:\n",
        "        print(\"[ERROR] dataset load failed\")\n",
        "        return\n",
        "\n",
        "    _ensure_dir(CONFIG[\"plot_dir\"])\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    exp_dirs = [\n",
        "        (7, 8),\n",
        "        (8, 10),\n",
        "    ]\n",
        "\n",
        "    def build_labels(act_id):\n",
        "        labels = []\n",
        "        for s in subjects:\n",
        "            if act_id not in CONFIG[\"COUNT_TABLE\"]:\n",
        "                continue\n",
        "            if s not in CONFIG[\"COUNT_TABLE\"][act_id]:\n",
        "                continue\n",
        "            labels.append((s, act_id, CONFIG[\"COUNT_TABLE\"][act_id][s]))\n",
        "        return labels\n",
        "\n",
        "    # ✅ Reference axis limits from A1_PCA_train7_test8_subject5\n",
        "    REF_SUBJ = \"subject5\"\n",
        "    REF_XLIM, REF_YLIM = None, None\n",
        "\n",
        "    final_blocks = []\n",
        "    exp_idx = 0\n",
        "\n",
        "    for train_act, test_act in exp_dirs:\n",
        "        exp_idx += 1\n",
        "\n",
        "        train_labels = build_labels(train_act)\n",
        "        test_labels  = build_labels(test_act)\n",
        "        if len(train_labels) == 0 or len(test_labels) == 0:\n",
        "            print(f\"[Skip] Missing COUNT_TABLE: train_act={train_act}, test_act={test_act}\")\n",
        "            continue\n",
        "\n",
        "        set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "        train_trials = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_trials  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        if len(train_trials) == 0 or len(test_trials) == 0:\n",
        "            print(f\"[Skip] Empty trials: train_act={train_act}, test_act={test_act}\")\n",
        "            continue\n",
        "\n",
        "        train_windows = trial_list_to_windows(\n",
        "            train_trials,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"win_sec\"],\n",
        "            stride_sec=CONFIG[\"stride_sec\"],\n",
        "            drop_last=CONFIG[\"drop_last\"]\n",
        "        )\n",
        "        if len(train_windows) == 0:\n",
        "            print(f\"[Skip] Empty train windows: train_act={train_act}\")\n",
        "            continue\n",
        "\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(CONFIG[\"seed\"])\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_windows),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            generator=g,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_windows[0][\"data\"].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        train_name = CONFIG[\"TARGET_ACTIVITIES_MAP\"][train_act]\n",
        "        test_name  = CONFIG[\"TARGET_ACTIVITIES_MAP\"][test_act]\n",
        "\n",
        "        block_lines = []\n",
        "        abs_errs = []\n",
        "        per_subj = {}\n",
        "\n",
        "        for item in test_trials:\n",
        "            subj = item[\"subj\"]\n",
        "            x_np = item[\"data\"]\n",
        "            gt = float(item[\"count\"])\n",
        "\n",
        "            pred = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"],\n",
        "                stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device,\n",
        "                tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            diff = pred - gt\n",
        "            ae = abs(diff)\n",
        "            abs_errs.append(ae)\n",
        "            per_subj[subj] = {\"ae\": ae, \"pred\": pred, \"gt\": gt, \"x_np\": x_np}\n",
        "\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "\n",
        "            line = (f\"{subj} | GT={gt:.0f} | Pred(win)={pred:.2f} | Diff={diff:+.2f} | \"\n",
        "                    f\"k_hat(full)={k_hat:.2f} | phase_entropy(full)={ent:.3f}\")\n",
        "            block_lines.append(line)\n",
        "\n",
        "        mae = float(np.mean(abs_errs)) if len(abs_errs) > 0 else float(\"nan\")\n",
        "\n",
        "        is_success_pair = (train_act == 7 and test_act == 8)\n",
        "        if is_success_pair:\n",
        "            pick_subj = sorted(per_subj.keys(), key=lambda s: per_subj[s][\"ae\"])[0]\n",
        "        else:\n",
        "            pick_subj = sorted(per_subj.keys(), key=lambda s: per_subj[s][\"ae\"], reverse=True)[0]\n",
        "\n",
        "        # ------------------\n",
        "        # ✅ REF axis set once: based on A1_PCA_train7_test8_subject5\n",
        "        # ------------------\n",
        "        if REF_XLIM is None and REF_YLIM is None and (train_act == 7 and test_act == 8):\n",
        "            ref_train_item = next((it for it in train_trials if it[\"subj\"] == REF_SUBJ), None)\n",
        "            ref_test_item  = next((it for it in test_trials  if it[\"subj\"] == REF_SUBJ), None)\n",
        "\n",
        "            # fallback (only if subject5 not found)\n",
        "            if ref_train_item is None or ref_test_item is None:\n",
        "                ref_train_item = next((it for it in train_trials if it[\"subj\"] == pick_subj), None)\n",
        "                ref_test_item  = next((it for it in test_trials  if it[\"subj\"] == pick_subj), None)\n",
        "\n",
        "            if ref_train_item is not None and ref_test_item is not None:\n",
        "                fs = CONFIG[\"fs\"]\n",
        "                xtr_ref = torch.tensor(ref_train_item[\"data\"], dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "                xte_ref = torch.tensor(ref_test_item[\"data\"],  dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    ztr_ref = model.encoder(xtr_ref).squeeze(0).detach().cpu().numpy()\n",
        "                    zte_ref = model.encoder(xte_ref).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "                REF_XLIM, REF_YLIM = _compute_ref_xlim_ylim_from_train_test_latents(ztr_ref, zte_ref, pad_ratio=0.05)\n",
        "                print(f\"[REF AXIS] Using A1_PCA_train7_test8_{REF_SUBJ} xlim={REF_XLIM}, ylim={REF_YLIM}\")\n",
        "\n",
        "        # ------------------\n",
        "        # A1 visualization for picked subject\n",
        "        # ------------------\n",
        "        train_item = next((it for it in train_trials if it[\"subj\"] == pick_subj), None)\n",
        "        test_item  = next((it for it in test_trials  if it[\"subj\"] == pick_subj), None)\n",
        "\n",
        "        if train_item is not None and test_item is not None:\n",
        "            fs = CONFIG[\"fs\"]\n",
        "\n",
        "            xtr = torch.tensor(train_item[\"data\"], dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            xte = torch.tensor(test_item[\"data\"],  dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                ztr = model.encoder(xtr).squeeze(0).detach().cpu().numpy()\n",
        "                zte = model.encoder(xte).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "            ttr = np.arange(ztr.shape[0], dtype=np.float32) / float(fs)\n",
        "            tte = np.arange(zte.shape[0], dtype=np.float32) / float(fs)\n",
        "\n",
        "            pca_path = os.path.join(\n",
        "                CONFIG[\"plot_dir\"],\n",
        "                f\"A1_PCA_train{train_act}_test{test_act}_{pick_subj}.png\"\n",
        "            )\n",
        "\n",
        "            plot_pca2d_train_test_overlay(\n",
        "                train_z_tD=ztr, train_t_sec=ttr,\n",
        "                test_z_tD=zte,  test_t_sec=tte,\n",
        "                save_path=pca_path,\n",
        "                dpi=600,\n",
        "                max_points=2000,\n",
        "                show_colorbar=True,\n",
        "                tick_pad=0,\n",
        "                point_size=10,\n",
        "                alpha_test=0.9,\n",
        "                alpha_train=0.35,\n",
        "                train_lw=2.0,\n",
        "                # ✅ apply fixed axis (subject5 기준)\n",
        "                fixed_xlim=REF_XLIM,\n",
        "                fixed_ylim=REF_YLIM\n",
        "            )\n",
        "\n",
        "            x_np = test_item[\"data\"]\n",
        "            gt_count = float(test_item[\"count\"])\n",
        "            pred_count, t_centers, rates, total_dur = predict_count_and_rates_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=fs,\n",
        "                win_sec=CONFIG[\"win_sec\"],\n",
        "                stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device,\n",
        "                tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            gt_rate = gt_count / max(1e-6, total_dur)\n",
        "\n",
        "            rate_path = os.path.join(\n",
        "                CONFIG[\"plot_dir\"],\n",
        "                f\"A1_Rate_train{train_act}_test{test_act}_{pick_subj}.png\"\n",
        "            )\n",
        "            plot_predrate_vs_gtrate(\n",
        "                t_sec=t_centers,\n",
        "                pred_rate=rates,\n",
        "                gt_rate=gt_rate,\n",
        "                save_path=rate_path,\n",
        "                dpi=600,\n",
        "                tick_pad=0,\n",
        "                smooth_sigma=1.0\n",
        "            )\n",
        "\n",
        "        final_blocks.append({\n",
        "            \"idx\": exp_idx,\n",
        "            \"title\": f\"{train_name} -> {test_name}\",\n",
        "            \"lines\": block_lines,\n",
        "            \"mae\": mae,\n",
        "            \"picked_subject_for_plot\": pick_subj,\n",
        "            \"plot_dir\": CONFIG[\"plot_dir\"]\n",
        "        })\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 110)\n",
        "    print(\"A1 Pairwise Activity Transfer Results (SUCCESS + FAILURE only, windowing inference)\")\n",
        "    print(\"=\" * 110)\n",
        "\n",
        "    if len(final_blocks) == 0:\n",
        "        print(\"[No results] COUNT_TABLE이 충분히 채워졌는지 확인해줘.\")\n",
        "        print(\"=\" * 110)\n",
        "        return\n",
        "\n",
        "    for b in final_blocks:\n",
        "        print(f\"\\n{b['idx']}. {b['title']}  |  MAE={b['mae']:.3f}\")\n",
        "        print(f\"   -> A1 plots saved for: {b['picked_subject_for_plot']}  |  dir: {b['plot_dir']}\")\n",
        "        for ln in b[\"lines\"]:\n",
        "            print(ln)\n",
        "\n",
        "    maes = [b[\"mae\"] for b in final_blocks if np.isfinite(b[\"mae\"])]\n",
        "    if len(maes) > 0:\n",
        "        print(\"\\n\" + \"-\" * 110)\n",
        "        print(f\"Overall Avg MAE={float(np.mean(maes)):.3f} | Std={float(np.std(maes)):.3f} | #experiments={len(maes)}\")\n",
        "        print(\"-\" * 110)\n",
        "\n",
        "    print(\"=\" * 110)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWAAO0TiZ7KV",
        "outputId": "5219a487-dc10-4e35-f123-247bd3d90837"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[REF AXIS] Using A1_PCA_train7_test8_subject5 xlim=(-4.4326539268493645, 4.755654520034789), ylim=(-4.507941735863685, 3.118046556591987)\n",
            "\n",
            "==============================================================================================================\n",
            "A1 Pairwise Activity Transfer Results (SUCCESS + FAILURE only, windowing inference)\n",
            "==============================================================================================================\n",
            "\n",
            "1. Frontal elevation of arms -> Knees bending  |  MAE=2.275\n",
            "   -> A1 plots saved for: subject5  |  dir: /content/A1_pairwise_A1viz\n",
            "subject1 | GT=20 | Pred(win)=24.42 | Diff=+4.42 | k_hat(full)=1.00 | phase_entropy(full)=0.014\n",
            "subject2 | GT=21 | Pred(win)=23.17 | Diff=+2.17 | k_hat(full)=1.01 | phase_entropy(full)=0.039\n",
            "subject3 | GT=21 | Pred(win)=22.01 | Diff=+1.01 | k_hat(full)=1.01 | phase_entropy(full)=0.030\n",
            "subject4 | GT=19 | Pred(win)=19.86 | Diff=+0.86 | k_hat(full)=1.01 | phase_entropy(full)=0.033\n",
            "subject5 | GT=20 | Pred(win)=19.23 | Diff=-0.77 | k_hat(full)=1.01 | phase_entropy(full)=0.035\n",
            "subject6 | GT=20 | Pred(win)=15.92 | Diff=-4.08 | k_hat(full)=1.01 | phase_entropy(full)=0.017\n",
            "subject7 | GT=21 | Pred(win)=19.29 | Diff=-1.71 | k_hat(full)=1.02 | phase_entropy(full)=0.042\n",
            "subject8 | GT=21 | Pred(win)=17.52 | Diff=-3.48 | k_hat(full)=1.01 | phase_entropy(full)=0.022\n",
            "subject9 | GT=21 | Pred(win)=19.58 | Diff=-1.42 | k_hat(full)=1.01 | phase_entropy(full)=0.030\n",
            "subject10 | GT=21 | Pred(win)=18.17 | Diff=-2.83 | k_hat(full)=1.02 | phase_entropy(full)=0.050\n",
            "\n",
            "2. Knees bending -> Jogging  |  MAE=137.272\n",
            "   -> A1 plots saved for: subject9  |  dir: /content/A1_pairwise_A1viz\n",
            "subject1 | GT=157 | Pred(win)=16.94 | Diff=-140.06 | k_hat(full)=1.02 | phase_entropy(full)=0.056\n",
            "subject2 | GT=161 | Pred(win)=19.57 | Diff=-141.43 | k_hat(full)=1.02 | phase_entropy(full)=0.057\n",
            "subject3 | GT=154 | Pred(win)=20.84 | Diff=-133.16 | k_hat(full)=1.01 | phase_entropy(full)=0.041\n",
            "subject4 | GT=154 | Pred(win)=20.81 | Diff=-133.19 | k_hat(full)=1.01 | phase_entropy(full)=0.040\n",
            "subject5 | GT=160 | Pred(win)=21.06 | Diff=-138.94 | k_hat(full)=1.02 | phase_entropy(full)=0.042\n",
            "subject6 | GT=156 | Pred(win)=21.22 | Diff=-134.78 | k_hat(full)=1.02 | phase_entropy(full)=0.045\n",
            "subject7 | GT=153 | Pred(win)=20.95 | Diff=-132.05 | k_hat(full)=1.01 | phase_entropy(full)=0.029\n",
            "subject8 | GT=160 | Pred(win)=17.46 | Diff=-142.54 | k_hat(full)=1.01 | phase_entropy(full)=0.041\n",
            "subject9 | GT=166 | Pred(win)=22.92 | Diff=-143.08 | k_hat(full)=1.01 | phase_entropy(full)=0.037\n",
            "subject10 | GT=156 | Pred(win)=22.51 | Diff=-133.49 | k_hat(full)=1.01 | phase_entropy(full)=0.039\n",
            "\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Overall Avg MAE=69.774 | Std=67.498 | #experiments=2\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "==============================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# for mm-Fit\n",
        "- 위 1번 3번과 동일한 시각화"
      ],
      "metadata": {
        "id": "sOVyzQwZ9xTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ MM-Fit DROP-IN (Scenario-wise LOSO) — FULL CODE\n",
        "#\n",
        "# Goal:\n",
        "# - Replace mHealth data pipeline with MM-Fit\n",
        "# - Test ALL directed A->B pairs among 3 exercises => 3*2 = 6 scenarios\n",
        "# - Keep visualization EXACTLY as-is (DO NOT TOUCH plot design)\n",
        "#\n",
        "# Protocol (same spirit as your mHealth scenario code):\n",
        "# - For each scenario (A->B):\n",
        "#   - TRAIN: single-activity windows from exercise A, subjects except held-out\n",
        "#   - TEST (LOSO on A): held-out subject's ALL A trials -> fold MAE averaged across its A trials\n",
        "#   - SCENARIO eval: build ONE mixed trial for held-out subject by concatenating:\n",
        "#       (selected A trial) + (selected B trial)\n",
        "#     then run windowing inference & do latent forward on mixed\n",
        "#\n",
        "# Notes:\n",
        "# - MM-Fit subjects (participant) may have different #trials per exercise.\n",
        "# - For LOSO(A) fold MAE: average over ALL A trials of held-out subject.\n",
        "# - For mixed scenario: pick ONE deterministic trial per exercise per subject (the first by sort order).\n",
        "# - If a subject lacks A or B trials, that fold is skipped for that scenario.\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns  # kept\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d  # kept\n",
        "from matplotlib.patches import Circle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from matplotlib.collections import LineCollection\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ Save helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _ensure_dir(path: str):\n",
        "    if path is None or path == \"\":\n",
        "        return\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def _safe_fname(s: str):\n",
        "    s = str(s)\n",
        "    for ch in ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|', '\\n', '\\t']:\n",
        "        s = s.replace(ch, '_')\n",
        "    s = s.strip().replace(' ', '_')\n",
        "    while '__' in s:\n",
        "        s = s.replace('__', '_')\n",
        "    return s\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) MM-Fit Data Loading (meta + npz)\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mmfit_meta(meta_csv_path: str,\n",
        "                    target_exercises: set,\n",
        "                    only_reps=None,\n",
        "                    require_device: str = \"sw_r\"):\n",
        "    \"\"\"\n",
        "    meta.csv 로딩.\n",
        "    expected columns:\n",
        "      participant, session, set_id, exercise, reps, fs, npz_path\n",
        "    optional:\n",
        "      device\n",
        "    \"\"\"\n",
        "    if not os.path.exists(meta_csv_path):\n",
        "        raise FileNotFoundError(f\"[MM-Fit] meta csv not found: {meta_csv_path}\")\n",
        "\n",
        "    meta = pd.read_csv(meta_csv_path)\n",
        "\n",
        "    need_cols = [\"participant\", \"session\", \"set_id\", \"exercise\", \"reps\", \"fs\", \"npz_path\"]\n",
        "    for c in need_cols:\n",
        "        if c not in meta.columns:\n",
        "            raise ValueError(f\"[MM-Fit] meta csv missing column: {c} (have={list(meta.columns)})\")\n",
        "\n",
        "    meta[\"exercise\"] = meta[\"exercise\"].astype(str).str.strip().str.lower()\n",
        "\n",
        "    meta[\"session\"] = meta[\"session\"].astype(str).str.strip()\n",
        "\n",
        "    # participant sanitize\n",
        "    meta[\"participant\"] = pd.to_numeric(meta[\"participant\"], errors=\"coerce\")\n",
        "    meta = meta.dropna(subset=[\"participant\"]).copy()\n",
        "    meta[\"participant\"] = meta[\"participant\"].astype(int)\n",
        "\n",
        "    # device filter if exists\n",
        "    if \"device\" in meta.columns and require_device is not None:\n",
        "        meta = meta[meta[\"device\"].astype(str).str.strip() == require_device].copy()\n",
        "\n",
        "    # exercise filter\n",
        "    meta = meta[meta[\"exercise\"].isin(set(target_exercises))].copy()\n",
        "\n",
        "    # reps filter\n",
        "    if only_reps is not None:\n",
        "        meta = meta[meta[\"reps\"].round().astype(int) == int(only_reps)].copy()\n",
        "\n",
        "    # path exists\n",
        "    meta[\"npz_path\"] = meta[\"npz_path\"].astype(str)\n",
        "    ok = meta[\"npz_path\"].apply(lambda p: os.path.exists(p))\n",
        "    meta = meta[ok].copy()\n",
        "\n",
        "    meta = meta.sort_values([\"participant\", \"session\", \"exercise\", \"set_id\"]).reset_index(drop=True)\n",
        "    return meta\n",
        "\n",
        "\n",
        "def _is_finite_np(x: np.ndarray) -> bool:\n",
        "    return np.isfinite(x).all()\n",
        "\n",
        "\n",
        "def _load_npz_X_and_fs(npz_path: str, fallback_fs: float):\n",
        "    d = np.load(npz_path, allow_pickle=True)\n",
        "    if \"X\" not in d.files:\n",
        "        return None, None\n",
        "    X = d[\"X\"].astype(np.float32)  # (T,C)\n",
        "    fs_npz = float(d[\"fs\"]) if \"fs\" in d.files else float(fallback_fs)\n",
        "    return X, fs_npz\n",
        "\n",
        "\n",
        "def prepare_trial_list_mmfit(meta_rows: pd.DataFrame,\n",
        "                            expected_fs: float,\n",
        "                            skip_nonfinite: bool = True,\n",
        "                            verbose_skip: bool = True):\n",
        "    \"\"\"\n",
        "    - each row = 1 trial\n",
        "    - count label = reps\n",
        "    - per-trial z-score\n",
        "    - skip nonfinite (NaN/Inf)\n",
        "    \"\"\"\n",
        "    trial_list = []\n",
        "    skipped = []  # (reason, npz_path)\n",
        "\n",
        "    for _, r in meta_rows.iterrows():\n",
        "        npz_path = str(r[\"npz_path\"])\n",
        "        reps = float(r[\"reps\"])\n",
        "        pid  = int(r[\"participant\"])\n",
        "        sid  = str(r[\"session\"])\n",
        "        set_id = int(r[\"set_id\"])\n",
        "        ex = str(r[\"exercise\"])\n",
        "\n",
        "        try:\n",
        "            X, fs_npz = _load_npz_X_and_fs(npz_path, fallback_fs=float(r[\"fs\"]))\n",
        "        except Exception:\n",
        "            skipped.append((\"npz_load_fail\", npz_path))\n",
        "            continue\n",
        "\n",
        "        if X is None:\n",
        "            skipped.append((\"missing_X\", npz_path))\n",
        "            continue\n",
        "\n",
        "        if abs(float(fs_npz) - float(expected_fs)) > 1e-3:\n",
        "            raise ValueError(\n",
        "                f\"[MM-Fit] fs mismatch for {npz_path}: fs_npz={fs_npz}, expected_fs={expected_fs}. \"\n",
        "                f\"-> extractor 단계에서 TARGET_FS를 expected_fs로 통일하세요.\"\n",
        "            )\n",
        "\n",
        "        if skip_nonfinite and (not _is_finite_np(X)):\n",
        "            skipped.append((\"nonfinite_X\", npz_path))\n",
        "            continue\n",
        "\n",
        "        # per-trial z-score\n",
        "        mean = X.mean(axis=0)\n",
        "        std  = X.std(axis=0)\n",
        "        std  = np.where(std < 1e-6, 1e-6, std).astype(np.float32)\n",
        "        norm_np = (X - mean) / std\n",
        "\n",
        "        if skip_nonfinite and (not _is_finite_np(norm_np)):\n",
        "            skipped.append((\"nonfinite_after_norm\", npz_path))\n",
        "            continue\n",
        "\n",
        "        trial_list.append({\n",
        "            \"data\": norm_np,              # (T,C)\n",
        "            \"count\": float(reps),         # trial total count\n",
        "            \"meta\": f\"subj{pid}_{sid}_set{set_id:02d}_{ex}\",\n",
        "            \"participant\": pid,\n",
        "            \"session\": sid,\n",
        "            \"exercise\": ex,\n",
        "            \"set_id\": set_id,\n",
        "            \"npz_path\": npz_path,\n",
        "        })\n",
        "\n",
        "    if verbose_skip and len(skipped) > 0:\n",
        "        print(f\"[MM-Fit] Skipped {len(skipped)} trials (nonfinite/load issues). Examples:\")\n",
        "        for i in range(min(5, len(skipped))):\n",
        "            print(\"  -\", skipped[i][0], \":\", skipped[i][1])\n",
        "\n",
        "    return trial_list, skipped\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ MM-Fit single-trial picker (deterministic) + mixed builder\n",
        "# ---------------------------------------------------------------------\n",
        "def _select_first_trial_row(meta_df: pd.DataFrame, participant: int, exercise: str):\n",
        "    \"\"\"\n",
        "    deterministic: meta already sorted by (participant, session, exercise, set_id)\n",
        "    -> pick the first row for (participant, exercise)\n",
        "    \"\"\"\n",
        "    sub = meta_df[(meta_df[\"participant\"] == int(participant)) & (meta_df[\"exercise\"] == str(exercise))].copy()\n",
        "    if len(sub) == 0:\n",
        "        return None\n",
        "    sub = sub.sort_values([\"participant\", \"session\", \"exercise\", \"set_id\"]).reset_index(drop=True)\n",
        "    return sub.iloc[0]\n",
        "\n",
        "\n",
        "def get_single_trial_raw_mmfit(meta_df: pd.DataFrame, participant: int, exercise: str, expected_fs: float):\n",
        "    \"\"\"\n",
        "    Return raw X (no per-trial zscore) + reps count, picking first trial deterministically.\n",
        "    \"\"\"\n",
        "    r = _select_first_trial_row(meta_df, participant, exercise)\n",
        "    if r is None:\n",
        "        return None\n",
        "\n",
        "    npz_path = str(r[\"npz_path\"])\n",
        "    reps = float(r[\"reps\"])\n",
        "    fs_fallback = float(r[\"fs\"])\n",
        "\n",
        "    try:\n",
        "        X, fs_npz = _load_npz_X_and_fs(npz_path, fallback_fs=fs_fallback)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "    if X is None:\n",
        "        return None\n",
        "\n",
        "    if abs(float(fs_npz) - float(expected_fs)) > 1e-3:\n",
        "        raise ValueError(\n",
        "            f\"[MM-Fit] fs mismatch for {npz_path}: fs_npz={fs_npz}, expected_fs={expected_fs}.\"\n",
        "        )\n",
        "\n",
        "    if not _is_finite_np(X):\n",
        "        return None\n",
        "\n",
        "    # meta string\n",
        "    pid = int(r[\"participant\"])\n",
        "    sid = str(r[\"session\"])\n",
        "    set_id = int(r[\"set_id\"])\n",
        "    ex = str(r[\"exercise\"])\n",
        "    meta = f\"subj{pid}_{sid}_set{set_id:02d}_{ex}\"\n",
        "\n",
        "    return {\n",
        "        \"data_raw\": X.astype(np.float32),   # raw (T,C)\n",
        "        \"count\": float(reps),\n",
        "        \"meta\": meta,\n",
        "        \"npz_path\": npz_path,\n",
        "        \"session\": sid,\n",
        "        \"set_id\": set_id,\n",
        "        \"exercise\": ex,\n",
        "        \"participant\": pid,\n",
        "    }\n",
        "\n",
        "\n",
        "def build_mixed_ab_trial_mmfit(participant: int, exA: str, exB: str, config: dict, meta_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Mixed trial = concat(raw A, raw B) -> z-score on concatenated signal\n",
        "    Also returns normalized A, normalized B under the same (mix) stats, and boundary.\n",
        "    \"\"\"\n",
        "    A = get_single_trial_raw_mmfit(meta_df, participant, exA, expected_fs=config[\"fs\"])\n",
        "    B = get_single_trial_raw_mmfit(meta_df, participant, exB, expected_fs=config[\"fs\"])\n",
        "    if A is None or B is None:\n",
        "        return None\n",
        "\n",
        "    xA_raw = A[\"data_raw\"]\n",
        "    xB_raw = B[\"data_raw\"]\n",
        "    boundary = int(xA_raw.shape[0])\n",
        "\n",
        "    x_mix_raw = np.concatenate([xA_raw, xB_raw], axis=0).astype(np.float32)\n",
        "\n",
        "    # global z-score on mixed\n",
        "    mean = x_mix_raw.mean(axis=0)\n",
        "    std  = x_mix_raw.std(axis=0) + 1e-6\n",
        "    x_mix = (x_mix_raw - mean) / std\n",
        "\n",
        "    xA = (xA_raw - mean) / std\n",
        "    xB = (xB_raw - mean) / std\n",
        "\n",
        "    gtA = float(A[\"count\"])\n",
        "    gtB = float(B[\"count\"])\n",
        "    gt_total = gtA + gtB\n",
        "\n",
        "    scen_name = f\"{exA}__TO__{exB}\"\n",
        "\n",
        "    return {\n",
        "        \"data\": x_mix,                         # (Tmix,C) normalized by mix stats\n",
        "        \"count\": float(gt_total),\n",
        "        \"meta\": f\"subject{participant}__{scen_name}\",\n",
        "        \"boundary\": boundary,\n",
        "        \"meta_detail\": {\n",
        "            \"subj\": f\"subject{participant}\",\n",
        "            \"actA_name\": exA, \"actB_name\": exB,\n",
        "            \"gtA\": gtA, \"gtB\": gtB,\n",
        "            \"gt_total\": gt_total,\n",
        "        },\n",
        "        \"data_A\": xA,\n",
        "        \"data_B\": xB,\n",
        "        \"T_A\": int(xA.shape[0]),\n",
        "        \"T_B\": int(xB.shape[0]),\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (UNCHANGED)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Helpers for scenario-avg plots (UNCHANGED)\n",
        "# ---------------------------------------------------------------------\n",
        "def get_window_rate_curve(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "\n",
        "    if T <= win_len:\n",
        "        pred_count, rates = predict_count_by_windowing(\n",
        "            model, x_np, fs, win_sec, stride_sec, device, tau=tau, batch_size=batch_size\n",
        "        )\n",
        "        t_cent = np.array([0.5 * (T / float(fs))], dtype=np.float32)\n",
        "        return t_cent, rates.astype(np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "    rates = np.concatenate(rates, axis=0).astype(np.float32)\n",
        "\n",
        "    centers = np.array([(st + 0.5 * win_len) / float(fs) for st in starts], dtype=np.float32)\n",
        "    return centers, rates\n",
        "\n",
        "\n",
        "def resample_1d(y, new_len):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    if y.size == 0:\n",
        "        return np.zeros((new_len,), dtype=np.float32)\n",
        "    if y.size == 1:\n",
        "        return np.full((new_len,), float(y[0]), dtype=np.float32)\n",
        "    x_old = np.linspace(0.0, 1.0, num=y.size, dtype=np.float32)\n",
        "    x_new = np.linspace(0.0, 1.0, num=new_len, dtype=np.float32)\n",
        "    return np.interp(x_new, x_old, y).astype(np.float32)\n",
        "\n",
        "\n",
        "def resample_2d(Y, new_len):\n",
        "    Y = np.asarray(Y, dtype=np.float32)\n",
        "    if Y.shape[0] == 0:\n",
        "        return np.zeros((new_len, Y.shape[1]), dtype=np.float32)\n",
        "    if Y.shape[0] == 1:\n",
        "        return np.repeat(Y, repeats=new_len, axis=0).astype(np.float32)\n",
        "    x_old = np.linspace(0.0, 1.0, num=Y.shape[0], dtype=np.float32)\n",
        "    x_new = np.linspace(0.0, 1.0, num=new_len, dtype=np.float32)\n",
        "    out = []\n",
        "    for d in range(Y.shape[1]):\n",
        "        out.append(np.interp(x_new, x_old, Y[:, d]))\n",
        "    return np.stack(out, axis=1).astype(np.float32)\n",
        "\n",
        "\n",
        "def global_pca2(Z_all):\n",
        "    Z_all = np.asarray(Z_all, dtype=np.float32)\n",
        "    mu = Z_all.mean(axis=0, keepdims=True)\n",
        "    Zc = Z_all - mu\n",
        "    _, _, Vt = np.linalg.svd(Zc, full_matrices=False)\n",
        "    V2 = Vt[:2].astype(np.float32)\n",
        "    return mu.squeeze(0).astype(np.float32), V2\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ PLOTTER (UNCHANGED DESIGN)\n",
        "# ---------------------------------------------------------------------\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable  # ✅ keep duplicated import as you had\n",
        "\n",
        "def plot_scenario_mean_std_two_plots(\n",
        "    save_dir,\n",
        "    scen_name,\n",
        "    win_t, win_mean, win_std,\n",
        "    lat_xy_mean, lat_xy_std,\n",
        "    n_subjects,\n",
        "    dpi=600,\n",
        "):\n",
        "    _ensure_dir(save_dir if save_dir is not None else \"scenario_viz\")\n",
        "    out_dir = save_dir if save_dir is not None else \"scenario_viz\"\n",
        "    scen_tag = _safe_fname(scen_name)\n",
        "\n",
        "    # -------------------------\n",
        "    # (1) Window-level Rep rate mean±std\n",
        "    # -------------------------\n",
        "    fig1 = plt.figure(figsize=(26.0, 9.2))\n",
        "    ax = fig1.gca()\n",
        "\n",
        "    # subtle A/B shading (keep but slightly lighter)\n",
        "    ax.axvspan(0.0, 1.0, alpha=0.035, color=\"k\", lw=0)\n",
        "    ax.axvspan(1.0, 2.0, alpha=0.022, color=\"k\", lw=0)\n",
        "\n",
        "    ax.plot(\n",
        "        win_t, win_mean,\n",
        "        linewidth=7.8,\n",
        "        solid_capstyle=\"round\",\n",
        "        label=\"Window rep rate (mean)\"\n",
        "    )\n",
        "    ax.fill_between(\n",
        "        win_t,\n",
        "        win_mean - win_std, win_mean + win_std,\n",
        "        alpha=0.16,\n",
        "        linewidth=0,\n",
        "        label=\"±1 std\"\n",
        "    )\n",
        "\n",
        "    ax.axvline(\n",
        "        1.0,\n",
        "        linestyle=\"--\",\n",
        "        linewidth=4.2,\n",
        "        label=\"Boundary\"\n",
        "    )\n",
        "\n",
        "    ax.set_xlabel(\"Normalized time (A:0–1, B:1–2)\", fontsize=54, labelpad=10)\n",
        "    ax.set_ylabel(\"Rep rate (reps/s)\", fontsize=54, labelpad=10)\n",
        "    ax.tick_params(axis=\"both\", labelsize=46, width=1.6, length=7)\n",
        "\n",
        "    for sp in ax.spines.values():\n",
        "        sp.set_linewidth(1.6)\n",
        "\n",
        "    ax.legend(\n",
        "        fontsize=42,\n",
        "        frameon=True,\n",
        "        loc=\"upper left\",\n",
        "        borderpad=0.55,\n",
        "        labelspacing=0.35,\n",
        "        handlelength=2.4\n",
        "    )\n",
        "\n",
        "    fig1.tight_layout()\n",
        "    fig1.savefig(os.path.join(out_dir, f\"{scen_tag}__1_win_rep_rate.png\"), dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig1)\n",
        "\n",
        "    # -------------------------\n",
        "    # (2) Latent trajectory PCA2 (gradient line)\n",
        "    # -------------------------\n",
        "    fig2 = plt.figure(figsize=(10.0, 10.0))\n",
        "    ax = fig2.gca()\n",
        "\n",
        "    lat_xy_mean = np.asarray(lat_xy_mean, dtype=np.float32)\n",
        "    T = lat_xy_mean.shape[0]\n",
        "    t_lat = np.linspace(0.0, 2.0, num=T, dtype=np.float32)\n",
        "\n",
        "    pts = lat_xy_mean.reshape(-1, 1, 2)\n",
        "    segs = np.concatenate([pts[:-1], pts[1:]], axis=1)\n",
        "\n",
        "    lc = LineCollection(\n",
        "        segs,\n",
        "        array=t_lat[:-1],\n",
        "        linewidth=2.6,\n",
        "        alpha=0.95\n",
        "    )\n",
        "    ax.add_collection(lc)\n",
        "\n",
        "    ax.set_xlabel(\"PC1\", fontsize=25, labelpad=10)\n",
        "    ax.set_ylabel(\"PC2\", fontsize=25, labelpad=10)\n",
        "    ax.tick_params(axis=\"both\", labelsize=20, width=1.4, length=6)\n",
        "\n",
        "    for sp in ax.spines.values():\n",
        "        sp.set_linewidth(1.4)\n",
        "\n",
        "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
        "    ax.set_anchor(\"C\")\n",
        "\n",
        "    x = lat_xy_mean[:, 0]; y = lat_xy_mean[:, 1]\n",
        "    xmin, xmax = float(np.min(x)), float(np.max(x))\n",
        "    ymin, ymax = float(np.min(y)), float(np.max(y))\n",
        "    cx = 0.5 * (xmin + xmax); cy = 0.5 * (ymin + ymax)\n",
        "    rx = 0.5 * (xmax - xmin); ry = 0.5 * (ymax - ymin)\n",
        "    r = max(rx, ry) * 1.12 + 1e-6\n",
        "    ax.set_xlim(cx - r, cx + r)\n",
        "    ax.set_ylim(cy - r, cy + r)\n",
        "\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"4.5%\", pad=0.10)\n",
        "    cbar = fig2.colorbar(lc, cax=cax)\n",
        "    cbar.set_label(\"Normalized time (0→2)\", fontsize=22, labelpad=10)\n",
        "    cbar.ax.tick_params(labelsize=18, width=1.2, length=5)\n",
        "\n",
        "    fig2.tight_layout()\n",
        "    fig2.savefig(os.path.join(out_dir, f\"{scen_tag}__2_latent_pca2.png\"), dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig2)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Dataset / Collate (UNCHANGED)\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),\n",
        "        \"mask\": torch.stack(masks),\n",
        "        \"count\": torch.stack(counts),\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Model (UNCHANGED)\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)\n",
        "        z = z.transpose(1, 2)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)\n",
        "        x_hat = self.net(zt)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)\n",
        "        amp = F.softplus(out[..., 0])\n",
        "        phase_logits = out[..., 1:]\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        micro_rate_t = amp_t\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"phase_p\": phase_p,\n",
        "            \"phase_logits\": phase_logits,\n",
        "            \"micro_rate_t\": micro_rate_t,\n",
        "            \"rep_rate_t\": rep_rate_t,\n",
        "            \"k_hat\": k_hat,\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Loss utils (UNCHANGED)\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Train (UNCHANGED)\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Main (Scenario-wise LOSO) — MM-Fit\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "\n",
        "        # ✅ MM-Fit meta csv\n",
        "        \"mmfit_meta_csv\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/mmfit_imu_3ex_trials/meta_sw_r_dumbbell_rows_lunges_pushups.csv\",\n",
        "\n",
        "        # ✅ exercises (3) => all directed pairs => 6 scenarios\n",
        "        \"MMFIT_EXERCISES\": [\"pushups\", \"lunges\", \"dumbbell_rows\"],\n",
        "\n",
        "        # optional filters\n",
        "        \"MMFIT_ONLY_REPS\": None,\n",
        "        \"MMFIT_REQUIRE_DEVICE\": \"sw_r\",\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 100,   # ✅ MM-Fit sw_r acc/gyro is typically 100Hz in your pipeline\n",
        "\n",
        "        # Windowing Params\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        \"tau\": 1.0,\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # load meta\n",
        "    meta_all = load_mmfit_meta(\n",
        "        meta_csv_path=CONFIG[\"mmfit_meta_csv\"],\n",
        "        target_exercises=set(CONFIG[\"MMFIT_EXERCISES\"]),\n",
        "        only_reps=CONFIG[\"MMFIT_ONLY_REPS\"],\n",
        "        require_device=CONFIG[\"MMFIT_REQUIRE_DEVICE\"],\n",
        "    )\n",
        "    if len(meta_all) == 0:\n",
        "        print(\"[MM-Fit] meta is empty after filtering. Check meta_csv / filters.\")\n",
        "        return\n",
        "\n",
        "    # build all directed scenarios (A->B), A != B => 6\n",
        "    ex_list = list(CONFIG[\"MMFIT_EXERCISES\"])\n",
        "    MIX_SCENARIOS = []\n",
        "    for A in ex_list:\n",
        "        for B in ex_list:\n",
        "            if A == B:\n",
        "                continue\n",
        "            MIX_SCENARIOS.append((f\"{A}-{B}\", A, B))\n",
        "\n",
        "    # output dir\n",
        "    VIZ_DIR = \"scenario_viz\"\n",
        "    os.makedirs(VIZ_DIR, exist_ok=True)\n",
        "\n",
        "    # resample lengths (fixed for averaging)\n",
        "    NW = 60     # window-rate points per segment (A,B)\n",
        "    NL = 200    # latent points per segment (A,B)\n",
        "\n",
        "    # For each scenario (A->B)\n",
        "    for scen_idx, (scen_name, exA, exB) in enumerate(MIX_SCENARIOS, start=1):\n",
        "        TRAIN_EX = exA\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 100)\n",
        "        print(f\"{scen_idx}. 시나리오: {scen_name}   (TRAIN={TRAIN_EX})\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        # subjects = participants who have at least one A trial\n",
        "        subjA = sorted(meta_all[meta_all[\"exercise\"] == TRAIN_EX][\"participant\"].unique().tolist())\n",
        "\n",
        "        # for scenario, need A and B available (at least 1 trial each)\n",
        "        subjB = set(meta_all[meta_all[\"exercise\"] == exB][\"participant\"].unique().tolist())\n",
        "        subjects = [s for s in subjA if s in subjB]\n",
        "\n",
        "        if len(subjects) == 0:\n",
        "            print(f\"[Skip scenario] No participants have both A={exA} and B={exB}.\")\n",
        "            continue\n",
        "\n",
        "        loso_results = []\n",
        "        scenario_records = []\n",
        "        scenario_diffs = []\n",
        "        scenario_diffs_A = []\n",
        "        scenario_diffs_B = []\n",
        "\n",
        "        # ✅ buffers for TWO plots only\n",
        "        buf_win_concat = []     # list of (2*NW,)\n",
        "        buf_z_concat = []       # list of (Tmix,D)\n",
        "        per_subject_latent = [] # list of {\"zA\",\"zB\"}\n",
        "\n",
        "        for fold_idx, test_pid in enumerate(subjects, start=1):\n",
        "            set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "            # ----------------------------------------\n",
        "            # TRAIN trials: ALL A trials from other subjects\n",
        "            # TEST(A) trials: ALL A trials from held-out subject\n",
        "            # ----------------------------------------\n",
        "            train_meta = meta_all[(meta_all[\"exercise\"] == TRAIN_EX) & (meta_all[\"participant\"] != int(test_pid))].copy()\n",
        "            test_metaA = meta_all[(meta_all[\"exercise\"] == TRAIN_EX) & (meta_all[\"participant\"] == int(test_pid))].copy()\n",
        "\n",
        "            train_trials, _ = prepare_trial_list_mmfit(train_meta, expected_fs=CONFIG[\"fs\"], skip_nonfinite=True, verbose_skip=False)\n",
        "            test_trialsA, _ = prepare_trial_list_mmfit(test_metaA, expected_fs=CONFIG[\"fs\"], skip_nonfinite=True, verbose_skip=False)\n",
        "\n",
        "            if len(train_trials) == 0 or len(test_trialsA) == 0:\n",
        "                continue\n",
        "\n",
        "            train_data = trial_list_to_windows(\n",
        "                train_trials, fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                drop_last=CONFIG[\"drop_last\"]\n",
        "            )\n",
        "            if len(train_data) == 0:\n",
        "                continue\n",
        "\n",
        "            g = torch.Generator()\n",
        "            g.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "            train_loader = DataLoader(\n",
        "                TrialDataset(train_data),\n",
        "                batch_size=CONFIG[\"batch_size\"],\n",
        "                shuffle=True,\n",
        "                collate_fn=collate_variable_length,\n",
        "                generator=g,\n",
        "                num_workers=0\n",
        "            )\n",
        "\n",
        "            # model init\n",
        "            input_ch = train_data[0][\"data\"].shape[1]\n",
        "            model = KAutoCountModel(\n",
        "                input_ch=input_ch,\n",
        "                hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "                latent_dim=CONFIG[\"latent_dim\"],\n",
        "                K_max=CONFIG[\"K_max\"]\n",
        "            ).to(device)\n",
        "\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "            for _ in range(CONFIG[\"epochs\"]):\n",
        "                _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "                scheduler.step()\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            # ----------------------------------------\n",
        "            # (A) LOSO performance on TRAIN_EX (A) — average over ALL A trials of held-out subject\n",
        "            # ----------------------------------------\n",
        "            abs_errs = []\n",
        "            res_str = \"\"\n",
        "\n",
        "            for itemA in test_trialsA:\n",
        "                x_np = itemA[\"data\"]\n",
        "                gtA = float(itemA[\"count\"])\n",
        "                predA, _ = predict_count_by_windowing(\n",
        "                    model, x_np=x_np,\n",
        "                    fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                    device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                    batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "                )\n",
        "                abs_errs.append(abs(predA - gtA))\n",
        "                res_str += f\"[Pred(win): {predA:.1f} / GT: {gtA:.0f}]\"\n",
        "\n",
        "            fold_mae = float(np.mean(abs_errs)) if len(abs_errs) > 0 else None\n",
        "            if fold_mae is not None:\n",
        "                loso_results.append(fold_mae)\n",
        "\n",
        "            # ----------------------------------------\n",
        "            # (B) Scenario eval on mixed A->B for held-out subject (one deterministic A trial + one deterministic B trial)\n",
        "            # ----------------------------------------\n",
        "            mixed_item = build_mixed_ab_trial_mmfit(\n",
        "                participant=int(test_pid),\n",
        "                exA=exA, exB=exB,\n",
        "                config=CONFIG,\n",
        "                meta_df=meta_all\n",
        "            )\n",
        "            if mixed_item is None:\n",
        "                # still print fold line for LOSO(A) if you want; keep minimal\n",
        "                continue\n",
        "\n",
        "            x_mix = mixed_item[\"data\"]\n",
        "            boundary = mixed_item[\"boundary\"]\n",
        "            gt_total = float(mixed_item[\"count\"])\n",
        "            md = mixed_item[\"meta_detail\"]\n",
        "\n",
        "            pred_total, _ = predict_count_by_windowing(\n",
        "                model, x_np=x_mix,\n",
        "                fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            diff = float(pred_total - gt_total)\n",
        "            mae_total = float(abs(diff))\n",
        "            scenario_diffs.append(diff)\n",
        "\n",
        "            # A/B split\n",
        "            xA = mixed_item[\"data_A\"]\n",
        "            xB = mixed_item[\"data_B\"]\n",
        "            gtA = float(md[\"gtA\"])\n",
        "            gtB = float(md[\"gtB\"])\n",
        "\n",
        "            pred_A, _ = predict_count_by_windowing(\n",
        "                model, x_np=xA,\n",
        "                fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            pred_B, _ = predict_count_by_windowing(\n",
        "                model, x_np=xB,\n",
        "                fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            diff_A = float(pred_A - gtA)\n",
        "            diff_B = float(pred_B - gtB)\n",
        "            mae_A = float(abs(diff_A))\n",
        "            mae_B = float(abs(diff_B))\n",
        "            scenario_diffs_A.append(diff_A)\n",
        "            scenario_diffs_B.append(diff_B)\n",
        "\n",
        "            # latent forward on mixed\n",
        "            x_tensor = torch.tensor(x_mix, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                _, z_m, _, aux_m = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "\n",
        "            phase_p_m = aux_m[\"phase_p\"].squeeze(0).detach().cpu().numpy()  # (Tmix,K)\n",
        "            k_hat_m = float(aux_m[\"k_hat\"].item())\n",
        "            ent_m = compute_phase_entropy_mean(phase_p_m)\n",
        "\n",
        "            z_td = z_m.squeeze(0).detach().cpu().numpy()  # (Tmix,D)\n",
        "\n",
        "            scenario_records.append({\n",
        "                \"subj\": f\"subject{test_pid}\",\n",
        "                \"line\": (\n",
        "                    f\"[Scenario] {scen_name} | subject{test_pid} | \"\n",
        "                    f\"{md['actA_name']}({md['gtA']:.0f}) -> {md['actB_name']}({md['gtB']:.0f}) | \"\n",
        "                    f\"GT_total={gt_total:.0f} | Pred_total(win)={pred_total:.2f} | Diff_total={diff:+.2f} | \"\n",
        "                    f\"MAE_total={mae_total:.2f} | \"\n",
        "                    f\"Pred_A={pred_A:.2f} (Diff_A={diff_A:+.2f}, MAE_A={mae_A:.2f}) | \"\n",
        "                    f\"Pred_B={pred_B:.2f} (Diff_B={diff_B:+.2f}, MAE_B={mae_B:.2f}) | \"\n",
        "                    f\"k_hat(full)={k_hat_m:.2f} | ent(full)={ent_m:.3f} | boundary={boundary}\"\n",
        "                )\n",
        "            })\n",
        "\n",
        "            # (1) window-level rep_rate curves: A/B separately -> resample -> concat\n",
        "            _, rA_w = get_window_rate_curve(\n",
        "                model, xA, fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            _, rB_w = get_window_rate_curve(\n",
        "                model, xB, fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            rA_rs = resample_1d(rA_w, NW)\n",
        "            rB_rs = resample_1d(rB_w, NW)\n",
        "            buf_win_concat.append(np.concatenate([rA_rs, rB_rs], axis=0))\n",
        "\n",
        "            # (2) latent: store raw z split for global PCA later\n",
        "            zA = z_td[:boundary, :]\n",
        "            zB = z_td[boundary:, :]\n",
        "            buf_z_concat.append(z_td)\n",
        "            per_subject_latent.append({\"zA\": zA, \"zB\": zB})\n",
        "\n",
        "            # optional: print fold LOSO(A) line in same style (kept minimal)\n",
        "            if fold_mae is not None:\n",
        "                print(f\"Fold {fold_idx:2d} | Test: subject{test_pid} | MAE(A)={fold_mae:.2f} | {res_str}\")\n",
        "\n",
        "        # -------------------------\n",
        "        # Print summaries (same style as your original)\n",
        "        # -------------------------\n",
        "        print(\"-\" * 100)\n",
        "        if len(loso_results) > 0:\n",
        "            print(\"TRain 결과 ->\")\n",
        "            print(\"-\" * 100)\n",
        "            print(f\" >>> Final LOSO Result (Average MAE): {np.mean(loso_results):.3f}\")\n",
        "            print(f\" >>> Standard Deviation: {np.std(loso_results):.3f}\")\n",
        "            print(\"-\" * 100)\n",
        "        else:\n",
        "            print(\"TRain 결과 -> (no folds computed)\")\n",
        "            print(\"-\" * 100)\n",
        "\n",
        "        # per subject scenario lines (stable ordering)\n",
        "        subj2line = {r[\"subj\"]: r[\"line\"] for r in scenario_records}\n",
        "        for pid in subjects:\n",
        "            key = f\"subject{pid}\"\n",
        "            if key in subj2line:\n",
        "                print(subj2line[key])\n",
        "\n",
        "        print(\"\\n\" + \"-\" * 100)\n",
        "        print(f\"시나리오{scen_idx} 결과\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        if len(scenario_diffs) == 0:\n",
        "            print(f\"[{scen_name}] No samples (all skipped). Check availability of A/B trials per subject.\")\n",
        "        else:\n",
        "            diffs = np.array(scenario_diffs, dtype=np.float32)\n",
        "            mae = float(np.mean(np.abs(diffs)))\n",
        "            rmse = float(np.sqrt(np.mean(diffs ** 2)))\n",
        "            print(f\"[TOTAL] [{scen_name}] N={len(diffs):2d} | MAE={mae:.3f} | RMSE={rmse:.3f} | mean(diff)={diffs.mean():+.3f} | std(diff)={diffs.std():.3f}\")\n",
        "\n",
        "            diffsA = np.array(scenario_diffs_A, dtype=np.float32)\n",
        "            maeA = float(np.mean(np.abs(diffsA)))\n",
        "            rmseA = float(np.sqrt(np.mean(diffsA ** 2)))\n",
        "            print(f\"[A]     [{scen_name}] N={len(diffsA):2d} | MAE={maeA:.3f} | RMSE={rmseA:.3f} | mean(diff)={diffsA.mean():+.3f} | std(diff)={diffsA.std():.3f}\")\n",
        "\n",
        "            diffsB = np.array(scenario_diffs_B, dtype=np.float32)\n",
        "            maeB = float(np.mean(np.abs(diffsB)))\n",
        "            rmseB = float(np.sqrt(np.mean(diffsB ** 2)))\n",
        "            print(f\"[B]     [{scen_name}] N={len(diffsB):2d} | MAE={maeB:.3f} | RMSE={rmseB:.3f} | mean(diff)={diffsB.mean():+.3f} | std(diff)={diffsB.std():.3f}\")\n",
        "\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        # -------------------------\n",
        "        # ✅ TWO-PLOT scenario-avg visualization (UNCHANGED)\n",
        "        # -------------------------\n",
        "        n_ok = len(buf_win_concat)\n",
        "        if n_ok == 0:\n",
        "            print(\"[Viz Skip] No scenario samples collected for averaging.\")\n",
        "            continue\n",
        "\n",
        "        # (1) window-level mean±std on normalized axis [0,2]\n",
        "        W = np.stack(buf_win_concat, axis=0)  # (N, 2*NW)\n",
        "        win_mean = W.mean(axis=0)\n",
        "        win_std  = W.std(axis=0)\n",
        "        win_t = np.linspace(0.0, 2.0, num=2*NW, dtype=np.float32)\n",
        "\n",
        "        # (2) latent: global PCA basis from all z\n",
        "        Z_all = np.concatenate(buf_z_concat, axis=0)  # (sumT, D)\n",
        "        mu, V2 = global_pca2(Z_all)\n",
        "\n",
        "        # project each subject, resample A/B to NL then concat -> (2*NL,2)\n",
        "        lat_list = []\n",
        "        for d in per_subject_latent:\n",
        "            zA = d[\"zA\"]; zB = d[\"zB\"]\n",
        "            pcA = (zA - mu) @ V2.T\n",
        "            pcB = (zB - mu) @ V2.T\n",
        "            pcA_rs = resample_2d(pcA, NL)\n",
        "            pcB_rs = resample_2d(pcB, NL)\n",
        "            lat_list.append(np.concatenate([pcA_rs, pcB_rs], axis=0))\n",
        "\n",
        "        LAT = np.stack(lat_list, axis=0)  # (N,2*NL,2)\n",
        "        lat_xy_mean = LAT.mean(axis=0)\n",
        "        lat_xy_std  = LAT.std(axis=0)\n",
        "\n",
        "        plot_scenario_mean_std_two_plots(\n",
        "            save_dir=VIZ_DIR,\n",
        "            scen_name=scen_name,\n",
        "            win_t=win_t, win_mean=win_mean, win_std=win_std,\n",
        "            lat_xy_mean=lat_xy_mean, lat_xy_std=lat_xy_std,\n",
        "            n_subjects=n_ok,\n",
        "            dpi=600\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-7fjaryL91rW",
        "outputId": "7310343a-d7a6-4eba-a54a-f5bb0781d4ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오: pushups-lunges   (TRAIN=pushups)\n",
            "====================================================================================================\n",
            "Fold  1 | Test: subject0 | MAE(A)=1.38 | [Pred(win): 7.3 / GT: 10][Pred(win): 10.0 / GT: 10][Pred(win): 9.8 / GT: 10][Pred(win): 9.5 / GT: 10][Pred(win): 8.4 / GT: 10][Pred(win): 7.5 / GT: 10][Pred(win): 9.7 / GT: 10][Pred(win): 11.9 / GT: 10][Pred(win): 10.5 / GT: 10][Pred(win): 7.5 / GT: 10][Pred(win): 8.2 / GT: 10][Pred(win): 7.8 / GT: 10][Pred(win): 11.4 / GT: 10][Pred(win): 9.3 / GT: 10][Pred(win): 9.0 / GT: 10][Pred(win): 9.1 / GT: 10][Pred(win): 7.6 / GT: 10][Pred(win): 8.2 / GT: 10]\n",
            "Fold  2 | Test: subject1 | MAE(A)=1.71 | [Pred(win): 8.6 / GT: 10][Pred(win): 8.8 / GT: 10][Pred(win): 8.9 / GT: 10][Pred(win): 6.9 / GT: 10][Pred(win): 9.6 / GT: 10][Pred(win): 11.2 / GT: 10][Pred(win): 8.9 / GT: 10][Pred(win): 12.3 / GT: 10][Pred(win): 12.0 / GT: 10][Pred(win): 8.3 / GT: 10][Pred(win): 9.3 / GT: 10][Pred(win): 13.9 / GT: 10][Pred(win): 9.8 / GT: 10][Pred(win): 8.8 / GT: 10][Pred(win): 13.4 / GT: 10][Pred(win): 5.5 / GT: 10][Pred(win): 10.3 / GT: 10][Pred(win): 11.1 / GT: 10]\n",
            "Fold  3 | Test: subject2 | MAE(A)=2.11 | [Pred(win): 8.5 / GT: 10][Pred(win): 7.8 / GT: 10][Pred(win): 7.9 / GT: 10][Pred(win): 9.3 / GT: 10][Pred(win): 6.0 / GT: 10]\n",
            "Fold  4 | Test: subject3 | MAE(A)=2.34 | [Pred(win): 6.9 / GT: 10][Pred(win): 7.3 / GT: 10][Pred(win): 8.8 / GT: 10]\n",
            "Fold  5 | Test: subject4 | MAE(A)=2.96 | [Pred(win): 8.1 / GT: 10][Pred(win): 7.1 / GT: 10][Pred(win): 5.9 / GT: 10]\n",
            "Fold  6 | Test: subject5 | MAE(A)=2.65 | [Pred(win): 9.7 / GT: 10][Pred(win): 15.4 / GT: 10][Pred(win): 13.6 / GT: 10][Pred(win): 11.4 / GT: 10]\n",
            "Fold  7 | Test: subject6 | MAE(A)=2.63 | [Pred(win): 7.9 / GT: 10][Pred(win): 6.8 / GT: 10][Pred(win): 6.8 / GT: 9][Pred(win): 9.0 / GT: 12]\n",
            "Fold  8 | Test: subject7 | MAE(A)=1.79 | [Pred(win): 8.9 / GT: 10][Pred(win): 5.8 / GT: 10][Pred(win): 9.9 / GT: 10]\n",
            "Fold  9 | Test: subject8 | MAE(A)=7.33 | [Pred(win): 14.1 / GT: 10][Pred(win): 18.2 / GT: 10][Pred(win): 16.7 / GT: 7]\n",
            "Fold 10 | Test: subject9 | MAE(A)=2.50 | [Pred(win): 9.9 / GT: 10][Pred(win): 13.0 / GT: 10][Pred(win): 14.4 / GT: 10]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 2.739\n",
            " >>> Standard Deviation: 1.599\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario] pushups-lunges | subject0 | pushups(10) -> lunges(10) | GT_total=20 | Pred_total(win)=19.67 | Diff_total=-0.33 | MAE_total=0.33 | Pred_A=9.10 (Diff_A=-0.90, MAE_A=0.90) | Pred_B=9.88 (Diff_B=-0.12, MAE_B=0.12) | k_hat(full)=1.25 | ent(full)=0.314 | boundary=1187\n",
            "[Scenario] pushups-lunges | subject1 | pushups(10) -> lunges(10) | GT_total=20 | Pred_total(win)=25.54 | Diff_total=+5.54 | MAE_total=5.54 | Pred_A=9.78 (Diff_A=-0.22, MAE_A=0.22) | Pred_B=17.33 (Diff_B=+7.33, MAE_B=7.33) | k_hat(full)=1.13 | ent(full)=0.221 | boundary=1556\n",
            "[Scenario] pushups-lunges | subject2 | pushups(10) -> lunges(10) | GT_total=20 | Pred_total(win)=27.46 | Diff_total=+7.46 | MAE_total=7.46 | Pred_A=10.51 (Diff_A=+0.51, MAE_A=0.51) | Pred_B=16.85 (Diff_B=+6.85, MAE_B=6.85) | k_hat(full)=1.01 | ent(full)=0.034 | boundary=1419\n",
            "[Scenario] pushups-lunges | subject3 | pushups(10) -> lunges(10) | GT_total=20 | Pred_total(win)=18.25 | Diff_total=-1.75 | MAE_total=1.75 | Pred_A=9.90 (Diff_A=-0.10, MAE_A=0.10) | Pred_B=11.08 (Diff_B=+1.08, MAE_B=1.08) | k_hat(full)=1.02 | ent(full)=0.068 | boundary=1123\n",
            "[Scenario] pushups-lunges | subject4 | pushups(10) -> lunges(10) | GT_total=20 | Pred_total(win)=19.98 | Diff_total=-0.02 | MAE_total=0.02 | Pred_A=5.06 (Diff_A=-4.94, MAE_A=4.94) | Pred_B=15.58 (Diff_B=+5.58, MAE_B=5.58) | k_hat(full)=1.10 | ent(full)=0.202 | boundary=1297\n",
            "[Scenario] pushups-lunges | subject5 | pushups(10) -> lunges(10) | GT_total=20 | Pred_total(win)=26.02 | Diff_total=+6.02 | MAE_total=6.02 | Pred_A=8.81 (Diff_A=-1.19, MAE_A=1.19) | Pred_B=16.74 (Diff_B=+6.74, MAE_B=6.74) | k_hat(full)=1.10 | ent(full)=0.178 | boundary=1313\n",
            "[Scenario] pushups-lunges | subject6 | pushups(10) -> lunges(10) | GT_total=20 | Pred_total(win)=25.68 | Diff_total=+5.68 | MAE_total=5.68 | Pred_A=8.97 (Diff_A=-1.03, MAE_A=1.03) | Pred_B=14.21 (Diff_B=+4.21, MAE_B=4.21) | k_hat(full)=1.08 | ent(full)=0.163 | boundary=1403\n",
            "[Scenario] pushups-lunges | subject7 | pushups(10) -> lunges(10) | GT_total=20 | Pred_total(win)=24.28 | Diff_total=+4.28 | MAE_total=4.28 | Pred_A=12.45 (Diff_A=+2.45, MAE_A=2.45) | Pred_B=11.03 (Diff_B=+1.03, MAE_B=1.03) | k_hat(full)=1.18 | ent(full)=0.311 | boundary=1606\n",
            "[Scenario] pushups-lunges | subject8 | pushups(10) -> lunges(10) | GT_total=20 | Pred_total(win)=33.40 | Diff_total=+13.40 | MAE_total=13.40 | Pred_A=13.00 (Diff_A=+3.00, MAE_A=3.00) | Pred_B=20.07 (Diff_B=+10.07, MAE_B=10.07) | k_hat(full)=1.09 | ent(full)=0.183 | boundary=2227\n",
            "[Scenario] pushups-lunges | subject9 | pushups(10) -> lunges(10) | GT_total=20 | Pred_total(win)=31.97 | Diff_total=+11.97 | MAE_total=11.97 | Pred_A=10.05 (Diff_A=+0.05, MAE_A=0.05) | Pred_B=20.84 (Diff_B=+10.84, MAE_B=10.84) | k_hat(full)=1.06 | ent(full)=0.101 | boundary=1759\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오1 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[TOTAL] [pushups-lunges] N=10 | MAE=5.644 | RMSE=7.064 | mean(diff)=+5.225 | std(diff)=4.754\n",
            "[A]     [pushups-lunges] N=10 | MAE=1.441 | RMSE=2.076 | mean(diff)=-0.237 | std(diff)=2.062\n",
            "[B]     [pushups-lunges] N=10 | MAE=5.385 | RMSE=6.451 | mean(diff)=+5.360 | std(diff)=3.589\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "====================================================================================================\n",
            "2. 시나리오: pushups-dumbbell_rows   (TRAIN=pushups)\n",
            "====================================================================================================\n",
            "Fold  1 | Test: subject0 | MAE(A)=1.38 | [Pred(win): 7.3 / GT: 10][Pred(win): 10.0 / GT: 10][Pred(win): 9.8 / GT: 10][Pred(win): 9.5 / GT: 10][Pred(win): 8.4 / GT: 10][Pred(win): 7.5 / GT: 10][Pred(win): 9.7 / GT: 10][Pred(win): 11.9 / GT: 10][Pred(win): 10.5 / GT: 10][Pred(win): 7.5 / GT: 10][Pred(win): 8.2 / GT: 10][Pred(win): 7.8 / GT: 10][Pred(win): 11.4 / GT: 10][Pred(win): 9.3 / GT: 10][Pred(win): 9.0 / GT: 10][Pred(win): 9.1 / GT: 10][Pred(win): 7.6 / GT: 10][Pred(win): 8.2 / GT: 10]\n",
            "Fold  2 | Test: subject1 | MAE(A)=1.71 | [Pred(win): 8.6 / GT: 10][Pred(win): 8.8 / GT: 10][Pred(win): 8.9 / GT: 10][Pred(win): 6.9 / GT: 10][Pred(win): 9.6 / GT: 10][Pred(win): 11.2 / GT: 10][Pred(win): 8.9 / GT: 10][Pred(win): 12.3 / GT: 10][Pred(win): 12.0 / GT: 10][Pred(win): 8.3 / GT: 10][Pred(win): 9.3 / GT: 10][Pred(win): 13.9 / GT: 10][Pred(win): 9.8 / GT: 10][Pred(win): 8.8 / GT: 10][Pred(win): 13.4 / GT: 10][Pred(win): 5.5 / GT: 10][Pred(win): 10.3 / GT: 10][Pred(win): 11.1 / GT: 10]\n",
            "Fold  3 | Test: subject2 | MAE(A)=2.11 | [Pred(win): 8.5 / GT: 10][Pred(win): 7.8 / GT: 10][Pred(win): 7.9 / GT: 10][Pred(win): 9.3 / GT: 10][Pred(win): 6.0 / GT: 10]\n",
            "Fold  4 | Test: subject3 | MAE(A)=2.34 | [Pred(win): 6.9 / GT: 10][Pred(win): 7.3 / GT: 10][Pred(win): 8.8 / GT: 10]\n",
            "Fold  5 | Test: subject4 | MAE(A)=2.96 | [Pred(win): 8.1 / GT: 10][Pred(win): 7.1 / GT: 10][Pred(win): 5.9 / GT: 10]\n",
            "Fold  6 | Test: subject5 | MAE(A)=2.65 | [Pred(win): 9.7 / GT: 10][Pred(win): 15.4 / GT: 10][Pred(win): 13.6 / GT: 10][Pred(win): 11.4 / GT: 10]\n",
            "Fold  7 | Test: subject6 | MAE(A)=2.63 | [Pred(win): 7.9 / GT: 10][Pred(win): 6.8 / GT: 10][Pred(win): 6.8 / GT: 9][Pred(win): 9.0 / GT: 12]\n",
            "Fold  8 | Test: subject7 | MAE(A)=1.79 | [Pred(win): 8.9 / GT: 10][Pred(win): 5.8 / GT: 10][Pred(win): 9.9 / GT: 10]\n",
            "Fold  9 | Test: subject8 | MAE(A)=7.33 | [Pred(win): 14.1 / GT: 10][Pred(win): 18.2 / GT: 10][Pred(win): 16.7 / GT: 7]\n",
            "Fold 10 | Test: subject9 | MAE(A)=2.50 | [Pred(win): 9.9 / GT: 10][Pred(win): 13.0 / GT: 10][Pred(win): 14.4 / GT: 10]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 2.739\n",
            " >>> Standard Deviation: 1.599\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario] pushups-dumbbell_rows | subject0 | pushups(10) -> dumbbell_rows(10) | GT_total=20 | Pred_total(win)=17.83 | Diff_total=-2.17 | MAE_total=2.17 | Pred_A=9.67 (Diff_A=-0.33, MAE_A=0.33) | Pred_B=10.06 (Diff_B=+0.06, MAE_B=0.06) | k_hat(full)=1.21 | ent(full)=0.161 | boundary=1187\n",
            "[Scenario] pushups-dumbbell_rows | subject1 | pushups(10) -> dumbbell_rows(10) | GT_total=20 | Pred_total(win)=21.55 | Diff_total=+1.55 | MAE_total=1.55 | Pred_A=9.32 (Diff_A=-0.68, MAE_A=0.68) | Pred_B=12.75 (Diff_B=+2.75, MAE_B=2.75) | k_hat(full)=1.14 | ent(full)=0.191 | boundary=1556\n",
            "[Scenario] pushups-dumbbell_rows | subject2 | pushups(10) -> dumbbell_rows(11) | GT_total=21 | Pred_total(win)=22.99 | Diff_total=+1.99 | MAE_total=1.99 | Pred_A=12.12 (Diff_A=+2.12, MAE_A=2.12) | Pred_B=11.67 (Diff_B=+0.67, MAE_B=0.67) | k_hat(full)=1.01 | ent(full)=0.020 | boundary=1419\n",
            "[Scenario] pushups-dumbbell_rows | subject3 | pushups(10) -> dumbbell_rows(10) | GT_total=20 | Pred_total(win)=14.66 | Diff_total=-5.34 | MAE_total=5.34 | Pred_A=8.53 (Diff_A=-1.47, MAE_A=1.47) | Pred_B=8.00 (Diff_B=-2.00, MAE_B=2.00) | k_hat(full)=1.01 | ent(full)=0.028 | boundary=1123\n",
            "[Scenario] pushups-dumbbell_rows | subject4 | pushups(10) -> dumbbell_rows(10) | GT_total=20 | Pred_total(win)=14.16 | Diff_total=-5.84 | MAE_total=5.84 | Pred_A=6.39 (Diff_A=-3.61, MAE_A=3.61) | Pred_B=8.48 (Diff_B=-1.52, MAE_B=1.52) | k_hat(full)=1.06 | ent(full)=0.125 | boundary=1297\n",
            "[Scenario] pushups-dumbbell_rows | subject5 | pushups(10) -> dumbbell_rows(12) | GT_total=22 | Pred_total(win)=22.20 | Diff_total=+0.20 | MAE_total=0.20 | Pred_A=12.04 (Diff_A=+2.04, MAE_A=2.04) | Pred_B=10.45 (Diff_B=-1.55, MAE_B=1.55) | k_hat(full)=1.11 | ent(full)=0.189 | boundary=1313\n",
            "[Scenario] pushups-dumbbell_rows | subject6 | pushups(10) -> dumbbell_rows(10) | GT_total=20 | Pred_total(win)=16.87 | Diff_total=-3.13 | MAE_total=3.13 | Pred_A=8.33 (Diff_A=-1.67, MAE_A=1.67) | Pred_B=6.34 (Diff_B=-3.66, MAE_B=3.66) | k_hat(full)=1.17 | ent(full)=0.337 | boundary=1403\n",
            "[Scenario] pushups-dumbbell_rows | subject7 | pushups(10) -> dumbbell_rows(10) | GT_total=20 | Pred_total(win)=19.98 | Diff_total=-0.02 | MAE_total=0.02 | Pred_A=10.05 (Diff_A=+0.05, MAE_A=0.05) | Pred_B=8.60 (Diff_B=-1.40, MAE_B=1.40) | k_hat(full)=1.06 | ent(full)=0.088 | boundary=1606\n",
            "[Scenario] pushups-dumbbell_rows | subject8 | pushups(10) -> dumbbell_rows(10) | GT_total=20 | Pred_total(win)=23.33 | Diff_total=+3.33 | MAE_total=3.33 | Pred_A=15.83 (Diff_A=+5.83, MAE_A=5.83) | Pred_B=7.13 (Diff_B=-2.87, MAE_B=2.87) | k_hat(full)=1.06 | ent(full)=0.123 | boundary=2227\n",
            "[Scenario] pushups-dumbbell_rows | subject9 | pushups(10) -> dumbbell_rows(10) | GT_total=20 | Pred_total(win)=23.21 | Diff_total=+3.21 | MAE_total=3.21 | Pred_A=8.77 (Diff_A=-1.23, MAE_A=1.23) | Pred_B=15.13 (Diff_B=+5.13, MAE_B=5.13) | k_hat(full)=1.06 | ent(full)=0.115 | boundary=1759\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오2 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[TOTAL] [pushups-dumbbell_rows] N=10 | MAE=2.678 | RMSE=3.238 | mean(diff)=-0.622 | std(diff)=3.178\n",
            "[A]     [pushups-dumbbell_rows] N=10 | MAE=1.905 | RMSE=2.506 | mean(diff)=+0.106 | std(diff)=2.504\n",
            "[B]     [pushups-dumbbell_rows] N=10 | MAE=2.163 | RMSE=2.583 | mean(diff)=-0.440 | std(diff)=2.545\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "====================================================================================================\n",
            "3. 시나리오: lunges-pushups   (TRAIN=lunges)\n",
            "====================================================================================================\n",
            "Fold  1 | Test: subject0 | MAE(A)=0.69 | [Pred(win): 10.2 / GT: 10][Pred(win): 8.9 / GT: 10][Pred(win): 10.5 / GT: 10][Pred(win): 9.6 / GT: 10][Pred(win): 9.4 / GT: 10][Pred(win): 9.1 / GT: 10][Pred(win): 10.4 / GT: 10][Pred(win): 9.5 / GT: 10][Pred(win): 10.0 / GT: 10][Pred(win): 10.9 / GT: 10][Pred(win): 10.6 / GT: 10][Pred(win): 9.5 / GT: 10][Pred(win): 11.3 / GT: 10][Pred(win): 11.6 / GT: 10][Pred(win): 11.2 / GT: 10][Pred(win): 10.1 / GT: 10][Pred(win): 9.5 / GT: 10][Pred(win): 9.0 / GT: 10]\n",
            "Fold  2 | Test: subject1 | MAE(A)=0.61 | [Pred(win): 9.0 / GT: 10][Pred(win): 10.6 / GT: 10][Pred(win): 9.3 / GT: 10][Pred(win): 10.8 / GT: 10][Pred(win): 11.1 / GT: 10][Pred(win): 10.0 / GT: 10][Pred(win): 10.5 / GT: 10][Pred(win): 10.0 / GT: 10][Pred(win): 10.8 / GT: 10][Pred(win): 9.7 / GT: 10][Pred(win): 10.5 / GT: 10][Pred(win): 12.0 / GT: 10][Pred(win): 9.8 / GT: 10][Pred(win): 10.3 / GT: 10][Pred(win): 9.4 / GT: 10][Pred(win): 9.3 / GT: 10][Pred(win): 9.7 / GT: 10]\n",
            "Fold  3 | Test: subject2 | MAE(A)=0.13 | [Pred(win): 9.9 / GT: 10][Pred(win): 9.9 / GT: 10][Pred(win): 10.4 / GT: 10][Pred(win): 10.0 / GT: 10][Pred(win): 9.9 / GT: 10][Pred(win): 11.0 / GT: 11]\n",
            "Fold  4 | Test: subject3 | MAE(A)=2.20 | [Pred(win): 8.2 / GT: 10][Pred(win): 7.4 / GT: 10][Pred(win): 7.8 / GT: 10]\n",
            "Fold  5 | Test: subject4 | MAE(A)=1.16 | [Pred(win): 8.9 / GT: 10][Pred(win): 9.3 / GT: 10][Pred(win): 8.4 / GT: 10]\n",
            "Fold  6 | Test: subject5 | MAE(A)=0.74 | [Pred(win): 10.9 / GT: 10][Pred(win): 9.8 / GT: 10][Pred(win): 11.1 / GT: 10]\n",
            "Fold  7 | Test: subject6 | MAE(A)=1.51 | [Pred(win): 9.1 / GT: 10][Pred(win): 8.3 / GT: 10][Pred(win): 12.1 / GT: 14]\n",
            "Fold  8 | Test: subject7 | MAE(A)=0.25 | [Pred(win): 10.2 / GT: 10][Pred(win): 9.6 / GT: 10][Pred(win): 9.8 / GT: 10]\n",
            "Fold  9 | Test: subject8 | MAE(A)=1.22 | [Pred(win): 12.3 / GT: 10][Pred(win): 9.7 / GT: 10][Pred(win): 9.0 / GT: 10]\n",
            "Fold 10 | Test: subject9 | MAE(A)=1.28 | [Pred(win): 11.7 / GT: 10][Pred(win): 10.3 / GT: 9][Pred(win): 10.8 / GT: 10]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 0.978\n",
            " >>> Standard Deviation: 0.590\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario] lunges-pushups | subject0 | lunges(10) -> pushups(10) | GT_total=20 | Pred_total(win)=15.30 | Diff_total=-4.70 | MAE_total=4.70 | Pred_A=10.68 (Diff_A=+0.68, MAE_A=0.68) | Pred_B=4.85 (Diff_B=-5.15, MAE_B=5.15) | k_hat(full)=1.01 | ent(full)=0.018 | boundary=2849\n",
            "[Scenario] lunges-pushups | subject1 | lunges(10) -> pushups(10) | GT_total=20 | Pred_total(win)=15.73 | Diff_total=-4.27 | MAE_total=4.27 | Pred_A=10.30 (Diff_A=+0.30, MAE_A=0.30) | Pred_B=5.66 (Diff_B=-4.34, MAE_B=4.34) | k_hat(full)=1.00 | ent(full)=0.014 | boundary=2642\n",
            "[Scenario] lunges-pushups | subject2 | lunges(10) -> pushups(10) | GT_total=20 | Pred_total(win)=15.89 | Diff_total=-4.11 | MAE_total=4.11 | Pred_A=10.40 (Diff_A=+0.40, MAE_A=0.40) | Pred_B=5.51 (Diff_B=-4.49, MAE_B=4.49) | k_hat(full)=1.00 | ent(full)=0.002 | boundary=2808\n",
            "[Scenario] lunges-pushups | subject3 | lunges(10) -> pushups(10) | GT_total=20 | Pred_total(win)=12.18 | Diff_total=-7.82 | MAE_total=7.82 | Pred_A=8.25 (Diff_A=-1.75, MAE_A=1.75) | Pred_B=4.06 (Diff_B=-5.94, MAE_B=5.94) | k_hat(full)=1.00 | ent(full)=0.008 | boundary=2279\n",
            "[Scenario] lunges-pushups | subject4 | lunges(10) -> pushups(10) | GT_total=20 | Pred_total(win)=13.32 | Diff_total=-6.68 | MAE_total=6.68 | Pred_A=8.88 (Diff_A=-1.12, MAE_A=1.12) | Pred_B=4.45 (Diff_B=-5.55, MAE_B=5.55) | k_hat(full)=1.01 | ent(full)=0.039 | boundary=2609\n",
            "[Scenario] lunges-pushups | subject5 | lunges(10) -> pushups(10) | GT_total=20 | Pred_total(win)=15.55 | Diff_total=-4.45 | MAE_total=4.45 | Pred_A=10.88 (Diff_A=+0.88, MAE_A=0.88) | Pred_B=4.59 (Diff_B=-5.41, MAE_B=5.41) | k_hat(full)=1.02 | ent(full)=0.044 | boundary=3059\n",
            "[Scenario] lunges-pushups | subject6 | lunges(10) -> pushups(10) | GT_total=20 | Pred_total(win)=14.12 | Diff_total=-5.88 | MAE_total=5.88 | Pred_A=9.28 (Diff_A=-0.72, MAE_A=0.72) | Pred_B=5.08 (Diff_B=-4.92, MAE_B=4.92) | k_hat(full)=1.01 | ent(full)=0.036 | boundary=2559\n",
            "[Scenario] lunges-pushups | subject7 | lunges(10) -> pushups(10) | GT_total=20 | Pred_total(win)=16.01 | Diff_total=-3.99 | MAE_total=3.99 | Pred_A=10.29 (Diff_A=+0.29, MAE_A=0.29) | Pred_B=5.81 (Diff_B=-4.19, MAE_B=4.19) | k_hat(full)=1.03 | ent(full)=0.091 | boundary=3079\n",
            "[Scenario] lunges-pushups | subject8 | lunges(10) -> pushups(10) | GT_total=20 | Pred_total(win)=20.12 | Diff_total=+0.12 | MAE_total=0.12 | Pred_A=12.21 (Diff_A=+2.21, MAE_A=2.21) | Pred_B=7.91 (Diff_B=-2.09, MAE_B=2.09) | k_hat(full)=1.01 | ent(full)=0.031 | boundary=3413\n",
            "[Scenario] lunges-pushups | subject9 | lunges(10) -> pushups(10) | GT_total=20 | Pred_total(win)=18.29 | Diff_total=-1.71 | MAE_total=1.71 | Pred_A=11.92 (Diff_A=+1.92, MAE_A=1.92) | Pred_B=6.44 (Diff_B=-3.56, MAE_B=3.56) | k_hat(full)=1.01 | ent(full)=0.018 | boundary=3346\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오3 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[TOTAL] [lunges-pushups] N=10 | MAE=4.373 | RMSE=4.860 | mean(diff)=-4.349 | std(diff)=2.169\n",
            "[A]     [lunges-pushups] N=10 | MAE=1.029 | RMSE=1.226 | mean(diff)=+0.310 | std(diff)=1.186\n",
            "[B]     [lunges-pushups] N=10 | MAE=4.563 | RMSE=4.686 | mean(diff)=-4.563 | std(diff)=1.066\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "====================================================================================================\n",
            "4. 시나리오: lunges-dumbbell_rows   (TRAIN=lunges)\n",
            "====================================================================================================\n",
            "Fold  1 | Test: subject0 | MAE(A)=0.69 | [Pred(win): 10.2 / GT: 10][Pred(win): 8.9 / GT: 10][Pred(win): 10.5 / GT: 10][Pred(win): 9.6 / GT: 10][Pred(win): 9.4 / GT: 10][Pred(win): 9.1 / GT: 10][Pred(win): 10.4 / GT: 10][Pred(win): 9.5 / GT: 10][Pred(win): 10.0 / GT: 10][Pred(win): 10.9 / GT: 10][Pred(win): 10.6 / GT: 10][Pred(win): 9.5 / GT: 10][Pred(win): 11.3 / GT: 10][Pred(win): 11.6 / GT: 10][Pred(win): 11.2 / GT: 10][Pred(win): 10.1 / GT: 10][Pred(win): 9.5 / GT: 10][Pred(win): 9.0 / GT: 10]\n",
            "Fold  2 | Test: subject1 | MAE(A)=0.61 | [Pred(win): 9.0 / GT: 10][Pred(win): 10.6 / GT: 10][Pred(win): 9.3 / GT: 10][Pred(win): 10.8 / GT: 10][Pred(win): 11.1 / GT: 10][Pred(win): 10.0 / GT: 10][Pred(win): 10.5 / GT: 10][Pred(win): 10.0 / GT: 10][Pred(win): 10.8 / GT: 10][Pred(win): 9.7 / GT: 10][Pred(win): 10.5 / GT: 10][Pred(win): 12.0 / GT: 10][Pred(win): 9.8 / GT: 10][Pred(win): 10.3 / GT: 10][Pred(win): 9.4 / GT: 10][Pred(win): 9.3 / GT: 10][Pred(win): 9.7 / GT: 10]\n",
            "Fold  3 | Test: subject2 | MAE(A)=0.13 | [Pred(win): 9.9 / GT: 10][Pred(win): 9.9 / GT: 10][Pred(win): 10.4 / GT: 10][Pred(win): 10.0 / GT: 10][Pred(win): 9.9 / GT: 10][Pred(win): 11.0 / GT: 11]\n",
            "Fold  4 | Test: subject3 | MAE(A)=2.20 | [Pred(win): 8.2 / GT: 10][Pred(win): 7.4 / GT: 10][Pred(win): 7.8 / GT: 10]\n",
            "Fold  5 | Test: subject4 | MAE(A)=1.16 | [Pred(win): 8.9 / GT: 10][Pred(win): 9.3 / GT: 10][Pred(win): 8.4 / GT: 10]\n",
            "Fold  6 | Test: subject5 | MAE(A)=0.74 | [Pred(win): 10.9 / GT: 10][Pred(win): 9.8 / GT: 10][Pred(win): 11.1 / GT: 10]\n",
            "Fold  7 | Test: subject6 | MAE(A)=1.51 | [Pred(win): 9.1 / GT: 10][Pred(win): 8.3 / GT: 10][Pred(win): 12.1 / GT: 14]\n",
            "Fold  8 | Test: subject7 | MAE(A)=0.25 | [Pred(win): 10.2 / GT: 10][Pred(win): 9.6 / GT: 10][Pred(win): 9.8 / GT: 10]\n",
            "Fold  9 | Test: subject8 | MAE(A)=1.22 | [Pred(win): 12.3 / GT: 10][Pred(win): 9.7 / GT: 10][Pred(win): 9.0 / GT: 10]\n",
            "Fold 10 | Test: subject9 | MAE(A)=1.28 | [Pred(win): 11.7 / GT: 10][Pred(win): 10.3 / GT: 9][Pred(win): 10.8 / GT: 10]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 0.978\n",
            " >>> Standard Deviation: 0.590\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario] lunges-dumbbell_rows | subject0 | lunges(10) -> dumbbell_rows(10) | GT_total=20 | Pred_total(win)=18.36 | Diff_total=-1.64 | MAE_total=1.64 | Pred_A=10.40 (Diff_A=+0.40, MAE_A=0.40) | Pred_B=8.00 (Diff_B=-2.00, MAE_B=2.00) | k_hat(full)=1.02 | ent(full)=0.065 | boundary=2849\n",
            "[Scenario] lunges-dumbbell_rows | subject1 | lunges(10) -> dumbbell_rows(10) | GT_total=20 | Pred_total(win)=16.19 | Diff_total=-3.81 | MAE_total=3.81 | Pred_A=9.39 (Diff_A=-0.61, MAE_A=0.61) | Pred_B=6.92 (Diff_B=-3.08, MAE_B=3.08) | k_hat(full)=1.04 | ent(full)=0.098 | boundary=2642\n",
            "[Scenario] lunges-dumbbell_rows | subject2 | lunges(10) -> dumbbell_rows(11) | GT_total=21 | Pred_total(win)=16.63 | Diff_total=-4.37 | MAE_total=4.37 | Pred_A=10.09 (Diff_A=+0.09, MAE_A=0.09) | Pred_B=6.53 (Diff_B=-4.47, MAE_B=4.47) | k_hat(full)=1.00 | ent(full)=0.008 | boundary=2808\n",
            "[Scenario] lunges-dumbbell_rows | subject3 | lunges(10) -> dumbbell_rows(10) | GT_total=20 | Pred_total(win)=13.40 | Diff_total=-6.60 | MAE_total=6.60 | Pred_A=8.51 (Diff_A=-1.49, MAE_A=1.49) | Pred_B=4.96 (Diff_B=-5.04, MAE_B=5.04) | k_hat(full)=1.00 | ent(full)=0.001 | boundary=2279\n",
            "[Scenario] lunges-dumbbell_rows | subject4 | lunges(10) -> dumbbell_rows(10) | GT_total=20 | Pred_total(win)=13.01 | Diff_total=-6.99 | MAE_total=6.99 | Pred_A=8.91 (Diff_A=-1.09, MAE_A=1.09) | Pred_B=4.14 (Diff_B=-5.86, MAE_B=5.86) | k_hat(full)=1.01 | ent(full)=0.020 | boundary=2609\n",
            "[Scenario] lunges-dumbbell_rows | subject5 | lunges(10) -> dumbbell_rows(12) | GT_total=22 | Pred_total(win)=18.00 | Diff_total=-4.00 | MAE_total=4.00 | Pred_A=10.86 (Diff_A=+0.86, MAE_A=0.86) | Pred_B=7.00 (Diff_B=-5.00, MAE_B=5.00) | k_hat(full)=1.02 | ent(full)=0.053 | boundary=3059\n",
            "[Scenario] lunges-dumbbell_rows | subject6 | lunges(10) -> dumbbell_rows(10) | GT_total=20 | Pred_total(win)=13.93 | Diff_total=-6.07 | MAE_total=6.07 | Pred_A=9.18 (Diff_A=-0.82, MAE_A=0.82) | Pred_B=4.89 (Diff_B=-5.11, MAE_B=5.11) | k_hat(full)=1.00 | ent(full)=0.014 | boundary=2559\n",
            "[Scenario] lunges-dumbbell_rows | subject7 | lunges(10) -> dumbbell_rows(10) | GT_total=20 | Pred_total(win)=17.05 | Diff_total=-2.95 | MAE_total=2.95 | Pred_A=10.32 (Diff_A=+0.32, MAE_A=0.32) | Pred_B=6.59 (Diff_B=-3.41, MAE_B=3.41) | k_hat(full)=1.01 | ent(full)=0.024 | boundary=3079\n",
            "[Scenario] lunges-dumbbell_rows | subject8 | lunges(10) -> dumbbell_rows(10) | GT_total=20 | Pred_total(win)=17.74 | Diff_total=-2.26 | MAE_total=2.26 | Pred_A=12.18 (Diff_A=+2.18, MAE_A=2.18) | Pred_B=5.59 (Diff_B=-4.41, MAE_B=4.41) | k_hat(full)=1.01 | ent(full)=0.022 | boundary=3413\n",
            "[Scenario] lunges-dumbbell_rows | subject9 | lunges(10) -> dumbbell_rows(10) | GT_total=20 | Pred_total(win)=19.02 | Diff_total=-0.98 | MAE_total=0.98 | Pred_A=11.65 (Diff_A=+1.65, MAE_A=1.65) | Pred_B=7.35 (Diff_B=-2.65, MAE_B=2.65) | k_hat(full)=1.01 | ent(full)=0.033 | boundary=3346\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오4 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[TOTAL] [lunges-dumbbell_rows] N=10 | MAE=3.968 | RMSE=4.432 | mean(diff)=-3.968 | std(diff)=1.975\n",
            "[A]     [lunges-dumbbell_rows] N=10 | MAE=0.950 | RMSE=1.137 | mean(diff)=+0.149 | std(diff)=1.127\n",
            "[B]     [lunges-dumbbell_rows] N=10 | MAE=4.104 | RMSE=4.272 | mean(diff)=-4.104 | std(diff)=1.186\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "====================================================================================================\n",
            "5. 시나리오: dumbbell_rows-pushups   (TRAIN=dumbbell_rows)\n",
            "====================================================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-574915811.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-574915811.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0mtrain_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_trial_list_mmfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_fs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_nonfinite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_skip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m             \u001b[0mtest_trialsA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_trial_list_mmfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_metaA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_fs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_nonfinite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_skip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_trials\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_trialsA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-574915811.py\u001b[0m in \u001b[0;36mprepare_trial_list_mmfit\u001b[0;34m(meta_rows, expected_fs, skip_nonfinite, verbose_skip)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_npz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_npz_X_and_fs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpz_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfallback_fs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mskipped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"npz_load_fail\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnpz_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-574915811.py\u001b[0m in \u001b[0;36m_load_npz_X_and_fs\u001b[0;34m(npz_path, fallback_fs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_load_npz_X_and_fs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpz_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfallback_fs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpz_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"X\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0m_ZIP_SUFFIX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb'PK\\x05\\x06'\u001b[0m  \u001b[0;31m# empty zip files start with this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mmagic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmagic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No data left in file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# A1 Pairwise Activity Transfer — mm-Fit (ALL combinations)  ✅\n",
        "#\n",
        "# - Train: activity A windows (window-proxy supervision)\n",
        "# - Test : activity B full trial -> windowing inference\n",
        "#\n",
        "# ✅ Visualization UNCHANGED:\n",
        "#   (1) Train-PCA basis -> project Test (overlay)  [fixed xlim/ylim once]\n",
        "#   (2) pred_rate(t) vs gt_rate overlay\n",
        "#\n",
        "# ✅ Runs ALL ordered pairs (A -> B, A!=B)\n",
        "#\n",
        "# ------------------------------------------------------------\n",
        "# Assumption:\n",
        "# - You already have mm-Fit extracted as:\n",
        "#     MMFIT_OUT_DIR/\n",
        "#       npz/*.npz\n",
        "#       meta_*.csv   (or meta.csv)\n",
        "# - Each meta row corresponds to one trial and contains:\n",
        "#     activity/exercise label, participant/subject id, repetition count, npz file path (or filename)\n",
        "# - Each npz contains time-series array:\n",
        "#     key in {\"X\",\"x\",\"data\",\"imu\",\"signal\"} with shape (T,C)\n",
        "#\n",
        "# If your meta/npz schema differs, adjust only the small \"schema mapping\" part.\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 0) Small helpers (dir / downsample / PCA)\n",
        "# ---------------------------------------------------------------------\n",
        "def _ensure_dir(path: str):\n",
        "    if path is None or path == \"\":\n",
        "        return\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "def _downsample_by_index(X, t, max_points=2000):\n",
        "    X = np.asarray(X)\n",
        "    t = np.asarray(t)\n",
        "    n = len(t)\n",
        "    if n <= max_points:\n",
        "        return X, t\n",
        "    idx = np.linspace(0, n - 1, max_points).astype(np.int64)\n",
        "    return X[idx], t[idx]\n",
        "\n",
        "\n",
        "def _fit_pca_basis(z_tD, n_comp=2, eps=1e-6):\n",
        "    \"\"\"\n",
        "    z_tD: (T,D)\n",
        "    returns: mu(D,), sig(D,), W(D,n_comp)\n",
        "    \"\"\"\n",
        "    Z = np.asarray(z_tD, dtype=np.float32)\n",
        "    mu = Z.mean(axis=0)\n",
        "    sig = Z.std(axis=0) + eps\n",
        "    Zs = (Z - mu) / sig\n",
        "    C = (Zs.T @ Zs) / max(1, (Zs.shape[0] - 1))\n",
        "    U, S, Vt = np.linalg.svd(C, full_matrices=False)\n",
        "    W = U[:, :n_comp]\n",
        "    return mu.astype(np.float32), sig.astype(np.float32), W.astype(np.float32)\n",
        "\n",
        "\n",
        "def _pca_project(z_tD, mu, sig, W):\n",
        "    Z = np.asarray(z_tD, dtype=np.float32)\n",
        "    Zs = (Z - mu) / sig\n",
        "    return (Zs @ W).astype(np.float32)\n",
        "\n",
        "\n",
        "def _compute_ref_xlim_ylim_from_train_test_latents(train_z_tD, test_z_tD, pad_ratio=0.05,\n",
        "                                                   q_low=1.0, q_high=99.0):\n",
        "    \"\"\"\n",
        "    ✅ 0-centered symmetric axis\n",
        "    - PCA basis fit on TRAIN\n",
        "    - project TRAIN/TEST\n",
        "    - robust range by percentiles (q_low~q_high)\n",
        "    - make limits symmetric around 0\n",
        "    \"\"\"\n",
        "    mu, sig, W = _fit_pca_basis(train_z_tD, n_comp=2)\n",
        "    P_tr = _pca_project(train_z_tD, mu, sig, W)\n",
        "    P_te = _pca_project(test_z_tD,  mu, sig, W)\n",
        "\n",
        "    all_x = np.concatenate([P_tr[:, 0], P_te[:, 0]], axis=0)\n",
        "    all_y = np.concatenate([P_tr[:, 1], P_te[:, 1]], axis=0)\n",
        "\n",
        "    # robust min/max (outlier 완화). outlier 신경 안 쓰면 q_low=0, q_high=100로 둬도 됨\n",
        "    x_lo, x_hi = np.percentile(all_x, [q_low, q_high])\n",
        "    y_lo, y_hi = np.percentile(all_y, [q_low, q_high])\n",
        "\n",
        "    Lx = max(abs(float(x_lo)), abs(float(x_hi)))\n",
        "    Ly = max(abs(float(y_lo)), abs(float(y_hi)))\n",
        "\n",
        "    # padding (range 비례)\n",
        "    Lx = Lx * (1.0 + float(pad_ratio))\n",
        "    Ly = Ly * (1.0 + float(pad_ratio))\n",
        "\n",
        "    return (-Lx, +Lx), (-Ly, +Ly)\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) mm-Fit Loading (meta.csv + npz)\n",
        "# ---------------------------------------------------------------------\n",
        "def _find_first_existing(paths):\n",
        "    for p in paths:\n",
        "        if p is not None and os.path.isfile(p):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "\n",
        "def _infer_col(df_cols, candidates):\n",
        "    cols_lower = {c.lower(): c for c in df_cols}\n",
        "    for cand in candidates:\n",
        "        if cand.lower() in cols_lower:\n",
        "            return cols_lower[cand.lower()]\n",
        "    # partial match\n",
        "    for c in df_cols:\n",
        "        cl = c.lower()\n",
        "        for cand in candidates:\n",
        "            if cand.lower() in cl:\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "\n",
        "def load_mmfit_meta(MMFIT_OUT_DIR):\n",
        "    \"\"\"\n",
        "    meta_*.csv 또는 meta.csv를 찾아서 concat.\n",
        "    \"\"\"\n",
        "    meta_paths = sorted(glob.glob(os.path.join(MMFIT_OUT_DIR, \"meta*.csv\")))\n",
        "    if len(meta_paths) == 0:\n",
        "        raise FileNotFoundError(f\"[ERROR] No meta*.csv found in: {MMFIT_OUT_DIR}\")\n",
        "\n",
        "    dfs = []\n",
        "    for p in meta_paths:\n",
        "        try:\n",
        "            df = pd.read_csv(p)\n",
        "            if len(df) > 0:\n",
        "                df[\"__meta_src__\"] = os.path.basename(p)\n",
        "                dfs.append(df)\n",
        "        except Exception as e:\n",
        "            print(f\"[Warning] meta read failed: {p} -> {e}\")\n",
        "\n",
        "    if len(dfs) == 0:\n",
        "        raise RuntimeError(\"[ERROR] meta csv files exist but all failed/empty.\")\n",
        "\n",
        "    meta = pd.concat(dfs, axis=0, ignore_index=True)\n",
        "    return meta\n",
        "\n",
        "\n",
        "def load_mmfit_trials_from_meta(MMFIT_OUT_DIR, meta_df, fs_default=100, npz_subdir=\"npz\"):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      activities: sorted list of unique activity names\n",
        "      participants: sorted list of participant IDs (string)\n",
        "      trials: list[dict] with keys:\n",
        "        data (T,C) float32  [trial-wise zscore normalized]\n",
        "        count float\n",
        "        meta str\n",
        "        subj str\n",
        "        act str\n",
        "        fs int\n",
        "        raw_path str\n",
        "    \"\"\"\n",
        "    npz_dir = os.path.join(MMFIT_OUT_DIR, npz_subdir)\n",
        "    if not os.path.isdir(npz_dir):\n",
        "        raise FileNotFoundError(f\"[ERROR] npz dir not found: {npz_dir}\")\n",
        "\n",
        "    # ---- robust column mapping ----\n",
        "    col_act = _infer_col(meta_df.columns, [\"activity\", \"exercise\", \"label\", \"act\", \"activity_name\", \"class\"])\n",
        "    col_sub = _infer_col(meta_df.columns, [\"participant\", \"subject\", \"user\", \"pid\", \"subj\", \"person\"])\n",
        "    col_cnt = _infer_col(meta_df.columns, [\"count\", \"reps\", \"rep\", \"repetition\", \"rep_count\", \"total_reps\", \"n_reps\"])\n",
        "    col_fs  = _infer_col(meta_df.columns, [\"fs\", \"freq\", \"sampling_rate\", \"hz\"])\n",
        "    col_pth = _infer_col(meta_df.columns, [\"npz_path\", \"path\", \"file\", \"filename\", \"npz_file\", \"npz\"])\n",
        "\n",
        "    if col_act is None or col_sub is None or col_cnt is None:\n",
        "        raise ValueError(\n",
        "            \"[ERROR] meta.csv must contain activity/participant/count columns.\\n\"\n",
        "            f\"  - found: act={col_act}, subj={col_sub}, count={col_cnt}, fs={col_fs}, path={col_pth}\\n\"\n",
        "            \"  - please rename columns or add mapping in load_mmfit_trials_from_meta().\"\n",
        "        )\n",
        "\n",
        "    trials = []\n",
        "    for i, row in meta_df.iterrows():\n",
        "        act = str(row[col_act])\n",
        "        subj = str(row[col_sub])\n",
        "        cnt = float(row[col_cnt])\n",
        "\n",
        "        fs = fs_default\n",
        "        if col_fs is not None:\n",
        "            try:\n",
        "                fs = int(row[col_fs])\n",
        "            except:\n",
        "                fs = fs_default\n",
        "\n",
        "        # resolve npz path\n",
        "        npz_path = None\n",
        "        if col_pth is not None and pd.notna(row[col_pth]):\n",
        "            p = str(row[col_pth])\n",
        "            if os.path.isabs(p) and os.path.isfile(p):\n",
        "                npz_path = p\n",
        "            else:\n",
        "                # relative or filename\n",
        "                cand = os.path.join(MMFIT_OUT_DIR, p)\n",
        "                if os.path.isfile(cand):\n",
        "                    npz_path = cand\n",
        "                else:\n",
        "                    cand2 = os.path.join(npz_dir, os.path.basename(p))\n",
        "                    if os.path.isfile(cand2):\n",
        "                        npz_path = cand2\n",
        "\n",
        "        if npz_path is None:\n",
        "            # fallback: try to find by pattern (subj + act)\n",
        "            # NOTE: your extractor naming convention may differ.\n",
        "            patt1 = os.path.join(npz_dir, f\"*{subj}*{act}*.npz\")\n",
        "            hits = sorted(glob.glob(patt1))\n",
        "            if len(hits) > 0:\n",
        "                npz_path = hits[0]\n",
        "\n",
        "        if npz_path is None or not os.path.isfile(npz_path):\n",
        "            # skip silently (meta row without file)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            npz = np.load(npz_path, allow_pickle=True)\n",
        "            X = None\n",
        "            for k in [\"X\", \"x\", \"data\", \"imu\", \"signal\"]:\n",
        "                if k in npz.files:\n",
        "                    X = npz[k]\n",
        "                    break\n",
        "            if X is None:\n",
        "                # try first array-like\n",
        "                for k in npz.files:\n",
        "                    arr = npz[k]\n",
        "                    if isinstance(arr, np.ndarray) and arr.ndim == 2:\n",
        "                        X = arr\n",
        "                        break\n",
        "            if X is None:\n",
        "                continue\n",
        "\n",
        "            X = np.asarray(X, dtype=np.float32)\n",
        "            if X.ndim != 2:\n",
        "                continue\n",
        "\n",
        "            # trial-wise z-score\n",
        "            mean = X.mean(axis=0)\n",
        "            std = X.std(axis=0) + 1e-6\n",
        "            Xn = (X - mean) / std\n",
        "\n",
        "            trials.append({\n",
        "                \"data\": Xn,                 # (T,C)\n",
        "                \"count\": float(cnt),         # total reps\n",
        "                \"meta\": f\"{subj}_{act}__row{i}\",\n",
        "                \"subj\": subj,\n",
        "                \"act\": act,\n",
        "                \"fs\": fs,\n",
        "                \"raw_path\": npz_path\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"[Warning] npz load failed: {npz_path} -> {e}\")\n",
        "            continue\n",
        "\n",
        "    if len(trials) == 0:\n",
        "        raise RuntimeError(\"[ERROR] No trials loaded. Check meta columns / npz paths / npz contents.\")\n",
        "\n",
        "    activities = sorted(list({t[\"act\"] for t in trials}))\n",
        "    participants = sorted(list({t[\"subj\"] for t in trials}))\n",
        "    return activities, participants, trials\n",
        "\n",
        "\n",
        "def filter_trials_by_activity(trials, act_name):\n",
        "    return [t for t in trials if t[\"act\"] == act_name]\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (same)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]  # (T,C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return float(pred_count)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N,C,win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count)\n",
        "\n",
        "\n",
        "def predict_count_and_rates_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        rate = float(rate_hat.item())\n",
        "        pred_count = rate * total_dur\n",
        "        t_centers = np.array([0.5 * total_dur], dtype=np.float32)\n",
        "        rates = np.array([rate], dtype=np.float32)\n",
        "        return float(pred_count), t_centers, rates, float(total_dur)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    centers = np.array([st + 0.5 * win_len for st in starts], dtype=np.float32) / float(fs)\n",
        "\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N,C,win_len)\n",
        "\n",
        "    rates_list = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates_list.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates_list, axis=0).astype(np.float32)  # (N,)\n",
        "    pred_count = float(rates.mean() * total_dur)\n",
        "    return float(pred_count), centers, rates, float(total_dur)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.8) Dataset / Collate\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model (same)\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B,D,T)\n",
        "        z = z.transpose(1, 2)      # (B,T,D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B,D,T)\n",
        "        x_hat = self.net(zt)       # (B,C,T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        micro_rate_t = amp_t             # (B,T)\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,)\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6) # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"phase_p\": phase_p,          # (B,T,K)\n",
        "            \"rep_rate_t\": rep_rate_t,    # (B,T)\n",
        "            \"k_hat\": k_hat,              # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils (same)\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean()\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)  # (T,K)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4.5) A1 Plots (UNCHANGED)\n",
        "# ---------------------------------------------------------------------\n",
        "def plot_pca2d_train_test_overlay(\n",
        "    train_z_tD, train_t_sec,\n",
        "    test_z_tD, test_t_sec,\n",
        "    save_path,\n",
        "    dpi=600,\n",
        "    max_points=2000,\n",
        "    show_colorbar=True,\n",
        "    tick_pad=0,\n",
        "    point_size=10,\n",
        "    alpha_test=0.9,\n",
        "    alpha_train=0.35,\n",
        "    train_lw=2.0,\n",
        "    fixed_xlim=None,\n",
        "    fixed_ylim=None\n",
        "):\n",
        "    train_z_tD = np.asarray(train_z_tD, dtype=np.float32)\n",
        "    train_t_sec = np.asarray(train_t_sec, dtype=np.float32)\n",
        "    test_z_tD  = np.asarray(test_z_tD,  dtype=np.float32)\n",
        "    test_t_sec = np.asarray(test_t_sec, dtype=np.float32)\n",
        "\n",
        "    train_z_tD, train_t_sec = _downsample_by_index(train_z_tD, train_t_sec, max_points)\n",
        "    test_z_tD,  test_t_sec  = _downsample_by_index(test_z_tD,  test_t_sec,  max_points)\n",
        "\n",
        "    test_c = (test_t_sec - test_t_sec.min()) / max(1e-6, (test_t_sec.max() - test_t_sec.min()))\n",
        "\n",
        "    mu, sig, W = _fit_pca_basis(train_z_tD, n_comp=2)\n",
        "    P_tr = _pca_project(train_z_tD, mu, sig, W)\n",
        "    P_te = _pca_project(test_z_tD,  mu, sig, W)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(6.0, 6.0))\n",
        "\n",
        "    ax.scatter(P_tr[:, 0], P_tr[:, 1], s=point_size, alpha=0.25, color=\"0.6\", label=\"Train (ref)\")\n",
        "    sc = ax.scatter(P_te[:, 0], P_te[:, 1], c=test_c, s=point_size, alpha=alpha_test, cmap=\"viridis\", label=\"Test (proj)\")\n",
        "\n",
        "    ax.set_xlabel(\"PC1\", fontsize=16)\n",
        "    ax.set_ylabel(\"PC2\", fontsize=16)\n",
        "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=14, pad=tick_pad)\n",
        "    ax.grid(False)\n",
        "\n",
        "    if fixed_xlim is not None and fixed_ylim is not None:\n",
        "        ax.set_xlim(float(fixed_xlim[0]), float(fixed_xlim[1]))\n",
        "        ax.set_ylim(float(fixed_ylim[0]), float(fixed_ylim[1]))\n",
        "    else:\n",
        "        all_x = np.concatenate([P_tr[:, 0], P_te[:, 0]], axis=0)\n",
        "        all_y = np.concatenate([P_tr[:, 1], P_te[:, 1]], axis=0)\n",
        "\n",
        "        x_min, x_max = float(all_x.min()), float(all_x.max())\n",
        "        y_min, y_max = float(all_y.min()), float(all_y.max())\n",
        "\n",
        "        x_pad = 0.05 * max(1e-6, (x_max - x_min))\n",
        "        y_pad = 0.05 * max(1e-6, (y_max - y_min))\n",
        "\n",
        "        ax.set_xlim(x_min - x_pad, x_max + x_pad)\n",
        "        ax.set_ylim(y_min - y_pad, y_max + y_pad)\n",
        "\n",
        "    if show_colorbar:\n",
        "        divider = make_axes_locatable(ax)\n",
        "        cax = divider.append_axes(\"right\", size=\"4.5%\", pad=0.10)\n",
        "        cbar = fig.colorbar(sc, cax=cax)\n",
        "        cbar.set_label(\"Normalized time (0→1)\", fontsize=14, labelpad=10)\n",
        "        cbar.ax.tick_params(labelsize=12, pad=tick_pad)\n",
        "        cbar.outline.set_linewidth(1.2)\n",
        "\n",
        "    _ensure_dir(os.path.dirname(save_path))\n",
        "    fig.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def plot_predrate_vs_gtrate(\n",
        "    t_sec,\n",
        "    pred_rate,\n",
        "    gt_rate,\n",
        "    save_path,\n",
        "    dpi=600,\n",
        "    tick_pad=0,\n",
        "    smooth_sigma=1.0,\n",
        "    lw_pred=4.0,\n",
        "    lw_gt=4.0,\n",
        "    legend_loc=\"best\"\n",
        "):\n",
        "    t_sec = np.asarray(t_sec, dtype=np.float32)\n",
        "    pred_rate = np.asarray(pred_rate, dtype=np.float32)\n",
        "\n",
        "    if smooth_sigma is not None and smooth_sigma > 0:\n",
        "        pred_plot = gaussian_filter1d(pred_rate, sigma=float(smooth_sigma))\n",
        "    else:\n",
        "        pred_plot = pred_rate\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(8.5, 5.5))\n",
        "\n",
        "    ax.plot(\n",
        "        t_sec, pred_plot,\n",
        "        linewidth=lw_pred,\n",
        "        label=\"Pred rate (window-level)\"\n",
        "    )\n",
        "\n",
        "    ax.axhline(\n",
        "        y=float(gt_rate),\n",
        "        linestyle=\"--\",\n",
        "        linewidth=lw_gt,\n",
        "        label=\"GT rate (trial-level)\"\n",
        "    )\n",
        "\n",
        "    ax.set_xlabel(\"Time (s)\", fontsize=34)\n",
        "    ax.set_ylabel(\"Repetition rate (reps/s)\", fontsize=34)\n",
        "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=24, pad=tick_pad)\n",
        "    ax.grid(False)\n",
        "    ax.legend(loc=legend_loc, frameon=False, fontsize=18)\n",
        "\n",
        "    _ensure_dir(os.path.dirname(save_path))\n",
        "    fig.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train (same)\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        rate_hat, _, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Main — mm-Fit ALL combinations\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "\n",
        "        # ✅ EDIT THIS\n",
        "        \"MMFIT_OUT_DIR\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/mmfit_imu_3ex_trials\",\n",
        "\n",
        "        # If your extractor put npz under different folder name:\n",
        "        \"npz_subdir\": \"npz\",\n",
        "\n",
        "        # sampling rate (if meta lacks fs col)\n",
        "        \"fs\": 100,\n",
        "\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "\n",
        "        # windowing\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # loss\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # outputs\n",
        "        \"plot_dir\": \"/content/A1_mmfit_pairwise_A1viz\",\n",
        "\n",
        "        # plot policy\n",
        "        # - each pair selects ONE \"picked trial\" for A1 plots.\n",
        "        #   \"worst\" is usually more diagnostic for failure-like alignment issues.\n",
        "        \"pick_policy\": \"worst\",   # \"worst\" | \"best\" | \"median\"\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    _ensure_dir(CONFIG[\"plot_dir\"])\n",
        "\n",
        "    # -----------------------------\n",
        "    # Load meta + trials\n",
        "    # -----------------------------\n",
        "    meta = load_mmfit_meta(CONFIG[\"MMFIT_OUT_DIR\"])\n",
        "    activities, participants, all_trials = load_mmfit_trials_from_meta(\n",
        "        MMFIT_OUT_DIR=CONFIG[\"MMFIT_OUT_DIR\"],\n",
        "        meta_df=meta,\n",
        "        fs_default=CONFIG[\"fs\"],\n",
        "        npz_subdir=CONFIG[\"npz_subdir\"]\n",
        "    )\n",
        "\n",
        "    print(f\"[mm-Fit] Loaded trials={len(all_trials)} | activities={activities} | participants={participants}\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # Build all ordered pairs (A->B, A!=B)\n",
        "    # -----------------------------\n",
        "    exp_dirs = []\n",
        "    for a in activities:\n",
        "        for b in activities:\n",
        "            if a == b:\n",
        "                continue\n",
        "            exp_dirs.append((a, b))\n",
        "\n",
        "    # ✅ Reference axis limits (compute once)\n",
        "    REF_XLIM, REF_YLIM = None, None\n",
        "    REF_PAIR = exp_dirs[0] if len(exp_dirs) > 0 else None\n",
        "    REF_SUBJ = participants[0] if len(participants) > 0 else None\n",
        "\n",
        "    final_blocks = []\n",
        "    exp_idx = 0\n",
        "\n",
        "    for train_act, test_act in exp_dirs:\n",
        "        exp_idx += 1\n",
        "        set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "        train_trials = filter_trials_by_activity(all_trials, train_act)\n",
        "        test_trials  = filter_trials_by_activity(all_trials, test_act)\n",
        "\n",
        "        if len(train_trials) == 0 or len(test_trials) == 0:\n",
        "            print(f\"[Skip] Empty trials: train_act={train_act}, test_act={test_act}\")\n",
        "            continue\n",
        "\n",
        "        # fs policy:\n",
        "        # - training uses CONFIG[\"fs\"] (assumed consistent after extraction)\n",
        "        # - if your trials have mixed fs, you should resample first (not done here to keep logic unchanged).\n",
        "        fs = CONFIG[\"fs\"]\n",
        "\n",
        "        # windowize train\n",
        "        train_windows = trial_list_to_windows(\n",
        "            train_trials,\n",
        "            fs=fs,\n",
        "            win_sec=CONFIG[\"win_sec\"],\n",
        "            stride_sec=CONFIG[\"stride_sec\"],\n",
        "            drop_last=CONFIG[\"drop_last\"]\n",
        "        )\n",
        "        if len(train_windows) == 0:\n",
        "            print(f\"[Skip] Empty train windows: train_act={train_act}\")\n",
        "            continue\n",
        "\n",
        "        # check channel consistency\n",
        "        input_ch = train_windows[0][\"data\"].shape[1]\n",
        "        ok = True\n",
        "        for t in train_trials + test_trials:\n",
        "            if t[\"data\"].shape[1] != input_ch:\n",
        "                ok = False\n",
        "                break\n",
        "        if not ok:\n",
        "            print(f\"[Skip] Channel mismatch in pair {train_act}->{test_act}. (C differs across trials)\")\n",
        "            continue\n",
        "\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(CONFIG[\"seed\"])\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_windows),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            generator=g,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # evaluate on test trials\n",
        "        block_lines = []\n",
        "        abs_errs = []\n",
        "        per_trial = {}\n",
        "\n",
        "        for item in test_trials:\n",
        "            subj = item[\"subj\"]\n",
        "            x_np = item[\"data\"]\n",
        "            gt = float(item[\"count\"])\n",
        "\n",
        "            pred = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=fs,\n",
        "                win_sec=CONFIG[\"win_sec\"],\n",
        "                stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device,\n",
        "                tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            diff = pred - gt\n",
        "            ae = abs(diff)\n",
        "            abs_errs.append(ae)\n",
        "\n",
        "            # full-trial k_hat/entropy\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "\n",
        "            trial_key = item[\"meta\"]\n",
        "            per_trial[trial_key] = {\"ae\": ae, \"pred\": pred, \"gt\": gt, \"item\": item}\n",
        "\n",
        "            line = (f\"{trial_key} | subj={subj} | GT={gt:.0f} | Pred(win)={pred:.2f} | Diff={diff:+.2f} | \"\n",
        "                    f\"k_hat(full)={k_hat:.2f} | phase_entropy(full)={ent:.3f}\")\n",
        "            block_lines.append(line)\n",
        "\n",
        "        mae = float(np.mean(abs_errs)) if len(abs_errs) > 0 else float(\"nan\")\n",
        "\n",
        "        # pick one trial for A1 plots\n",
        "        keys = list(per_trial.keys())\n",
        "        if len(keys) == 0:\n",
        "            continue\n",
        "\n",
        "        policy = CONFIG.get(\"pick_policy\", \"worst\")\n",
        "        if policy == \"best\":\n",
        "            pick_key = sorted(keys, key=lambda k: per_trial[k][\"ae\"])[0]\n",
        "        elif policy == \"median\":\n",
        "            ks = sorted(keys, key=lambda k: per_trial[k][\"ae\"])\n",
        "            pick_key = ks[len(ks)//2]\n",
        "        else:  # \"worst\"\n",
        "            pick_key = sorted(keys, key=lambda k: per_trial[k][\"ae\"], reverse=True)[0]\n",
        "\n",
        "        pick_item = per_trial[pick_key][\"item\"]\n",
        "        pick_subj = pick_item[\"subj\"]\n",
        "\n",
        "        # ------------------\n",
        "        # ✅ REF axis set once (first pair + first participant if exists)\n",
        "        # ------------------\n",
        "        if REF_XLIM is None and REF_YLIM is None and REF_PAIR is not None:\n",
        "            ref_train_act, ref_test_act = REF_PAIR\n",
        "            if (train_act == ref_train_act) and (test_act == ref_test_act):\n",
        "                ref_train_trials = filter_trials_by_activity(all_trials, ref_train_act)\n",
        "                ref_test_trials  = filter_trials_by_activity(all_trials, ref_test_act)\n",
        "\n",
        "                # try first participant\n",
        "                ref_train_item = next((it for it in ref_train_trials if it[\"subj\"] == REF_SUBJ), None)\n",
        "                ref_test_item  = next((it for it in ref_test_trials  if it[\"subj\"] == REF_SUBJ), None)\n",
        "\n",
        "                # fallback\n",
        "                if ref_train_item is None:\n",
        "                    ref_train_item = ref_train_trials[0]\n",
        "                if ref_test_item is None:\n",
        "                    ref_test_item = ref_test_trials[0]\n",
        "\n",
        "                xtr_ref = torch.tensor(ref_train_item[\"data\"], dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "                xte_ref = torch.tensor(ref_test_item[\"data\"],  dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "                with torch.no_grad():\n",
        "                    ztr_ref = model.encoder(xtr_ref).squeeze(0).detach().cpu().numpy()\n",
        "                    zte_ref = model.encoder(xte_ref).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "                REF_XLIM, REF_YLIM = _compute_ref_xlim_ylim_from_train_test_latents(ztr_ref, zte_ref, pad_ratio=0.05)\n",
        "                print(f\"[REF AXIS] Using REF pair {ref_train_act}->{ref_test_act}, subj={REF_SUBJ} xlim={REF_XLIM}, ylim={REF_YLIM}\")\n",
        "\n",
        "        # ------------------\n",
        "        # A1 visualization for picked trial\n",
        "        # ------------------\n",
        "        # need one train trial (same subject if possible) + picked test trial\n",
        "        train_item = next((it for it in train_trials if it[\"subj\"] == pick_subj), None)\n",
        "        if train_item is None:\n",
        "            train_item = train_trials[0]\n",
        "        test_item = pick_item\n",
        "\n",
        "        xtr = torch.tensor(train_item[\"data\"], dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "        xte = torch.tensor(test_item[\"data\"],  dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            ztr = model.encoder(xtr).squeeze(0).detach().cpu().numpy()\n",
        "            zte = model.encoder(xte).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "        ttr = np.arange(ztr.shape[0], dtype=np.float32) / float(fs)\n",
        "        tte = np.arange(zte.shape[0], dtype=np.float32) / float(fs)\n",
        "\n",
        "        safe_train = str(train_act).replace(\" \", \"_\").replace(\"/\", \"_\")\n",
        "        safe_test  = str(test_act).replace(\" \", \"_\").replace(\"/\", \"_\")\n",
        "        safe_subj  = str(pick_subj).replace(\" \", \"_\").replace(\"/\", \"_\")\n",
        "\n",
        "        pca_path = os.path.join(\n",
        "            CONFIG[\"plot_dir\"],\n",
        "            f\"A1_PCA_train{safe_train}_test{safe_test}_{safe_subj}.png\"\n",
        "        )\n",
        "        plot_pca2d_train_test_overlay(\n",
        "            train_z_tD=ztr, train_t_sec=ttr,\n",
        "            test_z_tD=zte,  test_t_sec=tte,\n",
        "            save_path=pca_path,\n",
        "            dpi=600,\n",
        "            max_points=2000,\n",
        "            show_colorbar=True,\n",
        "            tick_pad=0,\n",
        "            point_size=10,\n",
        "            alpha_test=0.9,\n",
        "            alpha_train=0.35,\n",
        "            train_lw=2.0,\n",
        "            fixed_xlim=REF_XLIM,\n",
        "            fixed_ylim=REF_YLIM\n",
        "        )\n",
        "\n",
        "        x_np = test_item[\"data\"]\n",
        "        gt_count = float(test_item[\"count\"])\n",
        "        pred_count, t_centers, rates, total_dur = predict_count_and_rates_by_windowing(\n",
        "            model,\n",
        "            x_np=x_np,\n",
        "            fs=fs,\n",
        "            win_sec=CONFIG[\"win_sec\"],\n",
        "            stride_sec=CONFIG[\"stride_sec\"],\n",
        "            device=device,\n",
        "            tau=CONFIG.get(\"tau\", 1.0),\n",
        "            batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "        )\n",
        "        gt_rate = gt_count / max(1e-6, total_dur)\n",
        "\n",
        "        rate_path = os.path.join(\n",
        "            CONFIG[\"plot_dir\"],\n",
        "            f\"A1_Rate_train{safe_train}_test{safe_test}_{safe_subj}.png\"\n",
        "        )\n",
        "        plot_predrate_vs_gtrate(\n",
        "            t_sec=t_centers,\n",
        "            pred_rate=rates,\n",
        "            gt_rate=gt_rate,\n",
        "            save_path=rate_path,\n",
        "            dpi=600,\n",
        "            tick_pad=0,\n",
        "            smooth_sigma=1.0\n",
        "        )\n",
        "\n",
        "        final_blocks.append({\n",
        "            \"idx\": exp_idx,\n",
        "            \"title\": f\"{train_act} -> {test_act}\",\n",
        "            \"lines\": block_lines,\n",
        "            \"mae\": mae,\n",
        "            \"picked_subject_for_plot\": pick_subj,\n",
        "            \"picked_trial_meta\": pick_key,\n",
        "            \"plot_dir\": CONFIG[\"plot_dir\"]\n",
        "        })\n",
        "\n",
        "        print(f\"[Done] {exp_idx:03d} {train_act} -> {test_act} | MAE={mae:.3f} | pick={pick_key}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 110)\n",
        "    print(\"A1 Pairwise Activity Transfer Results (mm-Fit ALL combinations, windowing inference)\")\n",
        "    print(\"=\" * 110)\n",
        "\n",
        "    if len(final_blocks) == 0:\n",
        "        print(\"[No results] meta/npz 로딩 또는 activity 필터링을 확인해줘.\")\n",
        "        print(\"=\" * 110)\n",
        "        return\n",
        "\n",
        "    # summary print\n",
        "    for b in final_blocks:\n",
        "        print(f\"\\n{b['idx']}. {b['title']}  |  MAE={b['mae']:.3f}\")\n",
        "        print(f\"   -> A1 plots saved for: subj={b['picked_subject_for_plot']} | trial={b['picked_trial_meta']}\")\n",
        "        print(f\"   -> dir: {b['plot_dir']}\")\n",
        "        # 너무 길면 주석 처리 가능\n",
        "        for ln in b[\"lines\"][:20]:\n",
        "            print(ln)\n",
        "        if len(b[\"lines\"]) > 20:\n",
        "            print(f\"... ({len(b['lines'])-20} more lines)\")\n",
        "\n",
        "    maes = [b[\"mae\"] for b in final_blocks if np.isfinite(b[\"mae\"])]\n",
        "    if len(maes) > 0:\n",
        "        print(\"\\n\" + \"-\" * 110)\n",
        "        print(f\"Overall Avg MAE={float(np.mean(maes)):.3f} | Std={float(np.std(maes)):.3f} | #pairs={len(maes)}\")\n",
        "        print(\"-\" * 110)\n",
        "\n",
        "    print(\"=\" * 110)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os1j8Hb2MpE7",
        "outputId": "3aee561e-c779-4bfe-bbcb-aefac9e8bb87"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[mm-Fit] Loaded trials=189 | activities=['dumbbell_rows', 'lunges', 'pushups'] | participants=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
            "[REF AXIS] Using REF pair dumbbell_rows->lunges, subj=0 xlim=(-7.368919578552251, 7.368919578552251), ylim=(-11.295863071918486, 11.295863071918486)\n",
            "[Done] 001 dumbbell_rows -> lunges | MAE=6.118 | pick=8_lunges__row174\n",
            "[Done] 002 dumbbell_rows -> pushups | MAE=1.781 | pick=8_pushups__row173\n",
            "[Done] 003 lunges -> dumbbell_rows | MAE=3.848 | pick=4_dumbbell_rows__row122\n",
            "[Done] 004 lunges -> pushups | MAE=4.226 | pick=6_pushups__row155\n",
            "[Done] 005 pushups -> dumbbell_rows | MAE=1.734 | pick=0_dumbbell_rows__row76\n",
            "[Done] 006 pushups -> lunges | MAE=6.285 | pick=0_lunges__row92\n",
            "\n",
            "==============================================================================================================\n",
            "A1 Pairwise Activity Transfer Results (mm-Fit ALL combinations, windowing inference)\n",
            "==============================================================================================================\n",
            "\n",
            "1. dumbbell_rows -> lunges  |  MAE=6.118\n",
            "   -> A1 plots saved for: subj=8 | trial=8_lunges__row174\n",
            "   -> dir: /content/A1_mmfit_pairwise_A1viz\n",
            "2_lunges__row2 | subj=2 | GT=10 | Pred(win)=16.62 | Diff=+6.62 | k_hat(full)=1.04 | phase_entropy(full)=0.082\n",
            "2_lunges__row3 | subj=2 | GT=10 | Pred(win)=18.15 | Diff=+8.15 | k_hat(full)=1.05 | phase_entropy(full)=0.080\n",
            "2_lunges__row4 | subj=2 | GT=10 | Pred(win)=16.56 | Diff=+6.56 | k_hat(full)=1.03 | phase_entropy(full)=0.072\n",
            "0_lunges__row11 | subj=0 | GT=10 | Pred(win)=14.48 | Diff=+4.48 | k_hat(full)=1.17 | phase_entropy(full)=0.314\n",
            "0_lunges__row12 | subj=0 | GT=10 | Pred(win)=14.05 | Diff=+4.05 | k_hat(full)=1.08 | phase_entropy(full)=0.184\n",
            "0_lunges__row13 | subj=0 | GT=10 | Pred(win)=18.07 | Diff=+8.07 | k_hat(full)=1.03 | phase_entropy(full)=0.061\n",
            "1_lunges__row21 | subj=1 | GT=10 | Pred(win)=13.61 | Diff=+3.61 | k_hat(full)=1.20 | phase_entropy(full)=0.299\n",
            "1_lunges__row22 | subj=1 | GT=10 | Pred(win)=15.91 | Diff=+5.91 | k_hat(full)=1.08 | phase_entropy(full)=0.128\n",
            "1_lunges__row23 | subj=1 | GT=10 | Pred(win)=16.25 | Diff=+6.25 | k_hat(full)=1.04 | phase_entropy(full)=0.067\n",
            "0_lunges__row30 | subj=0 | GT=10 | Pred(win)=14.64 | Diff=+4.64 | k_hat(full)=1.06 | phase_entropy(full)=0.127\n",
            "0_lunges__row31 | subj=0 | GT=10 | Pred(win)=14.85 | Diff=+4.85 | k_hat(full)=1.02 | phase_entropy(full)=0.038\n",
            "0_lunges__row32 | subj=0 | GT=10 | Pred(win)=14.03 | Diff=+4.03 | k_hat(full)=1.01 | phase_entropy(full)=0.029\n",
            "1_lunges__row39 | subj=1 | GT=10 | Pred(win)=17.40 | Diff=+7.40 | k_hat(full)=1.07 | phase_entropy(full)=0.127\n",
            "1_lunges__row40 | subj=1 | GT=10 | Pred(win)=18.40 | Diff=+8.40 | k_hat(full)=1.04 | phase_entropy(full)=0.090\n",
            "2_lunges__row47 | subj=2 | GT=10 | Pred(win)=16.02 | Diff=+6.02 | k_hat(full)=1.05 | phase_entropy(full)=0.110\n",
            "2_lunges__row48 | subj=2 | GT=10 | Pred(win)=17.65 | Diff=+7.65 | k_hat(full)=1.04 | phase_entropy(full)=0.073\n",
            "2_lunges__row49 | subj=2 | GT=11 | Pred(win)=19.12 | Diff=+8.12 | k_hat(full)=1.06 | phase_entropy(full)=0.095\n",
            "0_lunges__row56 | subj=0 | GT=10 | Pred(win)=15.72 | Diff=+5.72 | k_hat(full)=1.07 | phase_entropy(full)=0.118\n",
            "0_lunges__row57 | subj=0 | GT=10 | Pred(win)=16.57 | Diff=+6.57 | k_hat(full)=1.06 | phase_entropy(full)=0.081\n",
            "0_lunges__row58 | subj=0 | GT=10 | Pred(win)=15.49 | Diff=+5.49 | k_hat(full)=1.04 | phase_entropy(full)=0.062\n",
            "... (42 more lines)\n",
            "\n",
            "2. dumbbell_rows -> pushups  |  MAE=1.781\n",
            "   -> A1 plots saved for: subj=8 | trial=8_pushups__row173\n",
            "   -> dir: /content/A1_mmfit_pairwise_A1viz\n",
            "2_pushups__row0 | subj=2 | GT=10 | Pred(win)=7.75 | Diff=-2.25 | k_hat(full)=1.07 | phase_entropy(full)=0.126\n",
            "2_pushups__row1 | subj=2 | GT=10 | Pred(win)=9.00 | Diff=-1.00 | k_hat(full)=1.03 | phase_entropy(full)=0.063\n",
            "0_pushups__row8 | subj=0 | GT=10 | Pred(win)=6.27 | Diff=-3.73 | k_hat(full)=1.01 | phase_entropy(full)=0.024\n",
            "0_pushups__row9 | subj=0 | GT=10 | Pred(win)=8.41 | Diff=-1.59 | k_hat(full)=1.01 | phase_entropy(full)=0.037\n",
            "0_pushups__row10 | subj=0 | GT=10 | Pred(win)=9.81 | Diff=-0.19 | k_hat(full)=1.06 | phase_entropy(full)=0.123\n",
            "1_pushups__row18 | subj=1 | GT=10 | Pred(win)=8.55 | Diff=-1.45 | k_hat(full)=1.03 | phase_entropy(full)=0.077\n",
            "1_pushups__row19 | subj=1 | GT=10 | Pred(win)=9.02 | Diff=-0.98 | k_hat(full)=1.04 | phase_entropy(full)=0.080\n",
            "1_pushups__row20 | subj=1 | GT=10 | Pred(win)=7.96 | Diff=-2.04 | k_hat(full)=1.06 | phase_entropy(full)=0.114\n",
            "0_pushups__row27 | subj=0 | GT=10 | Pred(win)=10.58 | Diff=+0.58 | k_hat(full)=1.06 | phase_entropy(full)=0.120\n",
            "0_pushups__row28 | subj=0 | GT=10 | Pred(win)=9.41 | Diff=-0.59 | k_hat(full)=1.02 | phase_entropy(full)=0.052\n",
            "0_pushups__row29 | subj=0 | GT=10 | Pred(win)=9.37 | Diff=-0.63 | k_hat(full)=1.03 | phase_entropy(full)=0.061\n",
            "1_pushups__row36 | subj=1 | GT=10 | Pred(win)=8.81 | Diff=-1.19 | k_hat(full)=1.07 | phase_entropy(full)=0.138\n",
            "1_pushups__row37 | subj=1 | GT=10 | Pred(win)=9.17 | Diff=-0.83 | k_hat(full)=1.05 | phase_entropy(full)=0.081\n",
            "1_pushups__row38 | subj=1 | GT=10 | Pred(win)=8.79 | Diff=-1.21 | k_hat(full)=1.01 | phase_entropy(full)=0.035\n",
            "2_pushups__row44 | subj=2 | GT=10 | Pred(win)=7.45 | Diff=-2.55 | k_hat(full)=1.12 | phase_entropy(full)=0.196\n",
            "2_pushups__row45 | subj=2 | GT=10 | Pred(win)=9.89 | Diff=-0.11 | k_hat(full)=1.03 | phase_entropy(full)=0.069\n",
            "2_pushups__row46 | subj=2 | GT=10 | Pred(win)=7.12 | Diff=-2.88 | k_hat(full)=1.02 | phase_entropy(full)=0.051\n",
            "0_pushups__row53 | subj=0 | GT=10 | Pred(win)=8.69 | Diff=-1.31 | k_hat(full)=1.06 | phase_entropy(full)=0.097\n",
            "0_pushups__row54 | subj=0 | GT=10 | Pred(win)=11.27 | Diff=+1.27 | k_hat(full)=1.04 | phase_entropy(full)=0.095\n",
            "0_pushups__row55 | subj=0 | GT=10 | Pred(win)=10.60 | Diff=+0.60 | k_hat(full)=1.06 | phase_entropy(full)=0.115\n",
            "... (44 more lines)\n",
            "\n",
            "3. lunges -> dumbbell_rows  |  MAE=3.848\n",
            "   -> A1 plots saved for: subj=4 | trial=4_dumbbell_rows__row122\n",
            "   -> dir: /content/A1_mmfit_pairwise_A1viz\n",
            "2_dumbbell_rows__row5 | subj=2 | GT=11 | Pred(win)=6.46 | Diff=-4.54 | k_hat(full)=1.01 | phase_entropy(full)=0.025\n",
            "2_dumbbell_rows__row6 | subj=2 | GT=10 | Pred(win)=5.69 | Diff=-4.31 | k_hat(full)=1.00 | phase_entropy(full)=0.008\n",
            "2_dumbbell_rows__row7 | subj=2 | GT=11 | Pred(win)=5.75 | Diff=-5.25 | k_hat(full)=1.00 | phase_entropy(full)=0.003\n",
            "0_dumbbell_rows__row14 | subj=0 | GT=10 | Pred(win)=7.45 | Diff=-2.55 | k_hat(full)=1.00 | phase_entropy(full)=0.001\n",
            "0_dumbbell_rows__row15 | subj=0 | GT=10 | Pred(win)=7.39 | Diff=-2.61 | k_hat(full)=1.00 | phase_entropy(full)=0.007\n",
            "0_dumbbell_rows__row16 | subj=0 | GT=10 | Pred(win)=7.58 | Diff=-2.42 | k_hat(full)=1.00 | phase_entropy(full)=0.006\n",
            "0_dumbbell_rows__row17 | subj=0 | GT=10 | Pred(win)=6.20 | Diff=-3.80 | k_hat(full)=1.00 | phase_entropy(full)=0.001\n",
            "1_dumbbell_rows__row24 | subj=1 | GT=10 | Pred(win)=6.65 | Diff=-3.35 | k_hat(full)=1.01 | phase_entropy(full)=0.034\n",
            "1_dumbbell_rows__row25 | subj=1 | GT=10 | Pred(win)=6.12 | Diff=-3.88 | k_hat(full)=1.00 | phase_entropy(full)=0.002\n",
            "1_dumbbell_rows__row26 | subj=1 | GT=10 | Pred(win)=6.10 | Diff=-3.90 | k_hat(full)=1.01 | phase_entropy(full)=0.044\n",
            "0_dumbbell_rows__row33 | subj=0 | GT=10 | Pred(win)=6.81 | Diff=-3.19 | k_hat(full)=1.00 | phase_entropy(full)=0.002\n",
            "0_dumbbell_rows__row34 | subj=0 | GT=10 | Pred(win)=6.21 | Diff=-3.79 | k_hat(full)=1.00 | phase_entropy(full)=0.010\n",
            "0_dumbbell_rows__row35 | subj=0 | GT=10 | Pred(win)=6.79 | Diff=-3.21 | k_hat(full)=1.00 | phase_entropy(full)=0.002\n",
            "1_dumbbell_rows__row41 | subj=1 | GT=10 | Pred(win)=6.78 | Diff=-3.22 | k_hat(full)=1.00 | phase_entropy(full)=0.005\n",
            "1_dumbbell_rows__row42 | subj=1 | GT=10 | Pred(win)=6.12 | Diff=-3.88 | k_hat(full)=1.00 | phase_entropy(full)=0.004\n",
            "1_dumbbell_rows__row43 | subj=1 | GT=10 | Pred(win)=5.91 | Diff=-4.09 | k_hat(full)=1.00 | phase_entropy(full)=0.001\n",
            "2_dumbbell_rows__row50 | subj=2 | GT=10 | Pred(win)=5.66 | Diff=-4.34 | k_hat(full)=1.01 | phase_entropy(full)=0.019\n",
            "2_dumbbell_rows__row51 | subj=2 | GT=10 | Pred(win)=5.10 | Diff=-4.90 | k_hat(full)=1.00 | phase_entropy(full)=0.003\n",
            "2_dumbbell_rows__row52 | subj=2 | GT=10 | Pred(win)=4.76 | Diff=-5.24 | k_hat(full)=1.00 | phase_entropy(full)=0.013\n",
            "0_dumbbell_rows__row59 | subj=0 | GT=10 | Pred(win)=7.28 | Diff=-2.72 | k_hat(full)=1.00 | phase_entropy(full)=0.001\n",
            "... (43 more lines)\n",
            "\n",
            "4. lunges -> pushups  |  MAE=4.226\n",
            "   -> A1 plots saved for: subj=6 | trial=6_pushups__row155\n",
            "   -> dir: /content/A1_mmfit_pairwise_A1viz\n",
            "2_pushups__row0 | subj=2 | GT=10 | Pred(win)=5.06 | Diff=-4.94 | k_hat(full)=1.01 | phase_entropy(full)=0.019\n",
            "2_pushups__row1 | subj=2 | GT=10 | Pred(win)=4.44 | Diff=-5.56 | k_hat(full)=1.00 | phase_entropy(full)=0.004\n",
            "0_pushups__row8 | subj=0 | GT=10 | Pred(win)=4.37 | Diff=-5.63 | k_hat(full)=1.00 | phase_entropy(full)=0.002\n",
            "0_pushups__row9 | subj=0 | GT=10 | Pred(win)=5.23 | Diff=-4.77 | k_hat(full)=1.00 | phase_entropy(full)=0.003\n",
            "0_pushups__row10 | subj=0 | GT=10 | Pred(win)=5.90 | Diff=-4.10 | k_hat(full)=1.00 | phase_entropy(full)=0.015\n",
            "1_pushups__row18 | subj=1 | GT=10 | Pred(win)=5.48 | Diff=-4.52 | k_hat(full)=1.00 | phase_entropy(full)=0.014\n",
            "1_pushups__row19 | subj=1 | GT=10 | Pred(win)=5.32 | Diff=-4.68 | k_hat(full)=1.00 | phase_entropy(full)=0.001\n",
            "1_pushups__row20 | subj=1 | GT=10 | Pred(win)=4.82 | Diff=-5.18 | k_hat(full)=1.00 | phase_entropy(full)=0.009\n",
            "0_pushups__row27 | subj=0 | GT=10 | Pred(win)=5.44 | Diff=-4.56 | k_hat(full)=1.00 | phase_entropy(full)=0.013\n",
            "0_pushups__row28 | subj=0 | GT=10 | Pred(win)=6.09 | Diff=-3.91 | k_hat(full)=1.00 | phase_entropy(full)=0.006\n",
            "0_pushups__row29 | subj=0 | GT=10 | Pred(win)=5.40 | Diff=-4.60 | k_hat(full)=1.00 | phase_entropy(full)=0.004\n",
            "1_pushups__row36 | subj=1 | GT=10 | Pred(win)=5.23 | Diff=-4.77 | k_hat(full)=1.01 | phase_entropy(full)=0.022\n",
            "1_pushups__row37 | subj=1 | GT=10 | Pred(win)=5.54 | Diff=-4.46 | k_hat(full)=1.00 | phase_entropy(full)=0.003\n",
            "1_pushups__row38 | subj=1 | GT=10 | Pred(win)=5.75 | Diff=-4.25 | k_hat(full)=1.00 | phase_entropy(full)=0.001\n",
            "2_pushups__row44 | subj=2 | GT=10 | Pred(win)=5.24 | Diff=-4.76 | k_hat(full)=1.00 | phase_entropy(full)=0.017\n",
            "2_pushups__row45 | subj=2 | GT=10 | Pred(win)=5.66 | Diff=-4.34 | k_hat(full)=1.00 | phase_entropy(full)=0.004\n",
            "2_pushups__row46 | subj=2 | GT=10 | Pred(win)=4.47 | Diff=-5.53 | k_hat(full)=1.00 | phase_entropy(full)=0.004\n",
            "0_pushups__row53 | subj=0 | GT=10 | Pred(win)=5.14 | Diff=-4.86 | k_hat(full)=1.00 | phase_entropy(full)=0.008\n",
            "0_pushups__row54 | subj=0 | GT=10 | Pred(win)=6.39 | Diff=-3.61 | k_hat(full)=1.00 | phase_entropy(full)=0.012\n",
            "0_pushups__row55 | subj=0 | GT=10 | Pred(win)=6.41 | Diff=-3.59 | k_hat(full)=1.00 | phase_entropy(full)=0.013\n",
            "... (44 more lines)\n",
            "\n",
            "5. pushups -> dumbbell_rows  |  MAE=1.734\n",
            "   -> A1 plots saved for: subj=0 | trial=0_dumbbell_rows__row76\n",
            "   -> dir: /content/A1_mmfit_pairwise_A1viz\n",
            "2_dumbbell_rows__row5 | subj=2 | GT=11 | Pred(win)=10.01 | Diff=-0.99 | k_hat(full)=1.11 | phase_entropy(full)=0.196\n",
            "2_dumbbell_rows__row6 | subj=2 | GT=10 | Pred(win)=10.60 | Diff=+0.60 | k_hat(full)=1.07 | phase_entropy(full)=0.121\n",
            "2_dumbbell_rows__row7 | subj=2 | GT=11 | Pred(win)=10.79 | Diff=-0.21 | k_hat(full)=1.06 | phase_entropy(full)=0.104\n",
            "0_dumbbell_rows__row14 | subj=0 | GT=10 | Pred(win)=12.15 | Diff=+2.15 | k_hat(full)=1.03 | phase_entropy(full)=0.046\n",
            "0_dumbbell_rows__row15 | subj=0 | GT=10 | Pred(win)=13.30 | Diff=+3.30 | k_hat(full)=1.04 | phase_entropy(full)=0.085\n",
            "0_dumbbell_rows__row16 | subj=0 | GT=10 | Pred(win)=11.78 | Diff=+1.78 | k_hat(full)=1.04 | phase_entropy(full)=0.079\n",
            "0_dumbbell_rows__row17 | subj=0 | GT=10 | Pred(win)=15.31 | Diff=+5.31 | k_hat(full)=1.02 | phase_entropy(full)=0.040\n",
            "1_dumbbell_rows__row24 | subj=1 | GT=10 | Pred(win)=11.47 | Diff=+1.47 | k_hat(full)=1.10 | phase_entropy(full)=0.194\n",
            "1_dumbbell_rows__row25 | subj=1 | GT=10 | Pred(win)=9.90 | Diff=-0.10 | k_hat(full)=1.02 | phase_entropy(full)=0.047\n",
            "1_dumbbell_rows__row26 | subj=1 | GT=10 | Pred(win)=8.60 | Diff=-1.40 | k_hat(full)=1.03 | phase_entropy(full)=0.082\n",
            "0_dumbbell_rows__row33 | subj=0 | GT=10 | Pred(win)=10.65 | Diff=+0.65 | k_hat(full)=1.01 | phase_entropy(full)=0.035\n",
            "0_dumbbell_rows__row34 | subj=0 | GT=10 | Pred(win)=9.70 | Diff=-0.30 | k_hat(full)=1.03 | phase_entropy(full)=0.080\n",
            "0_dumbbell_rows__row35 | subj=0 | GT=10 | Pred(win)=11.03 | Diff=+1.03 | k_hat(full)=1.02 | phase_entropy(full)=0.055\n",
            "1_dumbbell_rows__row41 | subj=1 | GT=10 | Pred(win)=12.44 | Diff=+2.44 | k_hat(full)=1.06 | phase_entropy(full)=0.104\n",
            "1_dumbbell_rows__row42 | subj=1 | GT=10 | Pred(win)=10.61 | Diff=+0.61 | k_hat(full)=1.03 | phase_entropy(full)=0.060\n",
            "1_dumbbell_rows__row43 | subj=1 | GT=10 | Pred(win)=8.87 | Diff=-1.13 | k_hat(full)=1.02 | phase_entropy(full)=0.048\n",
            "2_dumbbell_rows__row50 | subj=2 | GT=10 | Pred(win)=8.53 | Diff=-1.47 | k_hat(full)=1.08 | phase_entropy(full)=0.152\n",
            "2_dumbbell_rows__row51 | subj=2 | GT=10 | Pred(win)=7.39 | Diff=-2.61 | k_hat(full)=1.02 | phase_entropy(full)=0.052\n",
            "2_dumbbell_rows__row52 | subj=2 | GT=10 | Pred(win)=8.11 | Diff=-1.89 | k_hat(full)=1.04 | phase_entropy(full)=0.091\n",
            "0_dumbbell_rows__row59 | subj=0 | GT=10 | Pred(win)=12.52 | Diff=+2.52 | k_hat(full)=1.02 | phase_entropy(full)=0.051\n",
            "... (43 more lines)\n",
            "\n",
            "6. pushups -> lunges  |  MAE=6.285\n",
            "   -> A1 plots saved for: subj=0 | trial=0_lunges__row92\n",
            "   -> dir: /content/A1_mmfit_pairwise_A1viz\n",
            "2_lunges__row2 | subj=2 | GT=10 | Pred(win)=16.50 | Diff=+6.50 | k_hat(full)=1.05 | phase_entropy(full)=0.094\n",
            "2_lunges__row3 | subj=2 | GT=10 | Pred(win)=18.46 | Diff=+8.46 | k_hat(full)=1.03 | phase_entropy(full)=0.057\n",
            "2_lunges__row4 | subj=2 | GT=10 | Pred(win)=16.71 | Diff=+6.71 | k_hat(full)=1.03 | phase_entropy(full)=0.068\n",
            "0_lunges__row11 | subj=0 | GT=10 | Pred(win)=13.42 | Diff=+3.42 | k_hat(full)=1.19 | phase_entropy(full)=0.348\n",
            "0_lunges__row12 | subj=0 | GT=10 | Pred(win)=13.67 | Diff=+3.67 | k_hat(full)=1.09 | phase_entropy(full)=0.214\n",
            "0_lunges__row13 | subj=0 | GT=10 | Pred(win)=19.94 | Diff=+9.94 | k_hat(full)=1.01 | phase_entropy(full)=0.021\n",
            "1_lunges__row21 | subj=1 | GT=10 | Pred(win)=13.68 | Diff=+3.68 | k_hat(full)=1.10 | phase_entropy(full)=0.198\n",
            "1_lunges__row22 | subj=1 | GT=10 | Pred(win)=16.74 | Diff=+6.74 | k_hat(full)=1.05 | phase_entropy(full)=0.093\n",
            "1_lunges__row23 | subj=1 | GT=10 | Pred(win)=14.75 | Diff=+4.75 | k_hat(full)=1.03 | phase_entropy(full)=0.056\n",
            "0_lunges__row30 | subj=0 | GT=10 | Pred(win)=16.42 | Diff=+6.42 | k_hat(full)=1.07 | phase_entropy(full)=0.149\n",
            "0_lunges__row31 | subj=0 | GT=10 | Pred(win)=15.31 | Diff=+5.31 | k_hat(full)=1.02 | phase_entropy(full)=0.045\n",
            "0_lunges__row32 | subj=0 | GT=10 | Pred(win)=13.22 | Diff=+3.22 | k_hat(full)=1.01 | phase_entropy(full)=0.032\n",
            "1_lunges__row39 | subj=1 | GT=10 | Pred(win)=18.15 | Diff=+8.15 | k_hat(full)=1.05 | phase_entropy(full)=0.112\n",
            "1_lunges__row40 | subj=1 | GT=10 | Pred(win)=16.91 | Diff=+6.91 | k_hat(full)=1.05 | phase_entropy(full)=0.117\n",
            "2_lunges__row47 | subj=2 | GT=10 | Pred(win)=16.10 | Diff=+6.10 | k_hat(full)=1.06 | phase_entropy(full)=0.124\n",
            "2_lunges__row48 | subj=2 | GT=10 | Pred(win)=17.35 | Diff=+7.35 | k_hat(full)=1.03 | phase_entropy(full)=0.074\n",
            "2_lunges__row49 | subj=2 | GT=11 | Pred(win)=18.80 | Diff=+7.80 | k_hat(full)=1.04 | phase_entropy(full)=0.076\n",
            "0_lunges__row56 | subj=0 | GT=10 | Pred(win)=15.62 | Diff=+5.62 | k_hat(full)=1.04 | phase_entropy(full)=0.099\n",
            "0_lunges__row57 | subj=0 | GT=10 | Pred(win)=17.37 | Diff=+7.37 | k_hat(full)=1.03 | phase_entropy(full)=0.055\n",
            "0_lunges__row58 | subj=0 | GT=10 | Pred(win)=16.34 | Diff=+6.34 | k_hat(full)=1.01 | phase_entropy(full)=0.038\n",
            "... (42 more lines)\n",
            "\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Overall Avg MAE=3.999 | Std=1.818 | #pairs=6\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "==============================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LmXdUFBRaTnr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}