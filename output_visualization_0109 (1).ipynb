{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Main plot\n",
        "- `Knees bending-Frontal elevation of arms`\n",
        "- `Waist bends forward-Jump front & back`\n",
        "- `Jogging-Running`"
      ],
      "metadata": {
        "id": "adV8_YVdU7_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ UPDATED (comment-based): only 2 plots per scenario\n",
        "#   1) Window-level rep_rate mean±std (+ boundary + A/B shading)\n",
        "#   2) Latent trajectory PCA2 (scatter + mean trajectory line + boundary marker + start/end + std circles)\n",
        "#   - NO titles\n",
        "#   - Bigger fonts\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns  # kept\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d  # kept\n",
        "from matplotlib.patches import Circle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from matplotlib.collections import LineCollection\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ Save helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _ensure_dir(path: str):\n",
        "    if path is None or path == \"\":\n",
        "        return\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def _safe_fname(s: str):\n",
        "    s = str(s)\n",
        "    for ch in ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|', '\\n', '\\t']:\n",
        "        s = s.replace(ch, '_')\n",
        "    s = s.strip().replace(' ', '_')\n",
        "    while '__' in s:\n",
        "        s = s.replace('__', '_')\n",
        "    return s\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ Mixed A-B builder\n",
        "# ---------------------------------------------------------------------\n",
        "def get_single_trial_from_full_data(subj, act_id, gt_count, full_data, target_map, feature_map, normalize=True):\n",
        "    act_name = target_map.get(act_id)\n",
        "    feats = feature_map.get(act_id)\n",
        "\n",
        "    if subj not in full_data or act_name not in full_data[subj]:\n",
        "        return None\n",
        "\n",
        "    raw_df = full_data[subj][act_name][feats]\n",
        "    x = raw_df.values.astype(np.float32)\n",
        "\n",
        "    if normalize:\n",
        "        mean = x.mean(axis=0)\n",
        "        std = x.std(axis=0) + 1e-6\n",
        "        x = (x - mean) / std\n",
        "\n",
        "    return {\n",
        "        \"data\": x,\n",
        "        \"count\": float(gt_count),\n",
        "        \"meta\": f\"{subj}_{act_name}\"\n",
        "    }\n",
        "\n",
        "\n",
        "def build_mixed_ab_trial(subj, actA_id, actB_id, config, full_data):\n",
        "    gt_map = config.get(\"GT_BY_ACT\", {})\n",
        "    if actA_id not in gt_map or actB_id not in gt_map:\n",
        "        return None\n",
        "    if subj not in gt_map[actA_id] or subj not in gt_map[actB_id]:\n",
        "        return None\n",
        "\n",
        "    gtA = float(gt_map[actA_id][subj])\n",
        "    gtB = float(gt_map[actB_id][subj])\n",
        "    gt_total = gtA + gtB\n",
        "\n",
        "    A = get_single_trial_from_full_data(\n",
        "        subj, actA_id, gtA, full_data,\n",
        "        config[\"TARGET_ACTIVITIES_MAP\"], config[\"ACT_FEATURE_MAP\"],\n",
        "        normalize=False\n",
        "    )\n",
        "    B = get_single_trial_from_full_data(\n",
        "        subj, actB_id, gtB, full_data,\n",
        "        config[\"TARGET_ACTIVITIES_MAP\"], config[\"ACT_FEATURE_MAP\"],\n",
        "        normalize=False\n",
        "    )\n",
        "    if A is None or B is None:\n",
        "        return None\n",
        "\n",
        "    xA_raw = A[\"data\"]\n",
        "    xB_raw = B[\"data\"]\n",
        "    boundary = int(xA_raw.shape[0])\n",
        "\n",
        "    x_mix_raw = np.concatenate([xA_raw, xB_raw], axis=0).astype(np.float32)\n",
        "\n",
        "    mean = x_mix_raw.mean(axis=0)\n",
        "    std = x_mix_raw.std(axis=0) + 1e-6\n",
        "    x_mix = (x_mix_raw - mean) / std\n",
        "\n",
        "    xA = (xA_raw - mean) / std\n",
        "    xB = (xB_raw - mean) / std\n",
        "\n",
        "    actA_name = config[\"TARGET_ACTIVITIES_MAP\"].get(actA_id, str(actA_id))\n",
        "    actB_name = config[\"TARGET_ACTIVITIES_MAP\"].get(actB_id, str(actB_id))\n",
        "\n",
        "    return {\n",
        "        \"data\": x_mix,\n",
        "        \"count\": float(gt_total),\n",
        "        \"meta\": f\"{subj}__{actA_name}__TO__{actB_name}\",\n",
        "        \"boundary\": boundary,\n",
        "        \"meta_detail\": {\n",
        "            \"subj\": subj,\n",
        "            \"actA_id\": actA_id, \"actB_id\": actB_id,\n",
        "            \"actA_name\": actA_name, \"actB_name\": actB_name,\n",
        "            \"gtA\": gtA, \"gtB\": gtB,\n",
        "            \"gt_total\": gt_total,\n",
        "        },\n",
        "        \"data_A\": xA,\n",
        "        \"data_B\": xB,\n",
        "        \"T_A\": int(xA.shape[0]),\n",
        "        \"T_B\": int(xB.shape[0]),\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Helpers for scenario-avg plots\n",
        "# ---------------------------------------------------------------------\n",
        "def get_window_rate_curve(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "\n",
        "    if T <= win_len:\n",
        "        pred_count, rates = predict_count_by_windowing(\n",
        "            model, x_np, fs, win_sec, stride_sec, device, tau=tau, batch_size=batch_size\n",
        "        )\n",
        "        t_cent = np.array([0.5 * (T / float(fs))], dtype=np.float32)\n",
        "        return t_cent, rates.astype(np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "    rates = np.concatenate(rates, axis=0).astype(np.float32)\n",
        "\n",
        "    centers = np.array([(st + 0.5 * win_len) / float(fs) for st in starts], dtype=np.float32)\n",
        "    return centers, rates\n",
        "\n",
        "\n",
        "def resample_1d(y, new_len):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    if y.size == 0:\n",
        "        return np.zeros((new_len,), dtype=np.float32)\n",
        "    if y.size == 1:\n",
        "        return np.full((new_len,), float(y[0]), dtype=np.float32)\n",
        "    x_old = np.linspace(0.0, 1.0, num=y.size, dtype=np.float32)\n",
        "    x_new = np.linspace(0.0, 1.0, num=new_len, dtype=np.float32)\n",
        "    return np.interp(x_new, x_old, y).astype(np.float32)\n",
        "\n",
        "\n",
        "def resample_2d(Y, new_len):\n",
        "    Y = np.asarray(Y, dtype=np.float32)\n",
        "    if Y.shape[0] == 0:\n",
        "        return np.zeros((new_len, Y.shape[1]), dtype=np.float32)\n",
        "    if Y.shape[0] == 1:\n",
        "        return np.repeat(Y, repeats=new_len, axis=0).astype(np.float32)\n",
        "    x_old = np.linspace(0.0, 1.0, num=Y.shape[0], dtype=np.float32)\n",
        "    x_new = np.linspace(0.0, 1.0, num=new_len, dtype=np.float32)\n",
        "    out = []\n",
        "    for d in range(Y.shape[1]):\n",
        "        out.append(np.interp(x_new, x_old, Y[:, d]))\n",
        "    return np.stack(out, axis=1).astype(np.float32)\n",
        "\n",
        "\n",
        "def global_pca2(Z_all):\n",
        "    Z_all = np.asarray(Z_all, dtype=np.float32)\n",
        "    mu = Z_all.mean(axis=0, keepdims=True)\n",
        "    Zc = Z_all - mu\n",
        "    _, _, Vt = np.linalg.svd(Zc, full_matrices=False)\n",
        "    V2 = Vt[:2].astype(np.float32)\n",
        "    return mu.squeeze(0).astype(np.float32), V2\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ UPDATED PLOTTER: comment-based 2 plots only, no titles, bigger fonts\n",
        "#   (1) Window mean±std + boundary + shading\n",
        "#   (2) PCA2 latent: scatter + mean trajectory line + boundary marker + start/end + std circles\n",
        "# ---------------------------------------------------------------------\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable  # ✅ 추가 import (plot 함수에서만 사용)\n",
        "\n",
        "def plot_scenario_mean_std_two_plots(\n",
        "    save_dir,\n",
        "    scen_name,\n",
        "    win_t, win_mean, win_std,\n",
        "    lat_xy_mean, lat_xy_std,\n",
        "    n_subjects,\n",
        "    dpi=600,\n",
        "):\n",
        "    _ensure_dir(save_dir if save_dir is not None else \"scenario_viz\")\n",
        "    out_dir = save_dir if save_dir is not None else \"scenario_viz\"\n",
        "    scen_tag = _safe_fname(scen_name)\n",
        "\n",
        "    # -------------------------\n",
        "    # (1) Window-level Rep rate mean±std\n",
        "    # -------------------------\n",
        "    fig1 = plt.figure(figsize=(26.0, 9.2))\n",
        "    ax = fig1.gca()\n",
        "\n",
        "    # subtle A/B shading (keep but slightly lighter)\n",
        "    ax.axvspan(0.0, 1.0, alpha=0.035, color=\"k\", lw=0)\n",
        "    ax.axvspan(1.0, 2.0, alpha=0.022, color=\"k\", lw=0)\n",
        "\n",
        "    ax.plot(\n",
        "        win_t, win_mean,\n",
        "        linewidth=6.7,\n",
        "        solid_capstyle=\"round\",\n",
        "        label=\"Window rep rate (mean)\"\n",
        "    )\n",
        "    ax.fill_between(\n",
        "        win_t,\n",
        "        win_mean - win_std, win_mean + win_std,\n",
        "        alpha=0.16,  # ✅ band alpha down\n",
        "        linewidth=0,\n",
        "        label=\"±1 std\"\n",
        "    )\n",
        "\n",
        "    ax.axvline(\n",
        "        1.0,\n",
        "        linestyle=\"--\",\n",
        "        linewidth=3.2,  # ✅ boundary thinner but still bold\n",
        "        label=\"Boundary\"\n",
        "    )\n",
        "\n",
        "    ax.set_xlabel(\"Normalized time (A:0–1, B:1–2)\", fontsize=49, labelpad=10)\n",
        "    ax.set_ylabel(\"Rep rate (reps/s)\", fontsize=49, labelpad=10)  # ✅ underscore 제거\n",
        "    ax.tick_params(axis=\"both\", labelsize=41, width=1.6, length=7)\n",
        "\n",
        "    # paper-like spine thickness\n",
        "    for sp in ax.spines.values():\n",
        "        sp.set_linewidth(1.6)\n",
        "\n",
        "    # legend: 조금만 줄이기\n",
        "    ax.legend(\n",
        "        fontsize=37,\n",
        "        frameon=True,\n",
        "        loc=\"upper left\",\n",
        "        borderpad=0.55,\n",
        "        labelspacing=0.35,\n",
        "        handlelength=2.4\n",
        "    )\n",
        "\n",
        "    fig1.tight_layout()\n",
        "    fig1.savefig(os.path.join(out_dir, f\"{scen_tag}__1_win_rep_rate.png\"), dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig1)\n",
        "\n",
        "    # -------------------------\n",
        "    # (2) Latent trajectory PCA2 (gradient line)\n",
        "    # -------------------------\n",
        "    fig2 = plt.figure(figsize=(10.0, 10.0))\n",
        "    ax = fig2.gca()\n",
        "\n",
        "    lat_xy_mean = np.asarray(lat_xy_mean, dtype=np.float32)\n",
        "    T = lat_xy_mean.shape[0]\n",
        "    t_lat = np.linspace(0.0, 2.0, num=T, dtype=np.float32)\n",
        "\n",
        "    pts = lat_xy_mean.reshape(-1, 1, 2)\n",
        "    segs = np.concatenate([pts[:-1], pts[1:]], axis=1)\n",
        "\n",
        "    lc = LineCollection(\n",
        "        segs,\n",
        "        array=t_lat[:-1],\n",
        "        linewidth=2.6,          # ✅ trajectory 더 또렷하게\n",
        "        alpha=0.95\n",
        "    )\n",
        "    ax.add_collection(lc)\n",
        "\n",
        "    ax.set_xlabel(\"PC1\", fontsize=25, labelpad=10)\n",
        "    ax.set_ylabel(\"PC2\", fontsize=25, labelpad=10)\n",
        "    ax.tick_params(axis=\"both\", labelsize=20, width=1.4, length=6)\n",
        "\n",
        "    for sp in ax.spines.values():\n",
        "        sp.set_linewidth(1.4)\n",
        "\n",
        "    # ✅ equal 유지 + 중앙 고정\n",
        "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
        "    ax.set_anchor(\"C\")\n",
        "\n",
        "    # square bounds 유지\n",
        "    x = lat_xy_mean[:, 0]; y = lat_xy_mean[:, 1]\n",
        "    xmin, xmax = float(np.min(x)), float(np.max(x))\n",
        "    ymin, ymax = float(np.min(y)), float(np.max(y))\n",
        "    cx = 0.5 * (xmin + xmax); cy = 0.5 * (ymin + ymax)\n",
        "    rx = 0.5 * (xmax - xmin); ry = 0.5 * (ymax - ymin)\n",
        "    r = max(rx, ry) * 1.12 + 1e-6\n",
        "    ax.set_xlim(cx - r, cx + r)\n",
        "    ax.set_ylim(cy - r, cy + r)\n",
        "\n",
        "    # ✅ colorbar가 axis를 “쪼그라뜨리는” 느낌을 줄이기 위해 divider로 별도 축 생성\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"4.5%\", pad=0.10)\n",
        "    cbar = fig2.colorbar(lc, cax=cax)\n",
        "    cbar.set_label(\"Normalized time (0→2)\", fontsize=22, labelpad=10)\n",
        "    cbar.ax.tick_params(labelsize=18, width=1.2, length=5)\n",
        "\n",
        "    fig2.tight_layout()\n",
        "    fig2.savefig(os.path.join(out_dir, f\"{scen_tag}__2_latent_pca2.png\"), dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig2)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Dataset / Collate\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),\n",
        "        \"mask\": torch.stack(masks),\n",
        "        \"count\": torch.stack(counts),\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)\n",
        "        z = z.transpose(1, 2)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)\n",
        "        x_hat = self.net(zt)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)\n",
        "        amp = F.softplus(out[..., 0])\n",
        "        phase_logits = out[..., 1:]\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        micro_rate_t = amp_t\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"phase_p\": phase_p,\n",
        "            \"phase_logits\": phase_logits,\n",
        "            \"micro_rate_t\": micro_rate_t,\n",
        "            \"rep_rate_t\": rep_rate_t,\n",
        "            \"k_hat\": k_hat,\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Main (Scenario-wise LOSO)\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6:  'Waist bends forward',\n",
        "            7:  'Frontal elevation of arms',\n",
        "            8:  'Knees bending',\n",
        "            10:  'Jogging',\n",
        "            11: 'Running',\n",
        "            12: 'Jump front & back',\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6:  ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            7:  ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            8:  ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            10: ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            11: ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            12: ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Windowing Params\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        \"GT_BY_ACT\": {\n",
        "            6:  {\"subject1\": 21, \"subject2\": 19, \"subject3\": 21, \"subject4\": 20, \"subject5\": 20,\n",
        "                 \"subject6\": 20, \"subject7\": 20, \"subject8\": 21, \"subject9\": 21, \"subject10\": 20},\n",
        "            7:  {\"subject1\": 20, \"subject2\": 20, \"subject3\": 20, \"subject4\": 20, \"subject5\": 20,\n",
        "                 \"subject6\": 20, \"subject7\": 20, \"subject8\": 19, \"subject9\": 19, \"subject10\": 20},\n",
        "            8:  {\"subject1\": 20, \"subject2\": 21, \"subject3\": 21, \"subject4\": 19, \"subject5\": 20,\n",
        "                 \"subject6\": 20, \"subject7\": 21, \"subject8\": 21, \"subject9\": 21, \"subject10\": 21},\n",
        "            10: {\"subject1\": 157, \"subject2\": 161, \"subject3\": 154, \"subject4\": 154, \"subject5\": 160,\n",
        "                 \"subject6\": 156, \"subject7\": 153, \"subject8\": 160, \"subject9\": 166, \"subject10\": 156},\n",
        "            11: {\"subject1\": 165, \"subject2\": 158, \"subject3\": 174, \"subject4\": 163, \"subject5\": 157,\n",
        "                 \"subject6\": 172, \"subject7\": 149, \"subject8\": 166, \"subject9\": 174, \"subject10\": 172},\n",
        "            12: {\"subject1\": 20, \"subject2\": 22, \"subject3\": 21, \"subject4\": 21, \"subject5\": 20,\n",
        "                 \"subject6\": 21, \"subject7\": 19, \"subject8\": 20, \"subject9\": 20, \"subject10\": 20},\n",
        "        },\n",
        "\n",
        "        \"MIX_SCENARIOS\": [\n",
        "            (\"Knees bending-Frontal elevation of arms\", 8, 7),\n",
        "            (\"Waist bends forward-Jump front & back\", 6, 12),\n",
        "            (\"Jogging-Running\", 10, 11),\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    VIZ_DIR = \"scenario_viz\"\n",
        "    os.makedirs(VIZ_DIR, exist_ok=True)\n",
        "\n",
        "    # resample lengths (fixed for averaging)\n",
        "    NW = 60     # window-rate points per segment (A,B)\n",
        "    NL = 200    # latent points per segment (A,B)\n",
        "\n",
        "    for scen_idx, (scen_name, actA_id, actB_id) in enumerate(CONFIG.get(\"MIX_SCENARIOS\", []), start=1):\n",
        "        TRAIN_ACT_ID = actA_id\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 100)\n",
        "        print(f\"1. 시나리오{scen_idx}: {scen_name}\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        loso_results = []\n",
        "        scenario_records = []\n",
        "        scenario_diffs = []\n",
        "        scenario_diffs_A = []\n",
        "        scenario_diffs_B = []\n",
        "\n",
        "        # ✅ buffers for TWO plots only\n",
        "        buf_win_concat = []     # (2*NW,)\n",
        "        buf_z_concat = []       # list of (Tmix,D)\n",
        "        per_subject_latent = [] # list of {\"zA\",\"zB\"}\n",
        "\n",
        "        for fold_idx, test_subj in enumerate(subjects):\n",
        "            set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "            gt_train_map = CONFIG[\"GT_BY_ACT\"][TRAIN_ACT_ID]\n",
        "            train_labels = [(s, TRAIN_ACT_ID, gt_train_map[s]) for s in subjects if s != test_subj]\n",
        "            test_labels  = [(test_subj, TRAIN_ACT_ID, gt_train_map[test_subj])]\n",
        "\n",
        "            train_trials = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "            test_trials  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "            if not test_trials or not train_trials:\n",
        "                continue\n",
        "\n",
        "            train_data = trial_list_to_windows(\n",
        "                train_trials, fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                drop_last=CONFIG[\"drop_last\"]\n",
        "            )\n",
        "\n",
        "            g = torch.Generator()\n",
        "            g.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "            train_loader = DataLoader(\n",
        "                TrialDataset(train_data),\n",
        "                batch_size=CONFIG[\"batch_size\"],\n",
        "                shuffle=True,\n",
        "                collate_fn=collate_variable_length,\n",
        "                generator=g,\n",
        "                num_workers=0\n",
        "            )\n",
        "\n",
        "            input_ch = train_data[0]['data'].shape[1]\n",
        "            model = KAutoCountModel(\n",
        "                input_ch=input_ch,\n",
        "                hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "                latent_dim=CONFIG[\"latent_dim\"],\n",
        "                K_max=CONFIG[\"K_max\"]\n",
        "            ).to(device)\n",
        "\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "            for epoch in range(CONFIG[\"epochs\"]):\n",
        "                _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "                scheduler.step()\n",
        "\n",
        "            model.eval\n",
        "            model.eval()\n",
        "\n",
        "            # (A) LOSO on TRAIN_ACT_ID\n",
        "            item = test_trials[0]\n",
        "            x_np = item[\"data\"]\n",
        "            count_gt = float(item[\"count\"])\n",
        "            count_pred_win, _ = predict_count_by_windowing(\n",
        "                model, x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            fold_mae = float(abs(count_pred_win - count_gt))\n",
        "            loso_results.append(fold_mae)\n",
        "\n",
        "            # (B) Scenario eval\n",
        "            mixed_item = build_mixed_ab_trial(test_subj, actA_id, actB_id, CONFIG, full_data)\n",
        "            if mixed_item is None:\n",
        "                continue\n",
        "\n",
        "            x_mix = mixed_item[\"data\"]\n",
        "            boundary = mixed_item[\"boundary\"]\n",
        "            gt_total = float(mixed_item[\"count\"])\n",
        "            md = mixed_item[\"meta_detail\"]\n",
        "\n",
        "            pred_total, _ = predict_count_by_windowing(\n",
        "                model, x_np=x_mix,\n",
        "                fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            diff = float(pred_total - gt_total)\n",
        "            mae = float(abs(diff))\n",
        "            scenario_diffs.append(diff)\n",
        "\n",
        "            # A/B split MAE\n",
        "            xA = mixed_item[\"data_A\"]\n",
        "            xB = mixed_item[\"data_B\"]\n",
        "            gtA = float(md[\"gtA\"])\n",
        "            gtB = float(md[\"gtB\"])\n",
        "\n",
        "            pred_A, _ = predict_count_by_windowing(\n",
        "                model, x_np=xA,\n",
        "                fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            pred_B, _ = predict_count_by_windowing(\n",
        "                model, x_np=xB,\n",
        "                fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            diff_A = float(pred_A - gtA)\n",
        "            diff_B = float(pred_B - gtB)\n",
        "            mae_A = float(abs(diff_A))\n",
        "            mae_B = float(abs(diff_B))\n",
        "            scenario_diffs_A.append(diff_A)\n",
        "            scenario_diffs_B.append(diff_B)\n",
        "\n",
        "            # full forward for z (latent) (+ keep entropy/k_hat log if you want)\n",
        "            x_tensor = torch.tensor(x_mix, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                _, z_m, _, aux_m = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "\n",
        "            phase_p_m = aux_m[\"phase_p\"].squeeze(0).detach().cpu().numpy()  # (Tmix,K)\n",
        "            k_hat_m = float(aux_m[\"k_hat\"].item())\n",
        "            ent_m = compute_phase_entropy_mean(phase_p_m)\n",
        "\n",
        "            z_td = z_m.squeeze(0).detach().cpu().numpy()  # (Tmix,D)\n",
        "\n",
        "            scenario_records.append({\n",
        "                \"subj\": test_subj,\n",
        "                \"line\": (\n",
        "                    f\"[Scenario] {scen_name} | {test_subj} | \"\n",
        "                    f\"{md['actA_name']}({md['gtA']:.0f}) -> {md['actB_name']}({md['gtB']:.0f}) | \"\n",
        "                    f\"GT_total={gt_total:.0f} | Pred_total(win)={pred_total:.2f} | Diff_total={diff:+.2f} | \"\n",
        "                    f\"MAE_total={mae:.2f} | \"\n",
        "                    f\"Pred_A={pred_A:.2f} (Diff_A={diff_A:+.2f}, MAE_A={mae_A:.2f}) | \"\n",
        "                    f\"Pred_B={pred_B:.2f} (Diff_B={diff_B:+.2f}, MAE_B={mae_B:.2f}) | \"\n",
        "                    f\"k_hat(full)={k_hat_m:.2f} | ent(full)={ent_m:.3f} | boundary={boundary}\"\n",
        "                )\n",
        "            })\n",
        "\n",
        "            # (1) window-level rep_rate curves: A/B separately -> resample -> concat\n",
        "            _, rA_w = get_window_rate_curve(\n",
        "                model, xA, fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            _, rB_w = get_window_rate_curve(\n",
        "                model, xB, fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            rA_rs = resample_1d(rA_w, NW)\n",
        "            rB_rs = resample_1d(rB_w, NW)\n",
        "            buf_win_concat.append(np.concatenate([rA_rs, rB_rs], axis=0))\n",
        "\n",
        "            # (2) latent: store raw z split for global PCA later\n",
        "            zA = z_td[:boundary, :]\n",
        "            zB = z_td[boundary:, :]\n",
        "            buf_z_concat.append(z_td)\n",
        "            per_subject_latent.append({\"zA\": zA, \"zB\": zB})\n",
        "\n",
        "        # -------------------------\n",
        "        # Print summaries (same as your original)\n",
        "        # -------------------------\n",
        "        print(\"-\" * 100)\n",
        "        if len(loso_results) > 0:\n",
        "            print(\"TRain 결과 ->\")\n",
        "            print(\"-\" * 100)\n",
        "            print(f\" >>> Final LOSO Result (Average MAE): {np.mean(loso_results):.3f}\")\n",
        "            print(f\" >>> Standard Deviation: {np.std(loso_results):.3f}\")\n",
        "            print(\"-\" * 100)\n",
        "        else:\n",
        "            print(\"TRain 결과 -> (no folds computed)\")\n",
        "            print(\"-\" * 100)\n",
        "\n",
        "        subj2line = {r[\"subj\"]: r[\"line\"] for r in scenario_records}\n",
        "        for s in subjects:\n",
        "            if s in subj2line:\n",
        "                print(subj2line[s])\n",
        "\n",
        "        print(\"\\n\" + \"-\" * 100)\n",
        "        print(f\"시나리오{scen_idx} 결과\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        if len(scenario_diffs) == 0:\n",
        "            print(f\"[{scen_name}] No samples (all skipped). Fill GT_BY_ACT for required activities.\")\n",
        "        else:\n",
        "            diffs = np.array(scenario_diffs, dtype=np.float32)\n",
        "            mae = float(np.mean(np.abs(diffs)))\n",
        "            rmse = float(np.sqrt(np.mean(diffs ** 2)))\n",
        "            print(f\"[TOTAL] [{scen_name}] N={len(diffs):2d} | MAE={mae:.3f} | RMSE={rmse:.3f} | mean(diff)={diffs.mean():+.3f} | std(diff)={diffs.std():.3f}\")\n",
        "\n",
        "            diffsA = np.array(scenario_diffs_A, dtype=np.float32)\n",
        "            maeA = float(np.mean(np.abs(diffsA)))\n",
        "            rmseA = float(np.sqrt(np.mean(diffsA ** 2)))\n",
        "            print(f\"[A]     [{scen_name}] N={len(diffsA):2d} | MAE={maeA:.3f} | RMSE={rmseA:.3f} | mean(diff)={diffsA.mean():+.3f} | std(diff)={diffsA.std():.3f}\")\n",
        "\n",
        "            diffsB = np.array(scenario_diffs_B, dtype=np.float32)\n",
        "            maeB = float(np.mean(np.abs(diffsB)))\n",
        "            rmseB = float(np.sqrt(np.mean(diffsB ** 2)))\n",
        "            print(f\"[B]     [{scen_name}] N={len(diffsB):2d} | MAE={maeB:.3f} | RMSE={rmseB:.3f} | mean(diff)={diffsB.mean():+.3f} | std(diff)={diffsB.std():.3f}\")\n",
        "\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        # -------------------------\n",
        "        # ✅ TWO-PLOT scenario-avg visualization\n",
        "        # -------------------------\n",
        "        n_ok = len(buf_win_concat)\n",
        "        if n_ok == 0:\n",
        "            print(\"[Viz Skip] No scenario samples collected for averaging.\")\n",
        "            continue\n",
        "\n",
        "        # (1) window-level mean±std on normalized axis [0,2]\n",
        "        W = np.stack(buf_win_concat, axis=0)  # (N, 2*NW)\n",
        "        win_mean = W.mean(axis=0)\n",
        "        win_std  = W.std(axis=0)\n",
        "        win_t = np.linspace(0.0, 2.0, num=2*NW, dtype=np.float32)\n",
        "\n",
        "        # (2) latent: global PCA basis from all z\n",
        "        Z_all = np.concatenate(buf_z_concat, axis=0)  # (sumT, D)\n",
        "        mu, V2 = global_pca2(Z_all)\n",
        "\n",
        "        # project each subject, resample A/B to NL then concat -> (2*NL,2)\n",
        "        lat_list = []\n",
        "        for d in per_subject_latent:\n",
        "            zA = d[\"zA\"]; zB = d[\"zB\"]\n",
        "            pcA = (zA - mu) @ V2.T\n",
        "            pcB = (zB - mu) @ V2.T\n",
        "            pcA_rs = resample_2d(pcA, NL)\n",
        "            pcB_rs = resample_2d(pcB, NL)\n",
        "            lat_list.append(np.concatenate([pcA_rs, pcB_rs], axis=0))\n",
        "\n",
        "        LAT = np.stack(lat_list, axis=0)  # (N,2*NL,2)\n",
        "        lat_xy_mean = LAT.mean(axis=0)\n",
        "        lat_xy_std  = LAT.std(axis=0)\n",
        "\n",
        "        plot_scenario_mean_std_two_plots(\n",
        "            save_dir=VIZ_DIR,\n",
        "            scen_name=scen_name,\n",
        "            win_t=win_t, win_mean=win_mean, win_std=win_std,\n",
        "            lat_xy_mean=lat_xy_mean, lat_xy_std=lat_xy_std,\n",
        "            n_subjects=n_ok,\n",
        "            dpi=600\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdfZ51vvo33a",
        "outputId": "8975fcae-56a0-4dfc-bc9f-2368bd401eba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오1: Knees bending-Frontal elevation of arms\n",
            "====================================================================================================\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 4.274\n",
            " >>> Standard Deviation: 3.012\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject1 | Knees bending(20) -> Frontal elevation of arms(20) | GT_total=40 | Pred_total(win)=66.34 | Diff_total=+26.34 | MAE_total=26.34 | Pred_A=47.42 (Diff_A=+27.42, MAE_A=27.42) | Pred_B=18.71 (Diff_B=-1.29, MAE_B=1.29) | k_hat(full)=1.02 | ent(full)=0.059 | boundary=3379\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject2 | Knees bending(21) -> Frontal elevation of arms(20) | GT_total=41 | Pred_total(win)=42.73 | Diff_total=+1.73 | MAE_total=1.73 | Pred_A=24.43 (Diff_A=+3.43, MAE_A=3.43) | Pred_B=18.26 (Diff_B=-1.74, MAE_B=1.74) | k_hat(full)=1.01 | ent(full)=0.041 | boundary=3430\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject3 | Knees bending(21) -> Frontal elevation of arms(20) | GT_total=41 | Pred_total(win)=30.07 | Diff_total=-10.93 | MAE_total=10.93 | Pred_A=14.94 (Diff_A=-6.06, MAE_A=6.06) | Pred_B=15.02 (Diff_B=-4.98, MAE_B=4.98) | k_hat(full)=1.01 | ent(full)=0.036 | boundary=3175\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject4 | Knees bending(19) -> Frontal elevation of arms(20) | GT_total=39 | Pred_total(win)=48.13 | Diff_total=+9.13 | MAE_total=9.13 | Pred_A=26.86 (Diff_A=+7.86, MAE_A=7.86) | Pred_B=21.45 (Diff_B=+1.45, MAE_B=1.45) | k_hat(full)=1.60 | ent(full)=0.163 | boundary=3123\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject5 | Knees bending(20) -> Frontal elevation of arms(20) | GT_total=40 | Pred_total(win)=31.99 | Diff_total=-8.01 | MAE_total=8.01 | Pred_A=14.43 (Diff_A=-5.57, MAE_A=5.57) | Pred_B=17.82 (Diff_B=-2.18, MAE_B=2.18) | k_hat(full)=1.02 | ent(full)=0.050 | boundary=2714\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject6 | Knees bending(20) -> Frontal elevation of arms(20) | GT_total=40 | Pred_total(win)=25.42 | Diff_total=-14.58 | MAE_total=14.58 | Pred_A=13.74 (Diff_A=-6.26, MAE_A=6.26) | Pred_B=11.71 (Diff_B=-8.29, MAE_B=8.29) | k_hat(full)=1.01 | ent(full)=0.033 | boundary=2304\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject7 | Knees bending(21) -> Frontal elevation of arms(20) | GT_total=41 | Pred_total(win)=36.12 | Diff_total=-4.88 | MAE_total=4.88 | Pred_A=12.28 (Diff_A=-8.72, MAE_A=8.72) | Pred_B=24.25 (Diff_B=+4.25, MAE_B=4.25) | k_hat(full)=1.00 | ent(full)=0.012 | boundary=2816\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject8 | Knees bending(21) -> Frontal elevation of arms(19) | GT_total=40 | Pred_total(win)=35.73 | Diff_total=-4.27 | MAE_total=4.27 | Pred_A=12.78 (Diff_A=-8.22, MAE_A=8.22) | Pred_B=23.25 (Diff_B=+4.25, MAE_B=4.25) | k_hat(full)=1.01 | ent(full)=0.036 | boundary=2560\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject9 | Knees bending(21) -> Frontal elevation of arms(19) | GT_total=40 | Pred_total(win)=36.83 | Diff_total=-3.17 | MAE_total=3.17 | Pred_A=12.01 (Diff_A=-8.99, MAE_A=8.99) | Pred_B=24.88 (Diff_B=+5.88, MAE_B=5.88) | k_hat(full)=1.01 | ent(full)=0.017 | boundary=2969\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject10 | Knees bending(21) -> Frontal elevation of arms(20) | GT_total=41 | Pred_total(win)=32.91 | Diff_total=-8.09 | MAE_total=8.09 | Pred_A=11.21 (Diff_A=-9.79, MAE_A=9.79) | Pred_B=21.97 (Diff_B=+1.97, MAE_B=1.97) | k_hat(full)=1.01 | ent(full)=0.018 | boundary=2867\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오1 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[TOTAL] [Knees bending-Frontal elevation of arms] N=10 | MAE=9.113 | RMSE=11.375 | mean(diff)=-1.672 | std(diff)=11.251\n",
            "[A]     [Knees bending-Frontal elevation of arms] N=10 | MAE=9.234 | RMSE=11.193 | mean(diff)=-1.490 | std(diff)=11.094\n",
            "[B]     [Knees bending-Frontal elevation of arms] N=10 | MAE=3.629 | RMSE=4.240 | mean(diff)=-0.068 | std(diff)=4.240\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오2: Waist bends forward-Jump front & back\n",
            "====================================================================================================\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 5.261\n",
            " >>> Standard Deviation: 3.006\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario] Waist bends forward-Jump front & back | subject1 | Waist bends forward(21) -> Jump front & back(20) | GT_total=41 | Pred_total(win)=26.26 | Diff_total=-14.74 | MAE_total=14.74 | Pred_A=13.11 (Diff_A=-7.89, MAE_A=7.89) | Pred_B=15.15 (Diff_B=-4.85, MAE_B=4.85) | k_hat(full)=1.10 | ent(full)=0.214 | boundary=3072\n",
            "[Scenario] Waist bends forward-Jump front & back | subject2 | Waist bends forward(19) -> Jump front & back(22) | GT_total=41 | Pred_total(win)=31.27 | Diff_total=-9.73 | MAE_total=9.73 | Pred_A=16.44 (Diff_A=-2.56, MAE_A=2.56) | Pred_B=18.16 (Diff_B=-3.84, MAE_B=3.84) | k_hat(full)=1.07 | ent(full)=0.155 | boundary=3174\n",
            "[Scenario] Waist bends forward-Jump front & back | subject3 | Waist bends forward(21) -> Jump front & back(21) | GT_total=42 | Pred_total(win)=39.70 | Diff_total=-2.30 | MAE_total=2.30 | Pred_A=15.13 (Diff_A=-5.87, MAE_A=5.87) | Pred_B=26.94 (Diff_B=+5.94, MAE_B=5.94) | k_hat(full)=1.04 | ent(full)=0.101 | boundary=3226\n",
            "[Scenario] Waist bends forward-Jump front & back | subject4 | Waist bends forward(20) -> Jump front & back(21) | GT_total=41 | Pred_total(win)=29.09 | Diff_total=-11.91 | MAE_total=11.91 | Pred_A=14.76 (Diff_A=-5.24, MAE_A=5.24) | Pred_B=16.69 (Diff_B=-4.31, MAE_B=4.31) | k_hat(full)=1.05 | ent(full)=0.121 | boundary=3328\n",
            "[Scenario] Waist bends forward-Jump front & back | subject5 | Waist bends forward(20) -> Jump front & back(20) | GT_total=40 | Pred_total(win)=37.98 | Diff_total=-2.02 | MAE_total=2.02 | Pred_A=20.05 (Diff_A=+0.05, MAE_A=0.05) | Pred_B=20.57 (Diff_B=+0.57, MAE_B=0.57) | k_hat(full)=1.06 | ent(full)=0.139 | boundary=2765\n",
            "[Scenario] Waist bends forward-Jump front & back | subject6 | Waist bends forward(20) -> Jump front & back(21) | GT_total=41 | Pred_total(win)=23.48 | Diff_total=-17.52 | MAE_total=17.52 | Pred_A=9.06 (Diff_A=-10.94, MAE_A=10.94) | Pred_B=14.99 (Diff_B=-6.01, MAE_B=6.01) | k_hat(full)=1.01 | ent(full)=0.043 | boundary=2202\n",
            "[Scenario] Waist bends forward-Jump front & back | subject7 | Waist bends forward(20) -> Jump front & back(19) | GT_total=39 | Pred_total(win)=39.25 | Diff_total=+0.25 | MAE_total=0.25 | Pred_A=17.74 (Diff_A=-2.26, MAE_A=2.26) | Pred_B=23.68 (Diff_B=+4.68, MAE_B=4.68) | k_hat(full)=1.09 | ent(full)=0.190 | boundary=3072\n",
            "[Scenario] Waist bends forward-Jump front & back | subject8 | Waist bends forward(21) -> Jump front & back(20) | GT_total=41 | Pred_total(win)=27.09 | Diff_total=-13.91 | MAE_total=13.91 | Pred_A=9.27 (Diff_A=-11.73, MAE_A=11.73) | Pred_B=20.10 (Diff_B=+0.10, MAE_B=0.10) | k_hat(full)=1.01 | ent(full)=0.037 | boundary=2151\n",
            "[Scenario] Waist bends forward-Jump front & back | subject9 | Waist bends forward(21) -> Jump front & back(20) | GT_total=41 | Pred_total(win)=35.21 | Diff_total=-5.79 | MAE_total=5.79 | Pred_A=15.25 (Diff_A=-5.75, MAE_A=5.75) | Pred_B=22.36 (Diff_B=+2.36, MAE_B=2.36) | k_hat(full)=1.01 | ent(full)=0.022 | boundary=2867\n",
            "[Scenario] Waist bends forward-Jump front & back | subject10 | Waist bends forward(20) -> Jump front & back(20) | GT_total=40 | Pred_total(win)=27.36 | Diff_total=-12.64 | MAE_total=12.64 | Pred_A=13.27 (Diff_A=-6.73, MAE_A=6.73) | Pred_B=15.24 (Diff_B=-4.76, MAE_B=4.76) | k_hat(full)=1.03 | ent(full)=0.073 | boundary=2458\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오2 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[TOTAL] [Waist bends forward-Jump front & back] N=10 | MAE=9.080 | RMSE=10.756 | mean(diff)=-9.030 | std(diff)=5.843\n",
            "[A]     [Waist bends forward-Jump front & back] N=10 | MAE=5.902 | RMSE=6.867 | mean(diff)=-5.893 | std(diff)=3.526\n",
            "[B]     [Waist bends forward-Jump front & back] N=10 | MAE=3.742 | RMSE=4.227 | mean(diff)=-1.012 | std(diff)=4.105\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오3: Jogging-Running\n",
            "====================================================================================================\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 10.421\n",
            " >>> Standard Deviation: 10.920\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario] Jogging-Running | subject1 | Jogging(157) -> Running(165) | GT_total=322 | Pred_total(win)=321.54 | Diff_total=-0.46 | MAE_total=0.46 | Pred_A=121.58 (Diff_A=-35.42, MAE_A=35.42) | Pred_B=201.85 (Diff_B=+36.85, MAE_B=36.85) | k_hat(full)=1.23 | ent(full)=0.311 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject2 | Jogging(161) -> Running(158) | GT_total=319 | Pred_total(win)=338.97 | Diff_total=+19.97 | MAE_total=19.97 | Pred_A=148.89 (Diff_A=-12.11, MAE_A=12.11) | Pred_B=189.01 (Diff_B=+31.01, MAE_B=31.01) | k_hat(full)=1.73 | ent(full)=0.485 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject3 | Jogging(154) -> Running(174) | GT_total=328 | Pred_total(win)=335.16 | Diff_total=+7.16 | MAE_total=7.16 | Pred_A=154.89 (Diff_A=+0.89, MAE_A=0.89) | Pred_B=182.06 (Diff_B=+8.06, MAE_B=8.06) | k_hat(full)=1.34 | ent(full)=0.315 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject4 | Jogging(154) -> Running(163) | GT_total=317 | Pred_total(win)=371.57 | Diff_total=+54.57 | MAE_total=54.57 | Pred_A=184.04 (Diff_A=+30.04, MAE_A=30.04) | Pred_B=187.40 (Diff_B=+24.40, MAE_B=24.40) | k_hat(full)=1.28 | ent(full)=0.278 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject5 | Jogging(160) -> Running(157) | GT_total=317 | Pred_total(win)=357.05 | Diff_total=+40.05 | MAE_total=40.05 | Pred_A=175.40 (Diff_A=+15.40, MAE_A=15.40) | Pred_B=181.58 (Diff_B=+24.58, MAE_B=24.58) | k_hat(full)=1.41 | ent(full)=0.330 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject6 | Jogging(156) -> Running(172) | GT_total=328 | Pred_total(win)=373.09 | Diff_total=+45.09 | MAE_total=45.09 | Pred_A=150.75 (Diff_A=-5.25, MAE_A=5.25) | Pred_B=225.75 (Diff_B=+53.75, MAE_B=53.75) | k_hat(full)=1.46 | ent(full)=0.359 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject7 | Jogging(153) -> Running(149) | GT_total=302 | Pred_total(win)=344.47 | Diff_total=+42.47 | MAE_total=42.47 | Pred_A=169.21 (Diff_A=+16.21, MAE_A=16.21) | Pred_B=174.35 (Diff_B=+25.35, MAE_B=25.35) | k_hat(full)=1.45 | ent(full)=0.356 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject8 | Jogging(160) -> Running(166) | GT_total=326 | Pred_total(win)=319.89 | Diff_total=-6.11 | MAE_total=6.11 | Pred_A=121.44 (Diff_A=-38.56, MAE_A=38.56) | Pred_B=201.68 (Diff_B=+35.68, MAE_B=35.68) | k_hat(full)=1.47 | ent(full)=0.433 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject9 | Jogging(166) -> Running(174) | GT_total=340 | Pred_total(win)=381.31 | Diff_total=+41.31 | MAE_total=41.31 | Pred_A=178.65 (Diff_A=+12.65, MAE_A=12.65) | Pred_B=203.41 (Diff_B=+29.41, MAE_B=29.41) | k_hat(full)=1.61 | ent(full)=0.324 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject10 | Jogging(156) -> Running(172) | GT_total=328 | Pred_total(win)=392.60 | Diff_total=+64.60 | MAE_total=64.60 | Pred_A=189.32 (Diff_A=+33.32, MAE_A=33.32) | Pred_B=203.67 (Diff_B=+31.67, MAE_B=31.67) | k_hat(full)=1.39 | ent(full)=0.330 | boundary=3072\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오3 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[TOTAL] [Jogging-Running] N=10 | MAE=32.179 | RMSE=38.452 | mean(diff)=+30.864 | std(diff)=22.934\n",
            "[A]     [Jogging-Running] N=10 | MAE=19.986 | RMSE=23.642 | mean(diff)=+1.715 | std(diff)=23.580\n",
            "[B]     [Jogging-Running] N=10 | MAE=30.075 | RMSE=32.026 | mean(diff)=+30.075 | std(diff)=11.007\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Integrator VS. Ours PCA plot\n",
        "- `Knees bending`, `Jump front and back`, `Jogging`"
      ],
      "metadata": {
        "id": "unCMHX10VAmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# PCA2D ONLY (Ours vs Integrator) — Activity-wise folders, fold-wise plots\n",
        "#\n",
        "# - For each activity:\n",
        "#   - LOSO fold loop\n",
        "#   - Train OURS model on train subjects (window-proxy supervision)\n",
        "#   - Extract OURS latent z(t) from the held-out test trial (full-trial forward)\n",
        "#   - Build Integrator window-feature trajectory from raw (order-invariant features)\n",
        "#   - Plot PCA2D comparison in ONE figure with 2 subplots (OURS vs INTEGRATOR)\n",
        "#   - NO titles, NO start/stop markers\n",
        "#   - Ticks padding tightened (closer to axes)\n",
        "#   - Saved as PNG (dpi=600), no plt.show()\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# IO helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _safe_filename(s: str) -> str:\n",
        "    s = str(s)\n",
        "    s = re.sub(r\"[^\\w\\-_\\. ]\", \"_\", s)\n",
        "    s = s.strip().replace(\" \", \"_\")\n",
        "    return s[:200] if len(s) > 200 else s\n",
        "\n",
        "\n",
        "def _ensure_dir(p: str):\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Strict seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Data loading (mHealth)\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    \"\"\"\n",
        "    Returns list of dict:\n",
        "      - data: z-scored (T,C)\n",
        "      - raw : raw (T,C)\n",
        "      - count: trial count\n",
        "      - meta : string\n",
        "    \"\"\"\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_id = int(act_id)\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            mean = raw_np.mean(axis=0, keepdims=True)\n",
        "            std = raw_np.std(axis=0, keepdims=True) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                \"data\": norm_np,\n",
        "                \"raw\": raw_np,\n",
        "                \"count\": float(gt_count),\n",
        "                \"meta\": f\"{subj}_{act_name}\",\n",
        "                \"subj\": subj,\n",
        "                \"act_id\": act_id,\n",
        "                \"act_name\": act_name,\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Windowing (TRAIN) + variable-length collate\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]  # (T,C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C,T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),    # (B,C,Tmax)\n",
        "        \"mask\": torch.stack(masks),          # (B,Tmax)\n",
        "        \"count\": torch.stack(counts),        # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),\n",
        "        \"meta\": metas,\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# OURS model (same as before but we only need z(t))\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B,D,T)\n",
        "        z = z.transpose(1, 2)      # (B,T,D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B,D,T)\n",
        "        return self.net(zt)        # (B,C,T)\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=128, K_max=6):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                 # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])     # (B,T)\n",
        "        logits = out[..., 1:]             # (B,T,K)\n",
        "        phase = F.softmax(logits / tau, dim=-1)\n",
        "        return amp, phase\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)        # (B,T,D)\n",
        "        x_hat = self.decoder(z)    # (B,C,T)\n",
        "        amp, phase = self.rate_head(z, tau=tau)  # (B,T), (B,T,K)\n",
        "\n",
        "        # rep_rate_t (for training losses)\n",
        "        p_bar = phase.mean(dim=1) if mask is None else (phase * mask.unsqueeze(-1)).sum(dim=1) / (mask.sum(dim=1, keepdim=True) + 1e-6)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)\n",
        "        rep_rate_t = amp / (k_hat.unsqueeze(1) + 1e-6)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        # avg rep rate (supervision target)\n",
        "        if mask is None:\n",
        "            avg_rep = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\"rep_rate_t\": rep_rate_t, \"phase\": phase}\n",
        "        return avg_rep, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Training (minimal)\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask.unsqueeze(1)\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase, mask=None, eps=1e-8):\n",
        "    ent = -(phase * (phase + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.unsqueeze(-1).to(dtype=phase.dtype, device=phase.device)\n",
        "        p_bar = (phase * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean()\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.0075)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        rate_hat, _, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_ent = phase_entropy_loss(aux[\"phase\"], mask)\n",
        "        loss_effk = effK_usage_loss(aux[\"phase\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_ent * loss_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# PCA2D helpers (no sklearn)\n",
        "# ---------------------------------------------------------------------\n",
        "def _fit_pca_basis(X, n_comp=2, eps=1e-6):\n",
        "    X = np.asarray(X, dtype=np.float32)\n",
        "    mu = X.mean(axis=0, keepdims=True)\n",
        "    sig = X.std(axis=0, keepdims=True) + eps\n",
        "    Xs = (X - mu) / sig\n",
        "    _, _, Vt = np.linalg.svd(Xs, full_matrices=False)\n",
        "    W = Vt[:n_comp].T\n",
        "    return mu.squeeze(0), sig.squeeze(0), W\n",
        "\n",
        "\n",
        "def _pca_project(X, mu, sig, W):\n",
        "    X = np.asarray(X, dtype=np.float32)\n",
        "    Xs = (X - mu[None, :]) / (sig[None, :] + 1e-12)\n",
        "    return Xs @ W\n",
        "\n",
        "\n",
        "def _downsample_by_index(X, t, max_N):\n",
        "    N = X.shape[0]\n",
        "    if N <= max_N:\n",
        "        return X, t\n",
        "    idx = np.linspace(0, N - 1, max_N).astype(int)\n",
        "    return X[idx], t[idx]\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Integrator: build window-feature trajectory (order-invariant)\n",
        "# ---------------------------------------------------------------------\n",
        "def integrator_window_feature_vector(x_raw: np.ndarray, fs: int, win_sec: float, stride_sec: float):\n",
        "    \"\"\"\n",
        "    Returns t_cent (N,), F (N, 2C)\n",
        "      per channel: log(1+energy), log(1+var)\n",
        "    \"\"\"\n",
        "    x = np.asarray(x_raw, dtype=np.float32)\n",
        "    T, C = x.shape\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    win_len = max(10, min(win_len, T))\n",
        "    stride = max(1, stride)\n",
        "\n",
        "    centers, feats = [], []\n",
        "    for st in range(0, max(1, T - win_len + 1), stride):\n",
        "        ed = st + win_len\n",
        "        w = x[st:ed] - x[st:ed].mean(axis=0, keepdims=True)\n",
        "        var = np.var(w, axis=0) + 1e-12\n",
        "        E = np.sum(w * w, axis=0) / max(float(fs), 1e-12)\n",
        "        f = np.concatenate([np.log1p(E), np.log1p(var)], axis=0)  # (2C,)\n",
        "        centers.append(0.5 * (st + ed) / float(fs))\n",
        "        feats.append(f.astype(np.float32))\n",
        "\n",
        "    return np.asarray(centers, dtype=np.float32), np.stack(feats, axis=0).astype(np.float32)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Plot: PCA2D comparison (2 subplots, no titles)\n",
        "# ---------------------------------------------------------------------\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "def plot_pca2d_comparison(\n",
        "    ours_z_tD, ours_t_sec,\n",
        "    int_F_nD, int_t_sec,\n",
        "    save_path,\n",
        "    dpi=600,\n",
        "    max_points=2000,\n",
        "    show_colorbar=True,\n",
        "    tick_pad=0,\n",
        "    point_size=10,\n",
        "    alpha=0.9\n",
        "):\n",
        "    ours_z_tD = np.asarray(ours_z_tD, dtype=np.float32)\n",
        "    ours_t_sec = np.asarray(ours_t_sec, dtype=np.float32)\n",
        "    int_F_nD = np.asarray(int_F_nD, dtype=np.float32)\n",
        "    int_t_sec = np.asarray(int_t_sec, dtype=np.float32)\n",
        "\n",
        "    ours_z_tD, ours_t_sec = _downsample_by_index(ours_z_tD, ours_t_sec, max_points)\n",
        "    int_F_nD, int_t_sec   = _downsample_by_index(int_F_nD, int_t_sec, max_points)\n",
        "\n",
        "    ours_c = (ours_t_sec - ours_t_sec.min()) / max(1e-6, (ours_t_sec.max() - ours_t_sec.min()))\n",
        "    int_c  = (int_t_sec  - int_t_sec.min())  / max(1e-6, (int_t_sec.max()  - int_t_sec.min()))\n",
        "\n",
        "    mu_o, sig_o, W_o = _fit_pca_basis(ours_z_tD, n_comp=2)\n",
        "    P_o = _pca_project(ours_z_tD, mu_o, sig_o, W_o)\n",
        "\n",
        "    mu_i, sig_i, W_i = _fit_pca_basis(int_F_nD, n_comp=2)\n",
        "    P_i = _pca_project(int_F_nD, mu_i, sig_i, W_i)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12.0, 5.0), sharey=True, gridspec_kw={\"wspace\": 0.10, \"width_ratios\": [1, 1]})\n",
        "    ax0, ax1 = axes\n",
        "\n",
        "    sc0 = ax0.scatter(P_o[:, 0], P_o[:, 1], c=ours_c, s=point_size, alpha=alpha, cmap=\"viridis\")\n",
        "    sc1 = ax1.scatter(P_i[:, 0], P_i[:, 1], c=int_c,  s=point_size, alpha=alpha, cmap=\"viridis\")\n",
        "\n",
        "    ax0.set_xlabel(\"PC1\", fontsize=16); ax0.set_ylabel(\"PC2\", fontsize=16)\n",
        "    ax1.set_xlabel(\"PC1\", fontsize=16); ax1.set_ylabel(\"\", fontsize=16)\n",
        "\n",
        "    for ax in (ax0, ax1):\n",
        "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=14, pad=tick_pad)\n",
        "        ax.grid(False)\n",
        "\n",
        "    # ✅ 여기 추가: 두 subplot에 동일한 xlim/ylim 적용\n",
        "    all_x = np.concatenate([P_o[:, 0], P_i[:, 0]], axis=0)\n",
        "    all_y = np.concatenate([P_o[:, 1], P_i[:, 1]], axis=0)\n",
        "\n",
        "    x_min, x_max = float(all_x.min()), float(all_x.max())\n",
        "    y_min, y_max = float(all_y.min()), float(all_y.max())\n",
        "\n",
        "    # padding (range의 5%)\n",
        "    x_pad = 0.05 * max(1e-6, (x_max - x_min))\n",
        "    y_pad = 0.05 * max(1e-6, (y_max - y_min))\n",
        "\n",
        "    xlim = (x_min - x_pad, x_max + x_pad)\n",
        "    ylim = (y_min - y_pad, y_max + y_pad)\n",
        "\n",
        "    ax0.set_xlim(*xlim); ax1.set_xlim(*xlim)\n",
        "    ax0.set_ylim(*ylim); ax1.set_ylim(*ylim)\n",
        "\n",
        "    if show_colorbar:\n",
        "        divider = make_axes_locatable(ax1)\n",
        "        cax = divider.append_axes(\"right\", size=\"4.5%\", pad=0.10)\n",
        "        cbar = fig.colorbar(sc1, cax=cax)\n",
        "        cbar.set_label(\"Normalized time (0→1)\", fontsize=14, labelpad=10)\n",
        "        cbar.ax.tick_params(labelsize=12, pad=tick_pad)\n",
        "        cbar.outline.set_linewidth(1.2)\n",
        "\n",
        "    _ensure_dir(os.path.dirname(save_path))\n",
        "    fig.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Main (LOSO per activity, PCA2D-only)\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "        \"out_dir\": \"./outputs_pca2d_only_compare\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            8:  'Knees bending',\n",
        "            10: 'Jogging',\n",
        "            12: 'Jump front & back',\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            8:  ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            10: ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            12: ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "        },\n",
        "\n",
        "        # Train params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Windowing for training\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # Integrator feature trajectory\n",
        "        \"INT_WIN_SEC\": 0.6,\n",
        "        \"INT_STRIDE_SEC\": 0.06,   # 더 촘촘하게 -> 점 개수 늘어서 blob 느낌\n",
        "    }\n",
        "\n",
        "    # Plot controls\n",
        "    PLOT = {\n",
        "        \"dpi\": 600,\n",
        "        \"max_points\": 2500,\n",
        "        \"show_colorbar\": True,   # 필요 없으면 False\n",
        "        \"tick_pad\": 3,          # 축에 붙게\n",
        "        \"point_size\": 10,\n",
        "        \"alpha\": 0.9,\n",
        "    }\n",
        "\n",
        "    # Count-only labels (그대로 사용)\n",
        "    CONFIG[\"ALL_LABELS\"] = [\n",
        "        # act 8\n",
        "        (\"subject1\", 8, 20), (\"subject2\", 8, 21), (\"subject3\", 8, 21), (\"subject4\", 8, 19), (\"subject5\", 8, 20),\n",
        "        (\"subject6\", 8, 20), (\"subject7\", 8, 21), (\"subject8\", 8, 21), (\"subject9\", 8, 21), (\"subject10\", 8, 21),\n",
        "\n",
        "        # act 10\n",
        "        (\"subject1\", 10, 157), (\"subject2\", 10, 161), (\"subject3\", 10, 154), (\"subject4\", 10, 154), (\"subject5\", 10, 160),\n",
        "        (\"subject6\", 10, 156), (\"subject7\", 10, 153), (\"subject8\", 10, 160), (\"subject9\", 10, 166), (\"subject10\", 10, 156),\n",
        "\n",
        "        # act 12\n",
        "        (\"subject1\", 12, 20), (\"subject2\", 12, 22), (\"subject3\", 12, 21), (\"subject4\", 12, 21), (\"subject5\", 12, 20),\n",
        "        (\"subject6\", 12, 21), (\"subject7\", 12, 19), (\"subject8\", 12, 20), (\"subject9\", 12, 20), (\"subject10\", 12, 20),\n",
        "    ]\n",
        "\n",
        "    _ensure_dir(CONFIG[\"out_dir\"])\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 90)\n",
        "    print(\" >>> PCA2D ONLY (OURS vs INTEGRATOR) — per activity, per fold\")\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    for act_id, act_name in CONFIG[\"TARGET_ACTIVITIES_MAP\"].items():\n",
        "        labels_act = [x for x in CONFIG[\"ALL_LABELS\"] if int(x[1]) == int(act_id)]\n",
        "        if len(labels_act) == 0:\n",
        "            print(f\"[Skip] act {act_id}: no labels.\")\n",
        "            continue\n",
        "\n",
        "        subjects_act = sorted(list(set([x[0] for x in labels_act])))\n",
        "\n",
        "        act_out_dir = _ensure_dir(os.path.join(CONFIG[\"out_dir\"], f\"act{act_id}_{_safe_filename(act_name)}\"))\n",
        "        print(\"\\n\" + \"=\" * 90)\n",
        "        print(f\"[Activity] {act_id}: {act_name} | #subjects={len(subjects_act)}\")\n",
        "        print(\"=\" * 90)\n",
        "\n",
        "        for fold_idx, test_subj in enumerate(subjects_act):\n",
        "            set_strict_seed(CONFIG[\"seed\"])\n",
        "            fold_out_dir = _ensure_dir(os.path.join(act_out_dir, f\"fold{fold_idx+1:02d}_{_safe_filename(test_subj)}\"))\n",
        "\n",
        "            train_labels = [x for x in labels_act if x[0] != test_subj]\n",
        "            test_labels  = [x for x in labels_act if x[0] == test_subj]\n",
        "\n",
        "            train_trials = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "            test_trials  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "            if not test_trials:\n",
        "                print(f\"[Skip] fold {fold_idx+1}: {test_subj} no data.\")\n",
        "                continue\n",
        "\n",
        "            # Train OURS\n",
        "            train_windows = trial_list_to_windows(\n",
        "                train_trials, fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                drop_last=CONFIG[\"drop_last\"]\n",
        "            )\n",
        "\n",
        "            g = torch.Generator()\n",
        "            g.manual_seed(CONFIG[\"seed\"])\n",
        "            train_loader = DataLoader(\n",
        "                TrialDataset(train_windows),\n",
        "                batch_size=CONFIG[\"batch_size\"],\n",
        "                shuffle=True,\n",
        "                collate_fn=collate_variable_length,\n",
        "                generator=g,\n",
        "                num_workers=0\n",
        "            )\n",
        "\n",
        "            input_ch = train_windows[0][\"data\"].shape[1]\n",
        "            model = KAutoCountModel(\n",
        "                input_ch=input_ch,\n",
        "                hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "                latent_dim=CONFIG[\"latent_dim\"],\n",
        "                K_max=CONFIG[\"K_max\"]\n",
        "            ).to(device)\n",
        "\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "            for _ in range(CONFIG[\"epochs\"]):\n",
        "                train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "                scheduler.step()\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            # Take ONE test trial (보통 1개)\n",
        "            item = test_trials[0]\n",
        "            x_np = item[\"data\"]     # (T,C) z-scored\n",
        "            x_raw = item[\"raw\"]     # (T,C) raw\n",
        "\n",
        "            # OURS z(t)\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "            with torch.no_grad():\n",
        "                _, z_full, _, _ = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "            ours_z = z_full.squeeze(0).detach().cpu().numpy()  # (T,D)\n",
        "            ours_t = np.arange(ours_z.shape[0], dtype=np.float32) / float(CONFIG[\"fs\"])\n",
        "\n",
        "            # Integrator features trajectory\n",
        "            int_t, int_F = integrator_window_feature_vector(\n",
        "                x_raw, fs=CONFIG[\"fs\"],\n",
        "                win_sec=float(CONFIG[\"INT_WIN_SEC\"]),\n",
        "                stride_sec=float(CONFIG[\"INT_STRIDE_SEC\"])\n",
        "            )\n",
        "\n",
        "            # Plot PCA2D comparison\n",
        "            save_path = os.path.join(fold_out_dir, \"pca2d_compare.png\")\n",
        "            plot_pca2d_comparison(\n",
        "                ours_z_tD=ours_z, ours_t_sec=ours_t,\n",
        "                int_F_nD=int_F, int_t_sec=int_t,\n",
        "                save_path=save_path,\n",
        "                dpi=int(PLOT[\"dpi\"]),\n",
        "                max_points=int(PLOT[\"max_points\"]),\n",
        "                show_colorbar=bool(PLOT[\"show_colorbar\"]),\n",
        "                tick_pad=float(PLOT[\"tick_pad\"]),\n",
        "                point_size=float(PLOT[\"point_size\"]),\n",
        "                alpha=float(PLOT[\"alpha\"]),\n",
        "            )\n",
        "\n",
        "            print(f\"  Fold {fold_idx+1:02d} | Test {test_subj} | saved: {save_path}\")\n",
        "\n",
        "    print(\"\\nDone.\")\n",
        "    print(f\"[Output root] {os.path.abspath(CONFIG['out_dir'])}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vxZ6apk9AK1",
        "outputId": "0d02010f-ce3b-437f-f7cb-b9ae457c31e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            " >>> PCA2D ONLY (OURS vs INTEGRATOR) — per activity, per fold\n",
            "------------------------------------------------------------------------------------------\n",
            "\n",
            "==========================================================================================\n",
            "[Activity] 8: Knees bending | #subjects=10\n",
            "==========================================================================================\n",
            "  Fold 01 | Test subject1 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold01_subject1/pca2d_compare.png\n",
            "  Fold 02 | Test subject10 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold02_subject10/pca2d_compare.png\n",
            "  Fold 03 | Test subject2 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold03_subject2/pca2d_compare.png\n",
            "  Fold 04 | Test subject3 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold04_subject3/pca2d_compare.png\n",
            "  Fold 05 | Test subject4 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold05_subject4/pca2d_compare.png\n",
            "  Fold 06 | Test subject5 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold06_subject5/pca2d_compare.png\n",
            "  Fold 07 | Test subject6 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold07_subject6/pca2d_compare.png\n",
            "  Fold 08 | Test subject7 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold08_subject7/pca2d_compare.png\n",
            "  Fold 09 | Test subject8 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold09_subject8/pca2d_compare.png\n",
            "  Fold 10 | Test subject9 | saved: ./outputs_pca2d_only_compare/act8_Knees_bending/fold10_subject9/pca2d_compare.png\n",
            "\n",
            "==========================================================================================\n",
            "[Activity] 10: Jogging | #subjects=10\n",
            "==========================================================================================\n",
            "  Fold 01 | Test subject1 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold01_subject1/pca2d_compare.png\n",
            "  Fold 02 | Test subject10 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold02_subject10/pca2d_compare.png\n",
            "  Fold 03 | Test subject2 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold03_subject2/pca2d_compare.png\n",
            "  Fold 04 | Test subject3 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold04_subject3/pca2d_compare.png\n",
            "  Fold 05 | Test subject4 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold05_subject4/pca2d_compare.png\n",
            "  Fold 06 | Test subject5 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold06_subject5/pca2d_compare.png\n",
            "  Fold 07 | Test subject6 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold07_subject6/pca2d_compare.png\n",
            "  Fold 08 | Test subject7 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold08_subject7/pca2d_compare.png\n",
            "  Fold 09 | Test subject8 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold09_subject8/pca2d_compare.png\n",
            "  Fold 10 | Test subject9 | saved: ./outputs_pca2d_only_compare/act10_Jogging/fold10_subject9/pca2d_compare.png\n",
            "\n",
            "==========================================================================================\n",
            "[Activity] 12: Jump front & back | #subjects=10\n",
            "==========================================================================================\n",
            "  Fold 01 | Test subject1 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold01_subject1/pca2d_compare.png\n",
            "  Fold 02 | Test subject10 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold02_subject10/pca2d_compare.png\n",
            "  Fold 03 | Test subject2 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold03_subject2/pca2d_compare.png\n",
            "  Fold 04 | Test subject3 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold04_subject3/pca2d_compare.png\n",
            "  Fold 05 | Test subject4 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold05_subject4/pca2d_compare.png\n",
            "  Fold 06 | Test subject5 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold06_subject5/pca2d_compare.png\n",
            "  Fold 07 | Test subject6 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold07_subject6/pca2d_compare.png\n",
            "  Fold 08 | Test subject7 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold08_subject7/pca2d_compare.png\n",
            "  Fold 09 | Test subject8 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold09_subject8/pca2d_compare.png\n",
            "  Fold 10 | Test subject9 | saved: ./outputs_pca2d_only_compare/act12_Jump_front___back/fold10_subject9/pca2d_compare.png\n",
            "\n",
            "Done.\n",
            "[Output root] /content/outputs_pca2d_only_compare\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Activity shift: Success VS. Failure case PCA & line plot\n",
        "- `Frontal elevation of arms→Knees bending `\n",
        "- `Knees bending→Jogging `"
      ],
      "metadata": {
        "id": "vHk87xiIZsbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# A1 Pairwise (SUCCESS + FAILURE only) Full Code  ✅ (requested)\n",
        "#\n",
        "# ✅ Only 2 experiments:\n",
        "#   (SUCCESS) Frontal elevation of arms (7) -> Knees bending (8)\n",
        "#   (FAILURE) Knees bending (8) -> Jogging (10)\n",
        "#\n",
        "# ✅ A1 visualization (per experiment):\n",
        "#   (1) Train-PCA basis에 Test를 투영해서 \"초점/방향(manifold alignment) 차이\"를 2-panel로 비교\n",
        "#   (2) pred_rate(t) vs gt_rate overlay (성공/실패 둘 다 저장; 실패에서 특히 해석용)\n",
        "#\n",
        "# -------------------------\n",
        "# \"다른건 건드리지 말고\" 원칙에 따라,\n",
        "# ❗ 꼭 필요해서 바꾼 부분:\n",
        "#   ✅ xlim, ylim만 A1_PCA_train7_test8_subject5 기준으로 \"고정\"되도록 추가/적용\n",
        "# -------------------------\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 0) Small helpers (dir / downsample / PCA)\n",
        "# ---------------------------------------------------------------------\n",
        "def _ensure_dir(path: str):\n",
        "    if path is None or path == \"\":\n",
        "        return\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "def _downsample_by_index(X, t, max_points=2000):\n",
        "    X = np.asarray(X)\n",
        "    t = np.asarray(t)\n",
        "    n = len(t)\n",
        "    if n <= max_points:\n",
        "        return X, t\n",
        "    idx = np.linspace(0, n - 1, max_points).astype(np.int64)\n",
        "    return X[idx], t[idx]\n",
        "\n",
        "\n",
        "def _fit_pca_basis(z_tD, n_comp=2, eps=1e-6):\n",
        "    \"\"\"\n",
        "    z_tD: (T,D)\n",
        "    returns: mu(D,), sig(D,), W(D,n_comp)\n",
        "    \"\"\"\n",
        "    Z = np.asarray(z_tD, dtype=np.float32)\n",
        "    mu = Z.mean(axis=0)\n",
        "    sig = Z.std(axis=0) + eps\n",
        "    Zs = (Z - mu) / sig\n",
        "    # Cov (D,D)\n",
        "    C = (Zs.T @ Zs) / max(1, (Zs.shape[0] - 1))\n",
        "    # eig via SVD (stable)\n",
        "    U, S, Vt = np.linalg.svd(C, full_matrices=False)\n",
        "    W = U[:, :n_comp]\n",
        "    return mu.astype(np.float32), sig.astype(np.float32), W.astype(np.float32)\n",
        "\n",
        "\n",
        "def _pca_project(z_tD, mu, sig, W):\n",
        "    Z = np.asarray(z_tD, dtype=np.float32)\n",
        "    Zs = (Z - mu) / sig\n",
        "    return (Zs @ W).astype(np.float32)\n",
        "\n",
        "\n",
        "# ✅ xlim/ylim 기준(Reference) 계산 helper: A1_PCA_train7_test8_subject5 기준으로 저장\n",
        "def _compute_ref_xlim_ylim_from_train_test_latents(train_z_tD, test_z_tD, pad_ratio=0.05):\n",
        "    \"\"\"\n",
        "    plot_pca2d_train_test_overlay()와 동일한 방식:\n",
        "      - PCA basis fit on TRAIN\n",
        "      - project TRAIN/TEST\n",
        "      - union min/max + pad\n",
        "    \"\"\"\n",
        "    mu, sig, W = _fit_pca_basis(train_z_tD, n_comp=2)\n",
        "    P_tr = _pca_project(train_z_tD, mu, sig, W)\n",
        "    P_te = _pca_project(test_z_tD,  mu, sig, W)\n",
        "\n",
        "    all_x = np.concatenate([P_tr[:, 0], P_te[:, 0]], axis=0)\n",
        "    all_y = np.concatenate([P_tr[:, 1], P_te[:, 1]], axis=0)\n",
        "\n",
        "    x_min, x_max = float(all_x.min()), float(all_x.max())\n",
        "    y_min, y_max = float(all_y.min()), float(all_y.max())\n",
        "\n",
        "    x_pad = float(pad_ratio) * max(1e-6, (x_max - x_min))\n",
        "    y_pad = float(pad_ratio) * max(1e-6, (y_max - y_min))\n",
        "\n",
        "    return (x_min - x_pad, x_max + x_pad), (y_min - y_pad, y_max + y_pad)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score (trial-wise)\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,              # (T,C)\n",
        "                'count': float(gt_count),      # trial total count\n",
        "                'meta': f\"{subj}_{act_name}\",\n",
        "                'subj': subj,\n",
        "                'act_id': act_id,\n",
        "            })\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    TRAIN 전용: trial -> sliding windows\n",
        "    window 라벨은 trial 평균 rate로부터 생성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]  # (T,C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    TEST 전용: trial -> windows inference -> window rate 평균 -> total count\n",
        "    x_np: (T,C) numpy (정규화된 상태)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return float(pred_count)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N,C,win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count)\n",
        "\n",
        "\n",
        "# ✅ [MOD-1] 추가: pred_rate(t) 시계열까지 반환\n",
        "def predict_count_and_rates_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    returns:\n",
        "      pred_count: float\n",
        "      t_centers_sec: (N,) window center time in sec\n",
        "      rates: (N,) predicted rate per window (reps/s)\n",
        "      gt_rate: float (trial-level gt_count / duration)\n",
        "      total_dur: float\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        rate = float(rate_hat.item())\n",
        "        pred_count = rate * total_dur\n",
        "        t_centers = np.array([0.5 * total_dur], dtype=np.float32)\n",
        "        rates = np.array([rate], dtype=np.float32)\n",
        "        return float(pred_count), t_centers, rates, float(total_dur)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    centers = np.array([st + 0.5 * win_len for st in starts], dtype=np.float32) / float(fs)\n",
        "\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N,C,win_len)\n",
        "\n",
        "    rates_list = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates_list.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates_list, axis=0).astype(np.float32)  # (N,)\n",
        "    pred_count = float(rates.mean() * total_dur)\n",
        "    return float(pred_count), centers, rates, float(total_dur)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.8) Dataset / Collate\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B,D,T)\n",
        "        z = z.transpose(1, 2)      # (B,T,D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B,D,T)\n",
        "        x_hat = self.net(zt)       # (B,C,T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        micro_rate_t = amp_t             # (B,T)\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,)\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6) # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"phase_p\": phase_p,          # (B,T,K)\n",
        "            \"rep_rate_t\": rep_rate_t,    # (B,T)\n",
        "            \"k_hat\": k_hat,              # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean()\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)  # (T,K)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4.5) A1 Plots\n",
        "# ---------------------------------------------------------------------\n",
        "def plot_pca2d_train_test_overlay(\n",
        "    train_z_tD, train_t_sec,\n",
        "    test_z_tD, test_t_sec,\n",
        "    save_path,\n",
        "    dpi=600,\n",
        "    max_points=2000,\n",
        "    show_colorbar=True,\n",
        "    tick_pad=0,\n",
        "    point_size=10,\n",
        "    alpha_test=0.9,\n",
        "    alpha_train=0.35,\n",
        "    train_lw=2.0,\n",
        "    # ✅ NEW: 고정 축 (A1_PCA_train7_test8_subject5 기준)\n",
        "    fixed_xlim=None,   # (xmin, xmax)\n",
        "    fixed_ylim=None    # (ymin, ymax)\n",
        "):\n",
        "    \"\"\"\n",
        "    ✅ PCA basis is FIT on TRAIN, then BOTH train/test are projected into the SAME basis.\n",
        "    ✅ Single-axis OVERLAY:\n",
        "       - TRAIN: gray scatter (reference manifold)\n",
        "       - TEST : time-colored scatter + single colorbar (time)\n",
        "    \"\"\"\n",
        "    train_z_tD = np.asarray(train_z_tD, dtype=np.float32)\n",
        "    train_t_sec = np.asarray(train_t_sec, dtype=np.float32)\n",
        "    test_z_tD  = np.asarray(test_z_tD,  dtype=np.float32)\n",
        "    test_t_sec = np.asarray(test_t_sec, dtype=np.float32)\n",
        "\n",
        "    train_z_tD, train_t_sec = _downsample_by_index(train_z_tD, train_t_sec, max_points)\n",
        "    test_z_tD,  test_t_sec  = _downsample_by_index(test_z_tD,  test_t_sec,  max_points)\n",
        "\n",
        "    test_c = (test_t_sec - test_t_sec.min()) / max(1e-6, (test_t_sec.max() - test_t_sec.min()))\n",
        "\n",
        "    mu, sig, W = _fit_pca_basis(train_z_tD, n_comp=2)\n",
        "    P_tr = _pca_project(train_z_tD, mu, sig, W)   # (T,2)\n",
        "    P_te = _pca_project(test_z_tD,  mu, sig, W)   # (T,2)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(7.0, 6.0))\n",
        "\n",
        "    ax.scatter(P_tr[:, 0], P_tr[:, 1], s=point_size, alpha=0.25, color=\"0.6\", label=\"Train (ref)\")\n",
        "    sc = ax.scatter(P_te[:, 0], P_te[:, 1], c=test_c, s=point_size, alpha=alpha_test, cmap=\"viridis\", label=\"Test (proj)\")\n",
        "\n",
        "    ax.set_xlabel(\"PC1\", fontsize=16)\n",
        "    ax.set_ylabel(\"PC2\", fontsize=16)\n",
        "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=14, pad=tick_pad)\n",
        "    ax.grid(False)\n",
        "\n",
        "    # ✅ xlim/ylim: 고정 축이 주어지면 그대로 사용\n",
        "    if fixed_xlim is not None and fixed_ylim is not None:\n",
        "        ax.set_xlim(float(fixed_xlim[0]), float(fixed_xlim[1]))\n",
        "        ax.set_ylim(float(fixed_ylim[0]), float(fixed_ylim[1]))\n",
        "    else:\n",
        "        all_x = np.concatenate([P_tr[:, 0], P_te[:, 0]], axis=0)\n",
        "        all_y = np.concatenate([P_tr[:, 1], P_te[:, 1]], axis=0)\n",
        "\n",
        "        x_min, x_max = float(all_x.min()), float(all_x.max())\n",
        "        y_min, y_max = float(all_y.min()), float(all_y.max())\n",
        "\n",
        "        x_pad = 0.05 * max(1e-6, (x_max - x_min))\n",
        "        y_pad = 0.05 * max(1e-6, (y_max - y_min))\n",
        "\n",
        "        ax.set_xlim(x_min - x_pad, x_max + x_pad)\n",
        "        ax.set_ylim(y_min - y_pad, y_max + y_pad)\n",
        "\n",
        "    if show_colorbar:\n",
        "        divider = make_axes_locatable(ax)\n",
        "        cax = divider.append_axes(\"right\", size=\"4.5%\", pad=0.10)\n",
        "        cbar = fig.colorbar(sc, cax=cax)\n",
        "        cbar.set_label(\"Normalized time (0→1)\", fontsize=14, labelpad=10)\n",
        "        cbar.ax.tick_params(labelsize=12, pad=tick_pad)\n",
        "        cbar.outline.set_linewidth(1.2)\n",
        "\n",
        "    _ensure_dir(os.path.dirname(save_path))\n",
        "    fig.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def plot_predrate_vs_gtrate(\n",
        "    t_sec,\n",
        "    pred_rate,\n",
        "    gt_rate,\n",
        "    save_path,\n",
        "    dpi=600,\n",
        "    tick_pad=0,\n",
        "    smooth_sigma=1.0,\n",
        "    lw_pred=4.0,\n",
        "    lw_gt=4.0,\n",
        "    legend_loc=\"best\"\n",
        "):\n",
        "    \"\"\"\n",
        "    pred_rate: (N,) window-level predicted repetition rate (reps/s)\n",
        "    gt_rate  : float, trial-level constant rate = count / duration\n",
        "    \"\"\"\n",
        "    t_sec = np.asarray(t_sec, dtype=np.float32)\n",
        "    pred_rate = np.asarray(pred_rate, dtype=np.float32)\n",
        "\n",
        "    if smooth_sigma is not None and smooth_sigma > 0:\n",
        "        pred_plot = gaussian_filter1d(pred_rate, sigma=float(smooth_sigma))\n",
        "    else:\n",
        "        pred_plot = pred_rate\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(8.5, 5.5))\n",
        "\n",
        "    ax.plot(\n",
        "        t_sec, pred_plot,\n",
        "        linewidth=lw_pred,\n",
        "        label=\"Pred rate (window-level)\"\n",
        "    )\n",
        "\n",
        "    ax.axhline(\n",
        "        y=float(gt_rate),\n",
        "        linestyle=\"--\",\n",
        "        linewidth=lw_gt,\n",
        "        label=\"GT rate (trial-level)\"\n",
        "    )\n",
        "\n",
        "    ax.set_xlabel(\"Time (s)\", fontsize=34)\n",
        "    ax.set_ylabel(\"Repetition rate (reps/s)\", fontsize=34)\n",
        "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=24, pad=tick_pad)\n",
        "    ax.grid(False)\n",
        "    ax.legend(loc=legend_loc, frameon=False, fontsize=18)\n",
        "\n",
        "    _ensure_dir(os.path.dirname(save_path))\n",
        "    fig.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        rate_hat, _, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Main (SUCCESS + FAILURE only)\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "        \"fs\": 50,\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "\n",
        "        # windowing\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # loss\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # outputs\n",
        "        \"plot_dir\": \"/content/A1_pairwise_A1viz\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            7: 'Frontal elevation of arms',\n",
        "            8: 'Knees bending',\n",
        "            10: 'Jogging',\n",
        "            11: 'Running',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            10: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                 'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                 'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                 'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                 'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            11: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                 'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                 'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                 'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                 'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                 'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                 'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                 'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                 'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "        },\n",
        "\n",
        "        \"COUNT_TABLE\": {\n",
        "            6: {\n",
        "                \"subject1\": 21, \"subject2\": 19, \"subject3\": 21, \"subject4\": 20, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 20, \"subject8\": 21, \"subject9\": 21, \"subject10\": 20,\n",
        "            },\n",
        "            7: {\n",
        "                \"subject1\": 20, \"subject2\": 20, \"subject3\": 20, \"subject4\": 20, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 20, \"subject8\": 19, \"subject9\": 19, \"subject10\": 20,\n",
        "            },\n",
        "            8: {\n",
        "                \"subject1\": 20, \"subject2\": 21, \"subject3\": 21, \"subject4\": 19, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 21, \"subject8\": 21, \"subject9\": 21, \"subject10\": 21,\n",
        "            },\n",
        "            10: {\n",
        "                \"subject1\": 157, \"subject2\": 161, \"subject3\": 154, \"subject4\": 154, \"subject5\": 160,\n",
        "                \"subject6\": 156, \"subject7\": 153, \"subject8\": 160, \"subject9\": 166, \"subject10\": 156,\n",
        "            },\n",
        "            11: {\n",
        "                \"subject1\": 165, \"subject2\": 158, \"subject3\": 174, \"subject4\": 163, \"subject5\": 157,\n",
        "                \"subject6\": 172, \"subject7\": 149, \"subject8\": 166, \"subject9\": 174, \"subject10\": 172,\n",
        "            },\n",
        "            12: {\n",
        "                \"subject1\": 20, \"subject2\": 22, \"subject3\": 21, \"subject4\": 21, \"subject5\": 20,\n",
        "                \"subject6\": 21, \"subject7\": 19, \"subject8\": 20, \"subject9\": 20, \"subject10\": 20,\n",
        "            },\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # -----------------------------\n",
        "    # sanity: feature 동일성 체크\n",
        "    # -----------------------------\n",
        "    feats_ref = None\n",
        "    for act_id, feats in CONFIG[\"ACT_FEATURE_MAP\"].items():\n",
        "        if feats_ref is None:\n",
        "            feats_ref = tuple(feats)\n",
        "        elif tuple(feats) != feats_ref:\n",
        "            raise ValueError(f\"[ERROR] ACT_FEATURE_MAP must be identical across activities. mismatch at act_id={act_id}\")\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n",
        "    if not full_data:\n",
        "        print(\"[ERROR] dataset load failed\")\n",
        "        return\n",
        "\n",
        "    _ensure_dir(CONFIG[\"plot_dir\"])\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    exp_dirs = [\n",
        "        (7, 8),\n",
        "        (8, 10),\n",
        "    ]\n",
        "\n",
        "    def build_labels(act_id):\n",
        "        labels = []\n",
        "        for s in subjects:\n",
        "            if act_id not in CONFIG[\"COUNT_TABLE\"]:\n",
        "                continue\n",
        "            if s not in CONFIG[\"COUNT_TABLE\"][act_id]:\n",
        "                continue\n",
        "            labels.append((s, act_id, CONFIG[\"COUNT_TABLE\"][act_id][s]))\n",
        "        return labels\n",
        "\n",
        "    # ✅ Reference axis limits from A1_PCA_train7_test8_subject5\n",
        "    REF_SUBJ = \"subject5\"\n",
        "    REF_XLIM, REF_YLIM = None, None\n",
        "\n",
        "    final_blocks = []\n",
        "    exp_idx = 0\n",
        "\n",
        "    for train_act, test_act in exp_dirs:\n",
        "        exp_idx += 1\n",
        "\n",
        "        train_labels = build_labels(train_act)\n",
        "        test_labels  = build_labels(test_act)\n",
        "        if len(train_labels) == 0 or len(test_labels) == 0:\n",
        "            print(f\"[Skip] Missing COUNT_TABLE: train_act={train_act}, test_act={test_act}\")\n",
        "            continue\n",
        "\n",
        "        set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "        train_trials = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_trials  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        if len(train_trials) == 0 or len(test_trials) == 0:\n",
        "            print(f\"[Skip] Empty trials: train_act={train_act}, test_act={test_act}\")\n",
        "            continue\n",
        "\n",
        "        train_windows = trial_list_to_windows(\n",
        "            train_trials,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"win_sec\"],\n",
        "            stride_sec=CONFIG[\"stride_sec\"],\n",
        "            drop_last=CONFIG[\"drop_last\"]\n",
        "        )\n",
        "        if len(train_windows) == 0:\n",
        "            print(f\"[Skip] Empty train windows: train_act={train_act}\")\n",
        "            continue\n",
        "\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(CONFIG[\"seed\"])\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_windows),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            generator=g,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_windows[0][\"data\"].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        train_name = CONFIG[\"TARGET_ACTIVITIES_MAP\"][train_act]\n",
        "        test_name  = CONFIG[\"TARGET_ACTIVITIES_MAP\"][test_act]\n",
        "\n",
        "        block_lines = []\n",
        "        abs_errs = []\n",
        "        per_subj = {}\n",
        "\n",
        "        for item in test_trials:\n",
        "            subj = item[\"subj\"]\n",
        "            x_np = item[\"data\"]\n",
        "            gt = float(item[\"count\"])\n",
        "\n",
        "            pred = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"],\n",
        "                stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device,\n",
        "                tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            diff = pred - gt\n",
        "            ae = abs(diff)\n",
        "            abs_errs.append(ae)\n",
        "            per_subj[subj] = {\"ae\": ae, \"pred\": pred, \"gt\": gt, \"x_np\": x_np}\n",
        "\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "\n",
        "            line = (f\"{subj} | GT={gt:.0f} | Pred(win)={pred:.2f} | Diff={diff:+.2f} | \"\n",
        "                    f\"k_hat(full)={k_hat:.2f} | phase_entropy(full)={ent:.3f}\")\n",
        "            block_lines.append(line)\n",
        "\n",
        "        mae = float(np.mean(abs_errs)) if len(abs_errs) > 0 else float(\"nan\")\n",
        "\n",
        "        is_success_pair = (train_act == 7 and test_act == 8)\n",
        "        if is_success_pair:\n",
        "            pick_subj = sorted(per_subj.keys(), key=lambda s: per_subj[s][\"ae\"])[0]\n",
        "        else:\n",
        "            pick_subj = sorted(per_subj.keys(), key=lambda s: per_subj[s][\"ae\"], reverse=True)[0]\n",
        "\n",
        "        # ------------------\n",
        "        # ✅ REF axis set once: based on A1_PCA_train7_test8_subject5\n",
        "        # ------------------\n",
        "        if REF_XLIM is None and REF_YLIM is None and (train_act == 7 and test_act == 8):\n",
        "            ref_train_item = next((it for it in train_trials if it[\"subj\"] == REF_SUBJ), None)\n",
        "            ref_test_item  = next((it for it in test_trials  if it[\"subj\"] == REF_SUBJ), None)\n",
        "\n",
        "            # fallback (only if subject5 not found)\n",
        "            if ref_train_item is None or ref_test_item is None:\n",
        "                ref_train_item = next((it for it in train_trials if it[\"subj\"] == pick_subj), None)\n",
        "                ref_test_item  = next((it for it in test_trials  if it[\"subj\"] == pick_subj), None)\n",
        "\n",
        "            if ref_train_item is not None and ref_test_item is not None:\n",
        "                fs = CONFIG[\"fs\"]\n",
        "                xtr_ref = torch.tensor(ref_train_item[\"data\"], dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "                xte_ref = torch.tensor(ref_test_item[\"data\"],  dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    ztr_ref = model.encoder(xtr_ref).squeeze(0).detach().cpu().numpy()\n",
        "                    zte_ref = model.encoder(xte_ref).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "                REF_XLIM, REF_YLIM = _compute_ref_xlim_ylim_from_train_test_latents(ztr_ref, zte_ref, pad_ratio=0.05)\n",
        "                print(f\"[REF AXIS] Using A1_PCA_train7_test8_{REF_SUBJ} xlim={REF_XLIM}, ylim={REF_YLIM}\")\n",
        "\n",
        "        # ------------------\n",
        "        # A1 visualization for picked subject\n",
        "        # ------------------\n",
        "        train_item = next((it for it in train_trials if it[\"subj\"] == pick_subj), None)\n",
        "        test_item  = next((it for it in test_trials  if it[\"subj\"] == pick_subj), None)\n",
        "\n",
        "        if train_item is not None and test_item is not None:\n",
        "            fs = CONFIG[\"fs\"]\n",
        "\n",
        "            xtr = torch.tensor(train_item[\"data\"], dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            xte = torch.tensor(test_item[\"data\"],  dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                ztr = model.encoder(xtr).squeeze(0).detach().cpu().numpy()\n",
        "                zte = model.encoder(xte).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "            ttr = np.arange(ztr.shape[0], dtype=np.float32) / float(fs)\n",
        "            tte = np.arange(zte.shape[0], dtype=np.float32) / float(fs)\n",
        "\n",
        "            pca_path = os.path.join(\n",
        "                CONFIG[\"plot_dir\"],\n",
        "                f\"A1_PCA_train{train_act}_test{test_act}_{pick_subj}.png\"\n",
        "            )\n",
        "\n",
        "            plot_pca2d_train_test_overlay(\n",
        "                train_z_tD=ztr, train_t_sec=ttr,\n",
        "                test_z_tD=zte,  test_t_sec=tte,\n",
        "                save_path=pca_path,\n",
        "                dpi=600,\n",
        "                max_points=2000,\n",
        "                show_colorbar=True,\n",
        "                tick_pad=0,\n",
        "                point_size=10,\n",
        "                alpha_test=0.9,\n",
        "                alpha_train=0.35,\n",
        "                train_lw=2.0,\n",
        "                # ✅ apply fixed axis (subject5 기준)\n",
        "                fixed_xlim=REF_XLIM,\n",
        "                fixed_ylim=REF_YLIM\n",
        "            )\n",
        "\n",
        "            x_np = test_item[\"data\"]\n",
        "            gt_count = float(test_item[\"count\"])\n",
        "            pred_count, t_centers, rates, total_dur = predict_count_and_rates_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=fs,\n",
        "                win_sec=CONFIG[\"win_sec\"],\n",
        "                stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device,\n",
        "                tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            gt_rate = gt_count / max(1e-6, total_dur)\n",
        "\n",
        "            rate_path = os.path.join(\n",
        "                CONFIG[\"plot_dir\"],\n",
        "                f\"A1_Rate_train{train_act}_test{test_act}_{pick_subj}.png\"\n",
        "            )\n",
        "            plot_predrate_vs_gtrate(\n",
        "                t_sec=t_centers,\n",
        "                pred_rate=rates,\n",
        "                gt_rate=gt_rate,\n",
        "                save_path=rate_path,\n",
        "                dpi=600,\n",
        "                tick_pad=0,\n",
        "                smooth_sigma=1.0\n",
        "            )\n",
        "\n",
        "        final_blocks.append({\n",
        "            \"idx\": exp_idx,\n",
        "            \"title\": f\"{train_name} -> {test_name}\",\n",
        "            \"lines\": block_lines,\n",
        "            \"mae\": mae,\n",
        "            \"picked_subject_for_plot\": pick_subj,\n",
        "            \"plot_dir\": CONFIG[\"plot_dir\"]\n",
        "        })\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 110)\n",
        "    print(\"A1 Pairwise Activity Transfer Results (SUCCESS + FAILURE only, windowing inference)\")\n",
        "    print(\"=\" * 110)\n",
        "\n",
        "    if len(final_blocks) == 0:\n",
        "        print(\"[No results] COUNT_TABLE이 충분히 채워졌는지 확인해줘.\")\n",
        "        print(\"=\" * 110)\n",
        "        return\n",
        "\n",
        "    for b in final_blocks:\n",
        "        print(f\"\\n{b['idx']}. {b['title']}  |  MAE={b['mae']:.3f}\")\n",
        "        print(f\"   -> A1 plots saved for: {b['picked_subject_for_plot']}  |  dir: {b['plot_dir']}\")\n",
        "        for ln in b[\"lines\"]:\n",
        "            print(ln)\n",
        "\n",
        "    maes = [b[\"mae\"] for b in final_blocks if np.isfinite(b[\"mae\"])]\n",
        "    if len(maes) > 0:\n",
        "        print(\"\\n\" + \"-\" * 110)\n",
        "        print(f\"Overall Avg MAE={float(np.mean(maes)):.3f} | Std={float(np.std(maes)):.3f} | #experiments={len(maes)}\")\n",
        "        print(\"-\" * 110)\n",
        "\n",
        "    print(\"=\" * 110)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWAAO0TiZ7KV",
        "outputId": "da3a12d0-b459-4886-ce06-32bb04aadde2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[REF AXIS] Using A1_PCA_train7_test8_subject5 xlim=(-6.684608030319214, 6.4062171459198), ylim=(-7.65162318944931, 4.551794230937958)\n",
            "\n",
            "==============================================================================================================\n",
            "A1 Pairwise Activity Transfer Results (SUCCESS + FAILURE only, windowing inference)\n",
            "==============================================================================================================\n",
            "\n",
            "1. Frontal elevation of arms -> Knees bending  |  MAE=2.275\n",
            "   -> A1 plots saved for: subject5  |  dir: /content/A1_pairwise_A1viz\n",
            "subject1 | GT=20 | Pred(win)=24.42 | Diff=+4.42 | k_hat(full)=1.00 | phase_entropy(full)=0.014\n",
            "subject2 | GT=21 | Pred(win)=23.17 | Diff=+2.17 | k_hat(full)=1.01 | phase_entropy(full)=0.039\n",
            "subject3 | GT=21 | Pred(win)=22.01 | Diff=+1.01 | k_hat(full)=1.01 | phase_entropy(full)=0.030\n",
            "subject4 | GT=19 | Pred(win)=19.86 | Diff=+0.86 | k_hat(full)=1.01 | phase_entropy(full)=0.033\n",
            "subject5 | GT=20 | Pred(win)=19.23 | Diff=-0.77 | k_hat(full)=1.01 | phase_entropy(full)=0.035\n",
            "subject6 | GT=20 | Pred(win)=15.92 | Diff=-4.08 | k_hat(full)=1.01 | phase_entropy(full)=0.017\n",
            "subject7 | GT=21 | Pred(win)=19.29 | Diff=-1.71 | k_hat(full)=1.02 | phase_entropy(full)=0.042\n",
            "subject8 | GT=21 | Pred(win)=17.52 | Diff=-3.48 | k_hat(full)=1.01 | phase_entropy(full)=0.022\n",
            "subject9 | GT=21 | Pred(win)=19.58 | Diff=-1.42 | k_hat(full)=1.01 | phase_entropy(full)=0.030\n",
            "subject10 | GT=21 | Pred(win)=18.17 | Diff=-2.83 | k_hat(full)=1.02 | phase_entropy(full)=0.050\n",
            "\n",
            "2. Knees bending -> Jogging  |  MAE=137.272\n",
            "   -> A1 plots saved for: subject9  |  dir: /content/A1_pairwise_A1viz\n",
            "subject1 | GT=157 | Pred(win)=16.94 | Diff=-140.06 | k_hat(full)=1.02 | phase_entropy(full)=0.056\n",
            "subject2 | GT=161 | Pred(win)=19.57 | Diff=-141.43 | k_hat(full)=1.02 | phase_entropy(full)=0.057\n",
            "subject3 | GT=154 | Pred(win)=20.84 | Diff=-133.16 | k_hat(full)=1.01 | phase_entropy(full)=0.041\n",
            "subject4 | GT=154 | Pred(win)=20.81 | Diff=-133.19 | k_hat(full)=1.01 | phase_entropy(full)=0.040\n",
            "subject5 | GT=160 | Pred(win)=21.06 | Diff=-138.94 | k_hat(full)=1.02 | phase_entropy(full)=0.042\n",
            "subject6 | GT=156 | Pred(win)=21.22 | Diff=-134.78 | k_hat(full)=1.02 | phase_entropy(full)=0.045\n",
            "subject7 | GT=153 | Pred(win)=20.95 | Diff=-132.05 | k_hat(full)=1.01 | phase_entropy(full)=0.029\n",
            "subject8 | GT=160 | Pred(win)=17.46 | Diff=-142.54 | k_hat(full)=1.01 | phase_entropy(full)=0.041\n",
            "subject9 | GT=166 | Pred(win)=22.92 | Diff=-143.08 | k_hat(full)=1.01 | phase_entropy(full)=0.037\n",
            "subject10 | GT=156 | Pred(win)=22.51 | Diff=-133.49 | k_hat(full)=1.01 | phase_entropy(full)=0.039\n",
            "\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Overall Avg MAE=69.774 | Std=67.498 | #experiments=2\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "==============================================================================================================\n"
          ]
        }
      ]
    }
  ]
}